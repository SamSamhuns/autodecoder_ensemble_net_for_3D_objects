{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder_Ensemble_Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and setting environ variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdqm.notebook not found. Try updating tdqm. Reverting to base tqdm No module named 'tqdm.notebook'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Try importing tqdm notebook\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ModuleNotFoundError as e:\n",
    "    print('tdqm.notebook not found. Try updating tdqm. Reverting to base tqdm',e)\n",
    "    import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# CUSTOM IMPORTS\n",
    "import LoaderFish                                               # function to gen custom point clouds (Wang et. al)\n",
    "from utils import chamfer_loss                                  # utility functions\n",
    "\n",
    "# TODO UNCOMMENT when implemented\n",
    "# from utils import chamfer_loss, HyperParameter, DirectorySetting\n",
    "\n",
    "# TODO UNCOMMENT when implemented\n",
    "# from nn_modles import AutoDecoder, CompNet, EnsembleCompNet       # autodecoder, comp_net, and ensemble_compnet modules\n",
    "# from dataset_module import PointDriftDS, EncodingDS, EncodingDS   # dataset modules\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Device Configuration\n",
    "\n",
    "Then, we set up and configure our computational devices: Whether we use GPU or perform the calculation on CPU. we use the torch.devices() and torch.cude.is_available() functions to configure our computational devices\n",
    "\n",
    "**Note: GPU Training is required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU training available\n",
      "Index of CUDA device in use is 0\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    # inbuilt cudnn auto-tuner searches for best algorithm for hardware\n",
    "    # cuddn.benchmark should be set to True when our input size does not vary\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"GPU training available\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Index of CUDA device in use is {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"GPU training NOT available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Can only train on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter Class\n",
    "\n",
    "# TODO save in a separate Python file\n",
    "\n",
    "Important hyperparameters\n",
    "\n",
    "-   latent Encoding dimension\n",
    "-   Encoding iterations and learning rate\n",
    "-   No of points sampled from point cloud\n",
    "-   No of sample shapes used in each class\n",
    "-   L2 regularization for encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "code_folding": [
     0,
     28
    ]
   },
   "outputs": [],
   "source": [
    "class HyperParameter:\n",
    "    def __init__(self,\n",
    "                 l2_reg=None,\n",
    "                 encoding_size=256,\n",
    "                 encoding_iters=50,\n",
    "                 num_point_cloud=3,\n",
    "                 epochs=4,\n",
    "                 lr=0.00001,\n",
    "                 batch_size=32):\n",
    "\n",
    "        self.l2_reg = l2_reg\n",
    "        self.learning_rate = lr\n",
    "        self.encoding_size = encoding_size\n",
    "        self.encoding_iters = encoding_iters\n",
    "        self.num_point_cloud = num_point_cloud\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"l2_reg: {self.l2_reg}\\n\" + \\\n",
    "               f\"learning_rate: {self.learning_rate}\\n\" + \\\n",
    "               f\"encoding_size: {self.encoding_size}\\n\" + \\\n",
    "               f\"encoding_iters: {self.encoding_iters}\\n\" + \\\n",
    "               f\"num_point_cloud: {self.num_point_cloud}\\n\" + \\\n",
    "               f\"epochs: {self.epochs}\\n\" + \\\n",
    "               f\"batch_size: {self.batch_size}\\n\"\n",
    "\n",
    "\n",
    "class DirectorySetting:\n",
    "\n",
    "    def __init__(self,\n",
    "                 DATA_DIR=\"./data\",\n",
    "                 OUTPUT_DIR=\"./tranformed/\",\n",
    "                 AUTODECODER_TRAINED_WEIGHT_DIR=\"./autodecoder_trained_weights\",\n",
    "                 CLASSIFIER_TRAINED_WEIGHT_DIR=\"./classifier_trained_weights\"):\n",
    "\n",
    "        self.AUTODECODER_TRAINED_WEIGHT_DIR = AUTODECODER_TRAINED_WEIGHT_DIR\n",
    "        self.CLASSIFIER_TRAINED_WEIGHT_DIR = CLASSIFIER_TRAINED_WEIGHT_DIR\n",
    "        self.OUTPUT_DIR = OUTPUT_DIR\n",
    "        self.DATA_DIR = DATA_DIR\n",
    "\n",
    "        os.makedirs(self.AUTODECODER_TRAINED_WEIGHT_DIR, exist_ok=True)\n",
    "        os.makedirs(self.CLASSIFIER_TRAINED_WEIGHT_DIR, exist_ok=True)\n",
    "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DATA_DIR: {self.DATA_DIR}\\n\" + \\\n",
    "               f\"OUTPUT_DIR: {self.OUTPUT_DIR}\\n\" + \\\n",
    "               f\"AUTODECODER_TRAINED_WEIGHT_DIR: {self.AUTODECODER_TRAINED_WEIGHT_DIR}\\n\" + \\\n",
    "               f\"CLASSIFIER_TRAINED_WEIGHT_DIR: {self.CLASSIFIER_TRAINED_WEIGHT_DIR}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set HyperParameter, DirectorySetting, SEED, and DEBUG Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "SEED  = 17*19\n",
    "\n",
    "HP = HyperParameter(epochs=10,\n",
    "                    l2_reg=None, \n",
    "                    batch_size=16,\n",
    "                    num_point_cloud=3,\n",
    "                    encoding_iters=1000, \n",
    "                    encoding_size=256)\n",
    "DS = DirectorySetting()\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training steps for autodecoder_ensemble_net\n",
    "\n",
    "1)   Train AutoDecoder\n",
    "\n",
    "2)   Find dataset latent encodings (latent representations of a point drift between two objects) and store\n",
    "\n",
    "3)   Train an ensemble of CompNets for 3D object point cloud classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models\n",
    "\n",
    "# TODO save in a separate Python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "code_folding": [
     0,
     22
    ]
   },
   "outputs": [],
   "source": [
    "class AutoDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoDecoder NN to learn point drift (latent encoding) between two 3D shapes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  encoding_dim=256, point_dim=3):\n",
    "        super(AutoDecoder, self).__init__()\n",
    "        self.fc1 = nn.Conv1d(encoding_dim + point_dim, 128, 1)\n",
    "        self.fc2 = nn.Conv1d(128, 64, 1)\n",
    "        self.fc3 = nn.Conv1d(64, point_dim, 1)\n",
    "\n",
    "    def forward(self, X, encoding):\n",
    "        num_points = X.shape[-1]  # num of points in each shape\n",
    "        enc = encoding.unsqueeze(-1).repeat(1, 1, num_points)\n",
    "        X_enc = torch.cat([X, enc], 1)\n",
    "        X_enc = F.leaky_relu(self.fc1(X_enc))\n",
    "        X_enc = F.leaky_relu(self.fc2(X_enc))\n",
    "\n",
    "        # Return the drift from obj X determined by the latent encoding\n",
    "        return X + self.fc3(X_enc)\n",
    "\n",
    "\n",
    "class CompNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Ingests the latent encoding of two 3D objects \n",
    "    and outputs the similarity score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding_size=256):\n",
    "        super(CompNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(encoding_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, encoding):\n",
    "        X = F.leaky_relu(self.fc1(encoding))\n",
    "        return torch.sigmoid(self.fc2(X))\n",
    "\n",
    "\n",
    "class EnsembleCompNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Ingests the latent encoding of two 3D objects \n",
    "    and outputs the similarity score using an ensemble of CompNets\n",
    "    Stacked Ensemble\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, comp_net=CompNet, num_ensemble=5, encoding_dim=256, seed_val=SEED):\n",
    "        \"\"\"\n",
    "        if comp_net is a module, EnsembleCompNet creates num_ensemble*comp_net NN modules\n",
    "        if comp_net is a list of modules, EnsembleCompNet iterates through comp_net to get the NN modules\n",
    "        \"\"\"\n",
    "        super(EnsembleCompNet, self).__init__()\n",
    "        self.ensemble_compnet = nn.ModuleList()\n",
    "\n",
    "        if isinstance(comp_net, list):\n",
    "            if num_ensemble != len(comp_net):\n",
    "                raise IndexError(\n",
    "                    f\"Length of comp_nets: {len(comp_net)} and num_ensemble: {num_ensemble} do not match\")\n",
    "            comp_net_list = comp_net\n",
    "            for i in range(num_ensemble):\n",
    "                self.ensemble_compnet.append(comp_net_list[i])\n",
    "        else:\n",
    "            for i in range(num_ensemble):\n",
    "                torch.manual_seed(seed_val*i+1)\n",
    "                if use_cuda:\n",
    "                    torch.cuda.manual_seed(seed_val*i+1)\n",
    "                self.ensemble_compnet.append(comp_net(encoding_dim))\n",
    "        self.final = nn.Linear(num_ensemble, 1)\n",
    "\n",
    "    def forward(self, encoding):\n",
    "        \"\"\" Returns the final value of the results after a nn.Linear layer \"\"\"\n",
    "        total_pred = torch.cat([net(encoding)\n",
    "                                for net in self.ensemble_compnet])\n",
    "        total_pred = total_pred.reshape(-1, len(self.ensemble_compnet))\n",
    "\n",
    "        return torch.sigmoid(self.final(total_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5570]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.2648]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" DEBUG Section \"\"\"\n",
    "if DEBUG:\n",
    "    def test_nn_modules():\n",
    "        autodecoder = AutoDecoder()\n",
    "        compnet = CompNet(256)\n",
    "        ensemblecompnet1 = EnsembleCompNet()\n",
    "        ensemblecompnet2 = EnsembleCompNet([CompNet(256), CompNet(256)], num_ensemble=2)\n",
    "        compnet.cuda()\n",
    "        autodecoder.cuda()\n",
    "        ensemblecompnet1.cuda()\n",
    "        ensemblecompnet2.cuda()\n",
    "        print(compnet(torch.Tensor(np.random.randn(1, 256)).cuda()))\n",
    "        print(ensemblecompnet2(torch.Tensor(np.random.randn(1, 256)).cuda()))\n",
    "        \n",
    "    test_nn_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Modules and DataLoader implementations\n",
    "\n",
    "# TODO Save in a separate Python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "code_folding": [
     0,
     18,
     54
    ]
   },
   "outputs": [],
   "source": [
    "class PointNetDS(Dataset):\n",
    "    \"\"\"\n",
    "    Create train, test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, sampling_interval=3):\n",
    "        # sample every sampling_interval-th point to speed up\n",
    "        self.data = data.transpose((0, 2, 1))[:, :, ::sampling_interval]\n",
    "        self.labels = labels.squeeze()\n",
    "        self.data = torch.from_numpy(self.data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class PointDriftDS(Dataset):\n",
    "    \"\"\"\n",
    "    Pairs each shape with one shape from the same class and one shape from a different class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, labels, sampling_interval=3):\n",
    "        # sample every sampling_interval-th point to speed up\n",
    "        self.data = data.transpose((0, 2, 1))[:, :, ::sampling_interval]\n",
    "        self.labels = labels.squeeze()\n",
    "\n",
    "        self.same_cls = []\n",
    "        self.diff_cls = []\n",
    "        idx_arr = np.arange(self.data.shape[0])\n",
    "        same_idx = []\n",
    "        diff_idx = []\n",
    "        for i in range(self.labels.max() + 1):\n",
    "            same_idx.append(idx_arr[self.labels == i])\n",
    "            diff_idx.append(idx_arr[self.labels != i])\n",
    "        for i in range(data.shape[0]):\n",
    "            same = same_idx[self.labels[i]]\n",
    "            diff = diff_idx[self.labels[i]]\n",
    "            self.same_cls.append(same[random.randint(0, len(same) - 1)])\n",
    "            self.diff_cls.append(diff[random.randint(0, len(diff) - 1)])\n",
    "        self.data = torch.from_numpy(self.data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx]\n",
    "        same_cls_data = self.data[self.same_cls[idx]]\n",
    "        diff_cls_data = self.data[self.diff_cls[idx]]\n",
    "\n",
    "        return X, same_cls_data, diff_cls_data, idx\n",
    "\n",
    "\n",
    "class EncodingDS(Dataset):\n",
    "    \"\"\"\n",
    "    Generate encoding for each pair of shapes in the PointDriftDS\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, PDDS, autodecoder, latent_size=256):\n",
    "        self.PointDriftDS = PDDS\n",
    "        self.autodecoder = autodecoder\n",
    "        self.latent_size = latent_size\n",
    "        self.same_cls = torch.zeros((len(self.PointDriftDS), latent_size))\n",
    "        self.diff_cls = torch.zeros((len(self.PointDriftDS), latent_size))\n",
    "\n",
    "    def train_encodings(self, num_iterations=50, lr=0.01, l2_reg=False, batch_size=16):\n",
    "        dl = DataLoader(self.PointDriftDS,\n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "        i = 0\n",
    "        batch_cnt = 0\n",
    "        same_cls_loss = 0.0\n",
    "        diff_cls_loss = 0.0\n",
    "        self.autodecoder.eval()\n",
    "\n",
    "        for batch_idx, (x, y, z, idx) in enumerate(dl):\n",
    "            j = i + len(idx)\n",
    "            loss, encoding = find_encoding(x, y, self.autodecoder, encoding_iters=num_iterations,\n",
    "                                           encoding_size=self.latent_size, lr=lr, l2_reg=l2_reg,)\n",
    "            same_cls_loss += loss\n",
    "            self.same_cls[i:j] = encoding\n",
    "            loss, encoding = find_encoding(x, z, self.autodecoder, encoding_iters=num_iterations,\n",
    "                                           encoding_size=self.latent_size, lr=lr, l2_reg=l2_reg,)\n",
    "            diff_cls_loss += loss\n",
    "            self.diff_cls[i:j] = encoding\n",
    "\n",
    "            i = j\n",
    "            batch_cnt += 1\n",
    "        print(\"Encodings trained\")\n",
    "        return (self.same_cls, self.diff_cls, same_cls_loss / batch_cnt, diff_cls_loss / batch_cnt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.PointDriftDS)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (*self.PointDriftDS[idx], self.same_cls[idx], self.diff_cls[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find encodings by using the autodecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def find_encoding(X, y, autodecoder, encoding_iters=300,\n",
    "                  encoding_size=256, lr=5e-4, l2_reg=False):\n",
    "    \"\"\"\n",
    "    Generate the encoding (latent vector) for each data in X\n",
    "    \"\"\"\n",
    "\n",
    "    def _adjust_lr(initial_lr, optimizer, num_iters, decreased_by, adjust_lr_every):\n",
    "\n",
    "        lr = initial_lr * ((1 / decreased_by) **\n",
    "                           (num_iters // adjust_lr_every))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    decreased_by = 10\n",
    "    adjust_lr_every = encoding_iters // 2\n",
    "\n",
    "    encoding = torch.ones(X.shape[0], encoding_size).normal_(\n",
    "        mean=0, std=1.0 / math.sqrt(encoding_size)).cuda()\n",
    "\n",
    "    encoding.requires_grad = True\n",
    "    optimizer = torch.optim.Adam([encoding], lr=lr)\n",
    "    loss_num = 0\n",
    "\n",
    "    for i in range(encoding_iters):\n",
    "        autodecoder.eval()\n",
    "        _adjust_lr(lr, optimizer, i, decreased_by, adjust_lr_every)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = autodecoder(X, encoding)\n",
    "        loss = chamfer_loss(y_pred, y, ps=y.shape[-1])\n",
    "\n",
    "        if l2_reg:\n",
    "            loss += 1e-4 * torch.mean(encoding.pow(2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(i, loss.cpu().data.numpy(), encoding.norm())\n",
    "        loss_num = loss.cpu().data.numpy()\n",
    "\n",
    "    return loss_num, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_decoder(HP, DS, train_ds, test_ds=None, decoder=None, save_weight=True):\n",
    "    \"\"\" \n",
    "    Default training is for 3D point dimensions\n",
    "    \n",
    "    Suggested Settings\n",
    "        EPOCHS = 10\n",
    "        point_dim = 3\n",
    "        batch_size = 16\n",
    "        learning_rate = 0.001\n",
    "        encoding_size = 256\n",
    "    \"\"\"\n",
    "    EPOCHS = HP.epochs\n",
    "    point_dim = HP.num_point_cloud\n",
    "    batch_size = HP.batch_size\n",
    "    encoding_size = HP.encoding_size\n",
    "    lr = HP.learning_rate\n",
    "    \n",
    "    if decoder is None:\n",
    "        adnet = AutoDecoder(encoding_size, point_dim)\n",
    "    else:\n",
    "        adnet = decoder\n",
    "    adnet = adnet.cuda()\n",
    "\n",
    "    # encodings for same class transformation\n",
    "    same_encoding = torch.nn.Embedding(\n",
    "        len(train_ds), encoding_size, max_norm=1.0)\n",
    "    # init encoding with Kaiming Initialization\n",
    "    torch.nn.init.normal_(same_encoding.weight.data,\n",
    "                          0.0,\n",
    "                          1.0 / math.sqrt(encoding_size))\n",
    "\n",
    "    # encodings for different class transformation\n",
    "    diff_encoding = torch.nn.Embedding(\n",
    "        len(train_ds), encoding_size, max_norm=1.0)\n",
    "    # init encoding with Kaiming Initialization\n",
    "    torch.nn.init.normal_(diff_encoding.weight.data,\n",
    "                          0.0,\n",
    "                          1.0 / math.sqrt(encoding_size))\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {\"params\": adnet.parameters(), \"lr\": lr, },\n",
    "        {\"params\": same_encoding.parameters(), \"lr\": lr, },\n",
    "        {\"params\": diff_encoding.parameters(), \"lr\": lr, }, ])\n",
    "\n",
    "    op_schedule = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    adnet = nn.DataParallel(adnet)\n",
    "    adnet.cuda()\n",
    "\n",
    "    data_loader_train = DataLoader(train_ds, batch_size=batch_size,\n",
    "                                   shuffle=True)\n",
    "    \n",
    "    for epoch in range(0, EPOCHS):\n",
    "        adnet.train()\n",
    "        same_total_loss = 0.0\n",
    "        diff_total_loss = 0.0\n",
    "\n",
    "        for batch_idx, (x, y, z, idx) in enumerate(data_loader_train):\n",
    "            optimizer.zero_grad()\n",
    "            x, y, z = x.cuda(), y.cuda(), z.cuda()\n",
    "            x, y, z = (Variable(x).float(),\n",
    "                       Variable(y).float(),\n",
    "                       Variable(z).float())\n",
    "            y_pred = adnet(x, same_encoding(torch.LongTensor(idx)))\n",
    "            loss_cham = chamfer_loss(y, y_pred, ps=y.shape[-1])\n",
    "            same_total_loss += loss_cham.data.cpu().numpy()\n",
    "            loss_cham.backward()\n",
    "\n",
    "            z_pred = adnet(x, diff_encoding(torch.LongTensor(idx)))\n",
    "            loss_cham = chamfer_loss(z, z_pred, ps=z.shape[-1])\n",
    "            diff_total_loss += loss_cham.data.cpu().numpy()\n",
    "            loss_cham.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                print(f\"Epoch: {epoch}. batch_idx: {batch_idx}\")\n",
    "                print(\"Loss: \",same_total_loss / 100, diff_total_loss / 100)\n",
    "                same_total_loss = 0.0\n",
    "                diff_total_loss = 0.0\n",
    "        op_schedule.step(epoch)\n",
    "\n",
    "        if test_ds is not None and epoch % 5 == 0:\n",
    "            print(\"Eval: \", eval_decoder(adnet, test_ds, batch_size=batch_size))\n",
    "\n",
    "    if save_weight:\n",
    "        torch.save(adnet.state_dict(), DS.AUTODECODER_TRAINED_WEIGHT_DIR + '/decoder.pth')\n",
    "    return adnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def eval_decoder(decoder, eval_ds, batch_size=16):\n",
    "    decoder.eval()\n",
    "    encoding_ds = EncodingDataset(eval_ds, decoder)\n",
    "    return encoding_ds.train_encodings(num_iterations=10, lr=0.05, batch_size=batch_size)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train_compnet(HP, DS, train_ds, test_ds=None, compnet=None, save_weights=True):\n",
    "    \"\"\"\n",
    "    Train the CompNet\n",
    "\n",
    "    Suggested Parameters\n",
    "    EPOCHS=10\n",
    "    batch_size=16\n",
    "    encoding_size=256\n",
    "    learning_rate=0.001\n",
    "    \"\"\"\n",
    "    EPOCHS = HP.epochs\n",
    "    point_dim = HP.num_point_cloud\n",
    "    batch_size = HP.batch_size\n",
    "    encoding_size = HP.encoding_size\n",
    "    lr = HP.learning_rate\n",
    "\n",
    "    if compnet is None:\n",
    "        cpnet = CompNet(encoding_size=encoding_size)\n",
    "    else:\n",
    "        cpnet = compnet\n",
    "    cpnet = cpnet.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(cpnet.parameters(), lr=lr)\n",
    "    op_schedule = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    cpnet = nn.DataParallel(cpnet)\n",
    "    cpnet.cuda()\n",
    "\n",
    "    data_loader_train = DataLoader(train_ds, batch_size=batch_size,\n",
    "                    shuffle=True)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(EPOCHS):\n",
    "        same_total_loss = 0.0\n",
    "        diff_total_loss = 0.0\n",
    "        cpnet.train()\n",
    "        for batch_idx, (x, y, z, idx, same_cls, diff_cls) in enumerate(data_loader_train):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            same_cls, diff_cls = same_cls.cuda(), diff_cls.cuda()\n",
    "            same_cls, diff_cls = Variable(\n",
    "                same_cls).float(), Variable(diff_cls).float()\n",
    "\n",
    "            same_pred = cpnet(same_cls)\n",
    "            same_target = torch.ones(same_pred.shape).float().cuda()\n",
    "            same_loss = loss_fn(same_pred, same_target)\n",
    "            same_loss.backward()\n",
    "            same_total_loss += same_loss.data.cpu().numpy()\n",
    "\n",
    "            diff_pred = cpnet(diff_cls)\n",
    "            diff_target = torch.zeros(diff_pred.shape).float().cuda()\n",
    "            diff_loss = loss_fn(diff_pred, diff_target)\n",
    "            diff_loss.backward()\n",
    "            diff_total_loss += diff_loss.data.cpu().numpy()\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                print(f\"Epoch: {epoch}. batch_idx: {batch_idx}\")\n",
    "                print(\"Loss: \", same_total_loss / 100, diff_total_loss / 100)\n",
    "                same_total_loss = 0.0\n",
    "                diff_total_loss = 0.0\n",
    "        op_schedule.step(epoch)\n",
    "        \n",
    "        if test_ds is not None and epoch % 5 == 0:\n",
    "            print(\"Eval: \", eval_compnet(cpnet, test_ds, batch_size=batch_size))\n",
    "\n",
    "    if save_weights:\n",
    "        torch.save(cpnet.state_dict(),\n",
    "                   DS.CLASSIFIER_TRAINED_WEIGHT_DIR + '/compnet.pth')\n",
    "    return cpnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def eval_compnet(cpnet, test_ds, batch_size=16, pred_threshold=0.5):\n",
    "    cpnet.eval()\n",
    "    test_dl = DataLoader(test_ds, batch_size=batch_size,\n",
    "                         shuffle=False)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    same_total_loss = 0.0\n",
    "    diff_total_loss = 0.0\n",
    "    batch_cnt = 0\n",
    "    same_corr_cnt = 0.0\n",
    "    diff_corr_cnt = 0.0\n",
    "    same_incorr_cnt = 0.0\n",
    "    diff_incorr_cnt = 0.0\n",
    "\n",
    "    for batch_idx, (x, y, z, idx, same_cls, diff_cls) in enumerate(test_dl):\n",
    "        batch_cnt += 1\n",
    "        same_cls, diff_cls = same_cls.cuda(), diff_cls.cuda()\n",
    "\n",
    "        same_pred = cpnet(same_cls)\n",
    "        same_target = torch.ones(same_pred.shape).float().cuda()\n",
    "        same_loss = loss_fn(same_pred, same_target)\n",
    "        same_total_loss += same_loss.data.cpu().numpy()\n",
    "\n",
    "        same_corr_cnt += np.sum(same_pred.detach().cpu().numpy()\n",
    "                                > pred_threshold)\n",
    "        same_incorr_cnt += np.sum(same_pred.detach().cpu().numpy()\n",
    "                                  <= pred_threshold)\n",
    "\n",
    "        diff_pred = cpnet(diff_cls)\n",
    "        diff_target = torch.zeros(diff_pred.shape).float().cuda()\n",
    "        diff_loss = loss_fn(diff_pred, diff_target)\n",
    "        diff_total_loss += diff_loss.data.cpu().numpy()\n",
    "\n",
    "        diff_corr_cnt += np.sum(diff_pred.detach().cpu().numpy()\n",
    "                                < pred_threshold)\n",
    "        diff_incorr_cnt += np.sum(diff_pred.detach().cpu().numpy()\n",
    "                                  >= pred_threshold)\n",
    "\n",
    "    precision = same_corr_cnt / (same_corr_cnt+diff_incorr_cnt)\n",
    "    recall = same_corr_cnt / (same_corr_cnt+same_incorr_cnt) # same_corr_cnt / len(test_ds)\n",
    "    print(\"------------------ Evaluation Report ------------------\")\n",
    "    print(f\"Total Accuracy: {(same_corr_cnt+diff_corr_cnt)/(2*len(test_ds))}\")\n",
    "    print(f\"After {batch_cnt} batches and {len(test_ds)} test points\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Metrics for the same class:\")\n",
    "    print(f\"Avg loss: {same_total_loss / batch_cnt}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {(2*precision*recall)/(precision+recall)}\")\n",
    "\n",
    "    precision = diff_corr_cnt / (diff_corr_cnt+same_incorr_cnt) \n",
    "    recall = diff_corr_cnt / (diff_corr_cnt+diff_incorr_cnt) # diff_corr_cnt / len(test_ds)\n",
    "    print()\n",
    "    print(f\"Metrics for the diff class:\")\n",
    "    print(f\"Avg loss: {diff_total_loss / batch_cnt}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {(2*precision*recall)/(precision+recall)}\")\n",
    "\n",
    "    return (same_total_loss, diff_total_loss,\n",
    "            same_corr_cnt, diff_corr_cnt,\n",
    "            same_incorr_cnt, diff_incorr_cnt,\n",
    "            batch_cnt, len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify(X, cls_samples, cls_num, autodecoder, compnet,\n",
    "             num_iterations=15, latent_size=256, lr=0.01, l2_reg=False):\n",
    "    autodecoder.eval()\n",
    "    compnet.eval()\n",
    "    num_samples = len(cls_samples) # 6\n",
    "    num_X = len(X)\n",
    "    print(\"Orig X Shape:\", X.shape) # 16 3 683\n",
    "    X = X.unsqueeze(1).repeat((1, num_samples, 1, 1))\n",
    "    # print(X.shape) # [16, 6, 3, 683]\n",
    "    # print(X.reshape((X.shape[0]*num_samples, 1, *X.shape[2:])).shape) # [96, 1, 3, 683]\n",
    "    X = X.reshape((X.shape[0]*num_samples, 1, *X.shape[2:])).squeeze()\n",
    "    print(\"X shape\", X.shape, \"num_X:\", num_X) # [96, 3, 683], 16\n",
    "    cls_samples = cls_samples.repeat(num_X, 1, 1)\n",
    "    print(\"cls_samples Shape\", cls_samples.shape) # [96, 3, 683]\n",
    "\n",
    "    loss, encoding = find_encoding(X, cls_samples, autodecoder, encoding_iters=num_iterations,\n",
    "                                   encoding_size=latent_size, lr=lr, l2_reg=l2_reg)\n",
    "    print(loss, 'ecnoding=;',encoding.shape) # 96 x 256\n",
    "    preds = compnet(encoding)\n",
    "    print(preds.shape) # 96,1\n",
    "    preds = preds.reshape(\n",
    "        (num_X, cls_num, (preds.shape[0]//(num_X * cls_num))))  # 16, 3, 96/(16x3)\n",
    "    \n",
    "    print(\"preds shape\", preds.shape) # 16 3 2\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig X Shape: torch.Size([11, 3, 683])\n",
      "X shape torch.Size([33, 3, 683]) num_X: 11\n",
      "cls_samples Shape torch.Size([33, 3, 683])\n",
      "0 0.050085843 tensor(5.7709, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0.011123054 ecnoding=; torch.Size([33, 256])\n",
      "torch.Size([33, 1])\n",
      "preds shape torch.Size([11, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9311, 0.7207, 0.4000]],\n",
       "\n",
       "        [[0.5229, 0.4717, 0.5597]],\n",
       "\n",
       "        [[0.9470, 0.7850, 0.6468]],\n",
       "\n",
       "        [[0.9074, 0.6133, 0.5556]],\n",
       "\n",
       "        [[0.6011, 0.7495, 0.5542]],\n",
       "\n",
       "        [[0.6198, 0.6978, 0.7069]],\n",
       "\n",
       "        [[0.6561, 0.4018, 0.3689]],\n",
       "\n",
       "        [[0.4924, 0.7097, 0.5966]],\n",
       "\n",
       "        [[0.0916, 0.3650, 0.5170]],\n",
       "\n",
       "        [[0.3604, 0.4960, 0.1065]],\n",
       "\n",
       "        [[0.5914, 0.5729, 0.0479]]], device='cuda:0',\n",
       "       grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds = PointNetDS(pn_test_data)\n",
    "dl1 = DataLoader(pn_test_ds, batch_size=16, shuffle=False)\n",
    "X = next(iter(dl1))[0]\n",
    "\n",
    "X = gds[(pn_test_labels == 2).squeeze()]\n",
    "cls_samples = gds[(pn_test_labels == 2).squeeze()][:3]\n",
    "\n",
    "classify(X, cls_samples, 1, pn_autodecoder, compnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "# Testing our AutoDecoder\n",
    "\n",
    "## Generating dummy Train and Test pair Fish shapes using LoaderFish (Wang et al. 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     9,
     25
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 73/20000 [00:00<00:27, 726.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......Synthesizing Training Pairs......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:27<00:00, 736.21it/s]\n",
      " 15%|█▍        | 148/1000 [00:00<00:01, 738.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......Synthesizing Testing Pairs......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 736.47it/s]\n"
     ]
    }
   ],
   "source": [
    "CD_before = []  # chamfer distance before registration\n",
    "CD_after = []  # chamfer distance after registration\n",
    "\n",
    "train_size = 20000  # size for training data\n",
    "test_size = 1000    # size for testing data\n",
    "deformation_list = [0.5]\n",
    "deformation = deformation_list[0]\n",
    "\n",
    "print(\".......Synthesizing Training Pairs......\")\n",
    "lf_train = LoaderFish.PointRegDataset(total_data=train_size,\n",
    "                                      deform_level=deformation,\n",
    "                                      noise_ratio=0,\n",
    "                                      outlier_ratio=0,\n",
    "                                      outlier_s=False,\n",
    "                                      outlier_t=True,\n",
    "                                      noise_s=False,\n",
    "                                      noise_t=True,\n",
    "                                      missing_points=0,\n",
    "                                      miss_source=False,\n",
    "                                      miss_targ=True)\n",
    "\n",
    "data_loader_lf_train = torch.utils.data.DataLoader(\n",
    "    lf_train, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\".......Synthesizing Testing Pairs......\")\n",
    "lf_test = LoaderFish.PointRegDataset(total_data=test_size,\n",
    "                                     deform_level=deformation,\n",
    "                                     noise_ratio=0,\n",
    "                                     outlier_ratio=0,\n",
    "                                     outlier_s=False,\n",
    "                                     outlier_t=True,\n",
    "                                     noise_s=False,\n",
    "                                     noise_t=True,\n",
    "                                     missing_points=0,\n",
    "                                     miss_source=False,\n",
    "                                     miss_targ=True)\n",
    "\n",
    "data_loader_lf_test = torch.utils.data.DataLoader(\n",
    "    lf_test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([16, 2, 91]) 16 torch.Size([2, 91])\n"
     ]
    }
   ],
   "source": [
    "sample_data = next(iter(data_loader_lf_train))\n",
    "print(len(sample_data), sample_data[0].shape, len(sample_data[0]), sample_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the AutoDecoder on the sample generated LoaderFish dataset\n",
    "\n",
    "We generate encoding latent vectors for the shapes generated by the LoaderFish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Setup for loaderfish test\n",
    "EPOCHS = 4\n",
    "lr = 0.0001\n",
    "encoding_size = 256\n",
    "scheduler_gamma = 0.5\n",
    "scheduler_step_sz = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch Index:0\n",
      "0.0526277\n",
      "Epoch:0 Batch Index:100\n",
      "0.04193649\n",
      "Epoch:0 Batch Index:200\n",
      "0.04914833\n",
      "Epoch:0 Batch Index:300\n",
      "0.04160332\n",
      "Epoch:0 Batch Index:400\n",
      "0.033825263\n",
      "Epoch:0 Batch Index:500\n",
      "0.030017994\n",
      "Epoch:0 Batch Index:600\n",
      "0.028596533\n",
      "Epoch:0 Batch Index:700\n",
      "0.033026434\n",
      "Epoch:0 Batch Index:800\n",
      "0.030410869\n",
      "Epoch:0 Batch Index:900\n",
      "0.021873951\n",
      "Epoch:0 Batch Index:1000\n",
      "0.021696791\n",
      "Epoch:0 Batch Index:1100\n",
      "0.022542069\n",
      "Epoch:0 Batch Index:1200\n",
      "0.019152097\n",
      "Epoch:1 Batch Index:0\n",
      "0.016948704\n",
      "Epoch:1 Batch Index:100\n",
      "0.017050937\n",
      "Epoch:1 Batch Index:200\n",
      "0.01621691\n",
      "Epoch:1 Batch Index:300\n",
      "0.015167228\n",
      "Epoch:1 Batch Index:400\n",
      "0.015761562\n",
      "Epoch:1 Batch Index:500\n",
      "0.014706012\n",
      "Epoch:1 Batch Index:600\n",
      "0.01617067\n",
      "Epoch:1 Batch Index:700\n",
      "0.015201665\n",
      "Epoch:1 Batch Index:800\n",
      "0.013097441\n",
      "Epoch:1 Batch Index:900\n",
      "0.015565841\n",
      "Epoch:1 Batch Index:1000\n",
      "0.013944613\n",
      "Epoch:1 Batch Index:1100\n",
      "0.012908214\n",
      "Epoch:1 Batch Index:1200\n",
      "0.012408565\n",
      "Epoch:2 Batch Index:0\n",
      "0.012165579\n",
      "Epoch:2 Batch Index:100\n",
      "0.0135458335\n",
      "Epoch:2 Batch Index:200\n",
      "0.010731203\n",
      "Epoch:2 Batch Index:300\n",
      "0.011571374\n",
      "Epoch:2 Batch Index:400\n",
      "0.0127799865\n",
      "Epoch:2 Batch Index:500\n",
      "0.012541294\n",
      "Epoch:2 Batch Index:600\n",
      "0.010115444\n",
      "Epoch:2 Batch Index:700\n",
      "0.012463987\n",
      "Epoch:2 Batch Index:800\n",
      "0.009743338\n",
      "Epoch:2 Batch Index:900\n",
      "0.009932697\n",
      "Epoch:2 Batch Index:1000\n",
      "0.0099725\n",
      "Epoch:2 Batch Index:1100\n",
      "0.0084174825\n",
      "Epoch:2 Batch Index:1200\n",
      "0.009836972\n",
      "Epoch:3 Batch Index:0\n",
      "0.0087154\n",
      "Epoch:3 Batch Index:100\n",
      "0.008174463\n",
      "Epoch:3 Batch Index:200\n",
      "0.008277431\n",
      "Epoch:3 Batch Index:300\n",
      "0.009805617\n",
      "Epoch:3 Batch Index:400\n",
      "0.008166348\n",
      "Epoch:3 Batch Index:500\n",
      "0.010587694\n",
      "Epoch:3 Batch Index:600\n",
      "0.008434207\n",
      "Epoch:3 Batch Index:700\n",
      "0.008640593\n",
      "Epoch:3 Batch Index:800\n",
      "0.009139008\n",
      "Epoch:3 Batch Index:900\n",
      "0.0065906527\n",
      "Epoch:3 Batch Index:1000\n",
      "0.0074547906\n",
      "Epoch:3 Batch Index:1100\n",
      "0.008245285\n",
      "Epoch:3 Batch Index:1200\n",
      "0.0075725922\n"
     ]
    }
   ],
   "source": [
    "adnet = AutoDecoder(point_dim=2)\n",
    "adnet.cuda()\n",
    "\n",
    "encoding = torch.nn.Embedding(len(lf_train), encoding_size, max_norm=1.0)\n",
    "# init encoding with Kaiming Initialization\n",
    "torch.nn.init.normal_(\n",
    "    encoding.weight.data,\n",
    "    0.0,\n",
    "    1.0 / math.sqrt(encoding_size))\n",
    "\n",
    "optimizer = torch.optim.Adam([{\"params\": adnet.parameters(), \"lr\": lr, },\n",
    "                           {\"params\": encoding.parameters(), \"lr\": lr, }, ])\n",
    "op_schedule = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=scheduler_step_sz, gamma=scheduler_gamma)\n",
    "\n",
    "# use multiple gpus\n",
    "adnet = nn.DataParallel(adnet)\n",
    "adnet.cuda()\n",
    "\n",
    "# Train the AutoDecoder adnet on the training data\n",
    "for epoch in range(EPOCHS):\n",
    "    adnet.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (X, y, theta, _, idx) in enumerate(data_loader_lf_train):\n",
    "        X, y, theta = X.cuda(), y.cuda(), theta.cuda()\n",
    "        X, y, theta = Variable(X).float(), Variable(y).float(), Variable(theta).float()\n",
    "        y_pred = adnet(X, encoding(torch.LongTensor(idx)))\n",
    "        loss_cham = chamfer_loss(y, y_pred, ps=y.shape[-1])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_cham.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch:{epoch} Batch Index:{batch_idx}\")\n",
    "            print(loss_cham.data.cpu().numpy())\n",
    "    op_schedule.step(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights of the decoder\n",
    "torch.save(adnet.state_dict(), DS.AUTODECODER_TRAINED_WEIGHT_DIR+'/loaderfish_encoder01.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the AudoDecoder network on the sample LoaderFish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.008613841 tensor(3.1736, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061344756 tensor(3.1572, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056720297 tensor(3.1910, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0053899586 tensor(3.2523, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053690346 tensor(3.2589, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005349036 tensor(3.2667, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "At batch:0, Chamfer Loss:0.005328803323209286, Encoding Chamfer Loss:0.005329195410013199\n",
      "0 0.0072438284 tensor(3.1175, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0056943465 tensor(3.1572, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005288393 tensor(3.2384, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005076689 tensor(3.3354, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005058905 tensor(3.3454, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0050417306 tensor(3.3560, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007972601 tensor(3.1994, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.00622008 tensor(3.1856, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0058319285 tensor(3.2332, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005637585 tensor(3.2979, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005623786 tensor(3.3044, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005610199 tensor(3.3118, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008125241 tensor(3.1591, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.00624587 tensor(3.1731, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005788705 tensor(3.2387, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055825086 tensor(3.3128, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0055622146 tensor(3.3213, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0055431738 tensor(3.3313, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008833405 tensor(3.2061, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006613531 tensor(3.2056, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0060993293 tensor(3.2496, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005814989 tensor(3.3209, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0057933885 tensor(3.3274, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005771108 tensor(3.3356, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0077987565 tensor(3.2407, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006176154 tensor(3.2279, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005612184 tensor(3.3161, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005384945 tensor(3.4075, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053663854 tensor(3.4162, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0053477446 tensor(3.4255, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0082073845 tensor(3.1746, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0062761046 tensor(3.2167, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005801805 tensor(3.2893, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055974205 tensor(3.3593, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0055814153 tensor(3.3666, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0055662994 tensor(3.3744, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007950207 tensor(3.1352, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006651357 tensor(3.1318, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0062179905 tensor(3.2046, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0059864004 tensor(3.2963, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005968023 tensor(3.3052, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0059508975 tensor(3.3142, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007718696 tensor(3.1487, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0062010395 tensor(3.1557, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056728534 tensor(3.2252, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054494473 tensor(3.3083, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0054332255 tensor(3.3179, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005416524 tensor(3.3281, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0076991357 tensor(3.1400, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061908737 tensor(3.2010, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0057916446 tensor(3.2892, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005556 tensor(3.3911, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0055371784 tensor(3.4011, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005519854 tensor(3.4117, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0076462612 tensor(3.1549, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0059616813 tensor(3.1588, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005626423 tensor(3.2172, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054162676 tensor(3.3086, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053983773 tensor(3.3177, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0053810594 tensor(3.3275, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008403219 tensor(3.1784, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0060961875 tensor(3.1797, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005646843 tensor(3.2466, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0053973515 tensor(3.3291, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053817667 tensor(3.3351, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0053666723 tensor(3.3414, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008919314 tensor(3.1886, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006430604 tensor(3.1745, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005919555 tensor(3.2215, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0056622853 tensor(3.2865, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005639467 tensor(3.2933, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0056189476 tensor(3.3010, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008680928 tensor(3.2136, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0063063796 tensor(3.2295, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005912766 tensor(3.2610, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057006357 tensor(3.3106, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0056838333 tensor(3.3166, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0056641474 tensor(3.3235, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007640188 tensor(3.1955, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006191344 tensor(3.2257, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005867433 tensor(3.2758, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057093557 tensor(3.3425, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005695434 tensor(3.3495, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0056811515 tensor(3.3573, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008500101 tensor(3.1527, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0065570525 tensor(3.1470, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0061844704 tensor(3.2181, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0059683225 tensor(3.3157, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0059510637 tensor(3.3251, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005934201 tensor(3.3356, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009595012 tensor(3.2628, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0068116765 tensor(3.2449, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0062515726 tensor(3.2570, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0059703286 tensor(3.3168, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.00594912 tensor(3.3231, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005928776 tensor(3.3303, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.00838287 tensor(3.1703, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005918378 tensor(3.2125, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0054671876 tensor(3.2926, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005200841 tensor(3.4008, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0051748166 tensor(3.4133, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0051507982 tensor(3.4261, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008481281 tensor(3.0990, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006492921 tensor(3.1095, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006037866 tensor(3.1833, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005810509 tensor(3.2653, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.0057919915 tensor(3.2736, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005773581 tensor(3.2827, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008929576 tensor(3.1081, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0075312797 tensor(3.1072, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0070575126 tensor(3.1825, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006728595 tensor(3.2858, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0067082355 tensor(3.2953, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0066883382 tensor(3.3057, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008518103 tensor(3.1289, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0067029214 tensor(3.1194, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0062985616 tensor(3.1710, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006103735 tensor(3.2522, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006086816 tensor(3.2610, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0060698045 tensor(3.2709, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009811864 tensor(3.1421, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007457374 tensor(3.1693, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0069872513 tensor(3.2233, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0066860826 tensor(3.3102, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0066598803 tensor(3.3186, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0066359057 tensor(3.3280, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007435495 tensor(3.1635, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005709818 tensor(3.1934, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005348824 tensor(3.2385, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005131119 tensor(3.3138, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005109719 tensor(3.3227, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0050915834 tensor(3.3311, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009726707 tensor(3.1753, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0071002445 tensor(3.2060, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006532678 tensor(3.2956, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0061991177 tensor(3.3999, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0061779497 tensor(3.4094, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0061577773 tensor(3.4203, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0089581655 tensor(3.1180, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006417187 tensor(3.1517, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005968535 tensor(3.2325, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057600886 tensor(3.3287, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0057434337 tensor(3.3371, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0057271007 tensor(3.3456, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008480133 tensor(3.1868, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006189268 tensor(3.2273, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0055514756 tensor(3.3099, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005305493 tensor(3.3885, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0052845934 tensor(3.3978, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0052650613 tensor(3.4075, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008387016 tensor(3.1100, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0059319474 tensor(3.1520, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005471998 tensor(3.2242, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005259403 tensor(3.3046, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005241523 tensor(3.3126, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005224021 tensor(3.3214, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0071320087 tensor(3.1106, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0056801485 tensor(3.0963, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005322357 tensor(3.1418, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0051257424 tensor(3.2051, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0051113465 tensor(3.2120, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005097821 tensor(3.2195, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0079963 tensor(3.1064, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061372365 tensor(3.1478, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056710974 tensor(3.2255, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005483812 tensor(3.3096, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005468664 tensor(3.3176, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005452795 tensor(3.3259, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.00893776 tensor(3.1325, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007199397 tensor(3.1605, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006737176 tensor(3.2408, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0064857192 tensor(3.3319, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0064666555 tensor(3.3409, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0064475047 tensor(3.3505, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008892006 tensor(3.1299, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0062960535 tensor(3.1498, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005701492 tensor(3.2067, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0053708875 tensor(3.3023, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053419764 tensor(3.3126, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0053169257 tensor(3.3232, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "At batch:30, Chamfer Loss:0.005291690584272146, Encoding Chamfer Loss:0.005292165093123913\n",
      "0 0.0064959484 tensor(3.1541, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0053814375 tensor(3.1404, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005109274 tensor(3.1818, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.004978674 tensor(3.2434, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.00496673 tensor(3.2509, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.004954829 tensor(3.2585, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008069825 tensor(3.1820, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006492702 tensor(3.1555, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006165563 tensor(3.1970, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005972309 tensor(3.2750, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0059552058 tensor(3.2846, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0059399214 tensor(3.2945, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008320291 tensor(3.1128, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0062251203 tensor(3.1630, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005737427 tensor(3.2386, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054866616 tensor(3.3230, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005467239 tensor(3.3332, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0054479893 tensor(3.3441, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008911664 tensor(3.1814, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0063914964 tensor(3.1933, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005936562 tensor(3.2228, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057173376 tensor(3.2815, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005696349 tensor(3.2867, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0056768754 tensor(3.2930, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007780401 tensor(3.1770, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0053660753 tensor(3.1961, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.004978878 tensor(3.2341, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0048078694 tensor(3.2918, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0047919853 tensor(3.2979, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0047756094 tensor(3.3054, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007148151 tensor(3.1990, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005583193 tensor(3.2119, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005246997 tensor(3.2761, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0050513945 tensor(3.3575, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005035163 tensor(3.3659, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0050198375 tensor(3.3745, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008317912 tensor(3.1865, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0062953834 tensor(3.1981, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.0059073158 tensor(3.2662, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0056890775 tensor(3.3479, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0056701973 tensor(3.3564, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0056521315 tensor(3.3655, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009324845 tensor(3.1899, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064241285 tensor(3.1910, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005946964 tensor(3.2334, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057177767 tensor(3.2831, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005699134 tensor(3.2892, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005679658 tensor(3.2957, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007810995 tensor(3.1521, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061094016 tensor(3.1472, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005630332 tensor(3.2187, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054397034 tensor(3.2906, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005422731 tensor(3.2975, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0054064794 tensor(3.3051, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010693456 tensor(3.1242, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0074027027 tensor(3.1249, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0066241487 tensor(3.1866, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0062834453 tensor(3.2521, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0062576225 tensor(3.2589, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006232829 tensor(3.2662, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008077262 tensor(3.1457, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006247958 tensor(3.1565, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0057918574 tensor(3.2169, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055778977 tensor(3.2901, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0055598193 tensor(3.2982, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0055416003 tensor(3.3068, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010279469 tensor(3.2029, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.008635914 tensor(3.1940, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.008072103 tensor(3.2697, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0077682603 tensor(3.3660, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0077412636 tensor(3.3750, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0077156764 tensor(3.3847, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007200122 tensor(3.1509, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005941896 tensor(3.1237, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005455317 tensor(3.1836, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0052468698 tensor(3.2584, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0052309236 tensor(3.2664, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0052149827 tensor(3.2752, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007046413 tensor(3.2107, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0057735257 tensor(3.1884, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0054164384 tensor(3.2435, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005254269 tensor(3.3220, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0052415435 tensor(3.3300, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005226611 tensor(3.3389, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007831785 tensor(3.1936, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0063896747 tensor(3.1925, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0059661088 tensor(3.2559, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057773665 tensor(3.3232, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0057618553 tensor(3.3304, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005746252 tensor(3.3384, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008423311 tensor(3.0761, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0068521723 tensor(3.1182, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006333403 tensor(3.2135, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0060691056 tensor(3.3391, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0060466686 tensor(3.3526, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006024858 tensor(3.3668, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010366077 tensor(3.1947, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0063418103 tensor(3.2093, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005831006 tensor(3.2295, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005612311 tensor(3.2610, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0055956594 tensor(3.2655, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005577841 tensor(3.2699, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0077702096 tensor(3.1638, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006122009 tensor(3.1516, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005679848 tensor(3.1914, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005441647 tensor(3.2696, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0054229926 tensor(3.2770, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0054035485 tensor(3.2852, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008471258 tensor(3.1142, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006590773 tensor(3.1416, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0059762956 tensor(3.2394, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057370733 tensor(3.3155, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005719908 tensor(3.3229, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005702768 tensor(3.3307, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.011526838 tensor(3.1665, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0075181746 tensor(3.2247, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0069249263 tensor(3.2889, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0066858176 tensor(3.3527, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006663903 tensor(3.3597, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00664303 tensor(3.3681, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008980453 tensor(3.2125, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006276253 tensor(3.2861, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005812276 tensor(3.3579, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055593126 tensor(3.4416, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0055337055 tensor(3.4488, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0055019837 tensor(3.4569, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008455876 tensor(3.1413, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064632646 tensor(3.1765, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0058937105 tensor(3.2647, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055999425 tensor(3.3596, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0055767256 tensor(3.3709, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0055549825 tensor(3.3826, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008234099 tensor(3.1887, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0062302006 tensor(3.1774, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005847679 tensor(3.2076, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0056066746 tensor(3.2897, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005585246 tensor(3.2984, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00556452 tensor(3.3073, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009921005 tensor(3.1422, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007196251 tensor(3.1506, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0065878737 tensor(3.2052, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0062858188 tensor(3.2745, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006257259 tensor(3.2827, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0062255026 tensor(3.2921, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009235979 tensor(3.1670, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0068953508 tensor(3.1811, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0062949667 tensor(3.2598, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006010522 tensor(3.3427, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0059879725 tensor(3.3511, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005966318 tensor(3.3601, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008313496 tensor(3.1474, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0060996204 tensor(3.1411, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.005597581 tensor(3.1983, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0053457166 tensor(3.2695, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053282785 tensor(3.2758, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005310865 tensor(3.2828, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008830138 tensor(3.1318, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007108544 tensor(3.1143, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0066543957 tensor(3.1423, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006435905 tensor(3.2082, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006418854 tensor(3.2155, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0064024064 tensor(3.2230, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009050413 tensor(3.1069, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005810829 tensor(3.0998, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0053099804 tensor(3.1636, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0051232246 tensor(3.2365, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005108313 tensor(3.2429, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0050940947 tensor(3.2502, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009445046 tensor(3.1950, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006622454 tensor(3.1975, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0061437786 tensor(3.2271, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0058838218 tensor(3.2912, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005862267 tensor(3.2992, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0058400156 tensor(3.3079, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008367809 tensor(3.1720, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0062677953 tensor(3.2029, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005868132 tensor(3.2681, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005645124 tensor(3.3512, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005626017 tensor(3.3599, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005606027 tensor(3.3699, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "At batch:60, Chamfer Loss:0.005583328660577536, Encoding Chamfer Loss:0.00558385020121932\n",
      "0 0.009808543 tensor(3.1660, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061329734 tensor(3.1819, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005530689 tensor(3.2413, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0052985908 tensor(3.2997, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005280015 tensor(3.3061, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005260021 tensor(3.3131, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007666674 tensor(3.1195, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0058601433 tensor(3.1408, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0055434 tensor(3.2248, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005395714 tensor(3.3059, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053826165 tensor(3.3146, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0053697573 tensor(3.3237, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0076670074 tensor(3.1491, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0056973062 tensor(3.1601, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005320978 tensor(3.2162, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005117908 tensor(3.3025, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005102234 tensor(3.3107, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005087791 tensor(3.3191, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007693999 tensor(3.1128, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0059975376 tensor(3.1080, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056277392 tensor(3.1424, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054360223 tensor(3.1961, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005418598 tensor(3.2026, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0054017794 tensor(3.2098, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008283709 tensor(3.2013, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006333429 tensor(3.1786, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005899374 tensor(3.2314, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0056540305 tensor(3.3152, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0056348178 tensor(3.3231, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0056163655 tensor(3.3322, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010066799 tensor(3.1147, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0076955385 tensor(3.1645, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.00716515 tensor(3.2332, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0067675416 tensor(3.3387, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006727625 tensor(3.3486, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0066587348 tensor(3.3585, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010287903 tensor(3.2125, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0068439916 tensor(3.2575, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0062037534 tensor(3.2983, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005991981 tensor(3.3408, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0059699514 tensor(3.3466, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005949424 tensor(3.3528, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008650894 tensor(3.2074, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061137024 tensor(3.2344, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056362255 tensor(3.2910, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054282662 tensor(3.3616, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0054138927 tensor(3.3680, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005399027 tensor(3.3749, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008328913 tensor(3.1713, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0062949625 tensor(3.1665, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005921342 tensor(3.1993, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005703229 tensor(3.2574, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0056867665 tensor(3.2640, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005671122 tensor(3.2711, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008533167 tensor(3.1910, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006364895 tensor(3.1904, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005904004 tensor(3.2388, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0056924196 tensor(3.3155, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005675494 tensor(3.3226, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0056578075 tensor(3.3309, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008231051 tensor(3.1989, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006310546 tensor(3.1461, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005907147 tensor(3.1795, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005735578 tensor(3.2395, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005722044 tensor(3.2459, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0057085142 tensor(3.2531, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009497838 tensor(3.1087, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007093411 tensor(3.0973, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0064953743 tensor(3.1543, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006190561 tensor(3.2505, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0061657876 tensor(3.2606, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006129488 tensor(3.2721, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0096695665 tensor(3.1634, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007396316 tensor(3.2090, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0067230933 tensor(3.2943, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0063922303 tensor(3.3902, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0063687116 tensor(3.4000, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0063448553 tensor(3.4108, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008597542 tensor(3.1972, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0066369334 tensor(3.2091, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006022991 tensor(3.3002, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057133655 tensor(3.4131, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005689323 tensor(3.4226, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005664764 tensor(3.4329, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.008920657 tensor(3.1621, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.00696095 tensor(3.2188, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006456427 tensor(3.3078, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006227441 tensor(3.3856, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0062106145 tensor(3.3925, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006194095 tensor(3.4004, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008863133 tensor(3.1265, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0065956605 tensor(3.1237, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0060301144 tensor(3.1643, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057180743 tensor(3.2395, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0056844708 tensor(3.2480, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005664442 tensor(3.2550, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0077097346 tensor(3.1478, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005999512 tensor(3.1532, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.00565479 tensor(3.2041, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054782247 tensor(3.2728, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0054642363 tensor(3.2795, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005448524 tensor(3.2871, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008759743 tensor(3.1049, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0067417556 tensor(3.0789, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0063014817 tensor(3.1128, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0060642073 tensor(3.1709, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006043309 tensor(3.1763, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006021043 tensor(3.1827, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0069797696 tensor(3.1902, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0055760993 tensor(3.2321, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005220655 tensor(3.3001, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0050305524 tensor(3.3664, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0050161337 tensor(3.3731, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0050023994 tensor(3.3800, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008668934 tensor(3.2444, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0067681 tensor(3.2693, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006258789 tensor(3.3251, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005965945 tensor(3.4127, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0059411936 tensor(3.4216, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0059149885 tensor(3.4322, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0076096714 tensor(3.0997, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061706984 tensor(3.1207, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0057974397 tensor(3.1771, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055800444 tensor(3.2551, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005558097 tensor(3.2629, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005538859 tensor(3.2707, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010896074 tensor(3.1528, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0066285273 tensor(3.2323, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006054154 tensor(3.3081, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005768494 tensor(3.3989, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0057453965 tensor(3.4070, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00572115 tensor(3.4159, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009293519 tensor(3.1583, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0068883523 tensor(3.1665, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0064259237 tensor(3.2080, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0062040207 tensor(3.2760, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006187217 tensor(3.2831, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0061709154 tensor(3.2903, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009711024 tensor(3.1933, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0073436247 tensor(3.2403, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0066820155 tensor(3.3239, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0063673127 tensor(3.3936, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0063365647 tensor(3.4029, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0063076625 tensor(3.4121, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008195499 tensor(3.1757, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006097485 tensor(3.2187, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005705186 tensor(3.2899, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054758 tensor(3.3824, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0054600653 tensor(3.3913, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0054442245 tensor(3.4012, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010029003 tensor(3.1829, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.00753769 tensor(3.2131, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006948881 tensor(3.2878, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0066506304 tensor(3.3601, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0066261594 tensor(3.3673, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006599996 tensor(3.3757, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0072109466 tensor(3.2292, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0057320003 tensor(3.2177, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005425641 tensor(3.2387, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005252613 tensor(3.3037, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005238346 tensor(3.3109, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00522438 tensor(3.3186, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0087746335 tensor(3.1788, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006712309 tensor(3.1880, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006265265 tensor(3.2500, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006046012 tensor(3.3305, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0060281344 tensor(3.3380, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00601094 tensor(3.3461, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0073094373 tensor(3.1536, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0057730135 tensor(3.1339, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005374108 tensor(3.2054, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0051628524 tensor(3.2953, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0051446445 tensor(3.3050, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0051271464 tensor(3.3156, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0075553395 tensor(3.1821, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0060740905 tensor(3.1677, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005679388 tensor(3.2245, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005481237 tensor(3.2941, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005460542 tensor(3.3013, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005441597 tensor(3.3087, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "At batch:90, Chamfer Loss:0.005424086004495621, Encoding Chamfer Loss:0.0054244245402514935\n",
      "0 0.008700658 tensor(3.1659, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006567575 tensor(3.1689, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005990136 tensor(3.2199, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005687995 tensor(3.2940, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005666219 tensor(3.3022, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0056446176 tensor(3.3110, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.006674521 tensor(3.1217, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005607682 tensor(3.1753, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0053141303 tensor(3.2710, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0051590954 tensor(3.3782, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0051462804 tensor(3.3905, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005133347 tensor(3.4033, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007860767 tensor(3.1931, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006109711 tensor(3.2296, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005448625 tensor(3.3227, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0052053872 tensor(3.4224, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.00519029 tensor(3.4297, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0051757316 tensor(3.4374, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0072665764 tensor(3.1733, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0058653406 tensor(3.1695, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0055492143 tensor(3.2230, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0053625684 tensor(3.3072, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005347906 tensor(3.3161, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0053333254 tensor(3.3253, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0075794156 tensor(3.2020, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0058167763 tensor(3.2284, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0054079457 tensor(3.2822, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005222482 tensor(3.3496, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005208645 tensor(3.3566, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005194139 tensor(3.3634, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009670165 tensor(3.1811, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007822375 tensor(3.2170, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0072945044 tensor(3.2985, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0069073844 tensor(3.3839, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006875553 tensor(3.3915, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0068389205 tensor(3.4015, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008231866 tensor(3.1146, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005865962 tensor(3.1416, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0054451106 tensor(3.1872, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0052401316 tensor(3.2543, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0052244407 tensor(3.2605, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0052086287 tensor(3.2673, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009712644 tensor(3.1967, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0070169307 tensor(3.2292, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0063585076 tensor(3.2941, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0060666534 tensor(3.3589, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0060464507 tensor(3.3639, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0060266447 tensor(3.3696, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009362116 tensor(3.2309, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064647426 tensor(3.2868, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0058734557 tensor(3.3449, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005582237 tensor(3.4173, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005558249 tensor(3.4250, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0055340906 tensor(3.4330, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "adnet.eval()\n",
    "\n",
    "for batch_idx, (X, y, theta, _, idx) in enumerate(data_loader_lf_test):\n",
    "    X, y, theta = X.cuda(), y.cuda(), theta.cuda()\n",
    "    X, y, theta = Variable(X).float(), Variable(y).float(), Variable(theta).float()\n",
    "    loss_encoding, generated_encoding = find_encoding(X, y, adnet, \n",
    "                                             encoding_iters=300, \n",
    "                                             encoding_size=256)\n",
    "    \n",
    "    y_pred = adnet(X, generated_encoding)\n",
    "    loss_cham = chamfer_loss(y, y_pred, ps=y.shape[-1])\n",
    "    if batch_idx % 30 == 0:\n",
    "        print(f\"At batch:{batch_idx}, Chamfer Loss:{loss_cham}, Encoding Chamfer Loss:{loss_encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our Model with PointNet Data\n",
    "\n",
    "## Load PointNet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10817, 2048, 3), (10817, 1), (2507, 2048, 3), (2507, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn_train_data   = np.load(\"./data/PointNetData/train/pntcloud_7.npy\")\n",
    "pn_train_labels = np.load(\"./data/PointNetData/train/label_7.npy\")\n",
    "pn_test_data    = np.load(\"./data/PointNetData/test/pntcloud_7.npy\")\n",
    "pn_test_labels  = np.load(\"./data/PointNetData/test/label_7.npy\")\n",
    "\n",
    "pn_train_data.shape, pn_train_labels.shape, pn_test_data.shape, pn_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_train_full        = np.load(\"./data/PointNetData/train/pntcloud_full.npy\")\n",
    "pn_train_full_labels = np.load(\"./data/PointNetData/train/label_full.npy\")\n",
    "pn_test_full         = np.load(\"./data/PointNetData/test/pntcloud_full.npy\")\n",
    "pn_test_full_labels  = np.load(\"./data/PointNetData/test/label_full.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12137, 2048, 3), (12137, 1), (2874, 2048, 3), (2874, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn_train_full.shape, pn_train_full_labels.shape, pn_test_full.shape, pn_test_full_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_train_ds = PointDriftDS(pn_train_data, pn_train_labels)\n",
    "pn_test_ds = PointDriftDS(pn_test_data, pn_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train AutoDecoder\n",
    "\n",
    "**Note:** Training takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l2_reg: None\n",
       "learning_rate: 0.001\n",
       "encoding_size: 256\n",
       "encoding_iters: 50\n",
       "num_point_cloud: 3\n",
       "epochs: 4\n",
       "batch_size: 32"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adnet_HP = HyperParameter(lr=0.001)\n",
    "adnet_HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) in loading state_dict for AutoDecoder:\n",
      "\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"module.fc1.weight\", \"module.fc1.bias\", \"module.fc2.weight\", \"module.fc2.bias\", \"module.fc3.weight\", \"module.fc3.bias\". \n",
      "Epoch: 0. batch_idx: 100\n",
      "Loss:  0.02694659827277064 0.07462658740580082\n",
      "Epoch: 0. batch_idx: 200\n",
      "Loss:  0.027464010566473008 0.06767778392881155\n",
      "Epoch: 0. batch_idx: 300\n",
      "Loss:  0.02799369568005204 0.06513361304998398\n",
      "0 0.032697514 tensor(7.1102, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060688097 tensor(7.1478, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.021846473 tensor(7.1961, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064103164 tensor(7.1548, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03029972 tensor(7.1403, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05798519 tensor(6.9900, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025432939 tensor(7.1767, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06382163 tensor(7.1106, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02478944 tensor(7.1194, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064129665 tensor(7.1030, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.020940114 tensor(7.1185, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062339876 tensor(7.0776, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028311515 tensor(7.1934, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06549791 tensor(7.0790, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034216512 tensor(7.1326, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06302952 tensor(7.0566, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035796516 tensor(7.1704, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06444051 tensor(7.1276, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035251856 tensor(7.1840, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049532928 tensor(7.1564, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033319123 tensor(7.0846, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.074536346 tensor(7.2028, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029603213 tensor(7.0790, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064003736 tensor(7.1265, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025430327 tensor(7.0935, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.065201946 tensor(7.0703, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03403511 tensor(7.0807, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.069014266 tensor(7.1310, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026348112 tensor(7.1231, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063850515 tensor(7.0667, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028415121 tensor(7.0696, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046138644 tensor(7.0714, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028752163 tensor(7.0862, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06948053 tensor(7.0415, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.020976571 tensor(7.1775, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062547855 tensor(7.0594, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02744719 tensor(7.1538, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07009727 tensor(7.1002, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025294771 tensor(7.1449, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059864976 tensor(7.1102, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024405794 tensor(7.0667, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06379888 tensor(7.0453, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025702989 tensor(7.1021, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057699565 tensor(7.0112, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03074359 tensor(7.1591, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061316606 tensor(7.1553, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031669047 tensor(7.1025, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.072806455 tensor(7.1202, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031769156 tensor(7.0952, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06704954 tensor(7.1324, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031635273 tensor(7.1361, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060212903 tensor(7.1237, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030186431 tensor(7.1776, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06829704 tensor(7.1001, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030613085 tensor(7.0878, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06459667 tensor(7.0946, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026639087 tensor(7.1948, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06561491 tensor(7.1223, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.023260247 tensor(7.0970, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.066506624 tensor(7.1062, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025706261 tensor(7.0111, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.067293294 tensor(7.1012, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028653773 tensor(7.1292, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062343802 tensor(7.0957, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03056896 tensor(7.1445, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054655135 tensor(6.9882, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025621701 tensor(7.1389, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063421525 tensor(7.1728, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03076328 tensor(7.2502, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06971144 tensor(7.0709, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04266357 tensor(7.1658, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.065730356 tensor(7.0596, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02912917 tensor(7.2127, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07446299 tensor(7.0104, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028760746 tensor(7.0486, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06221167 tensor(7.0592, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031706244 tensor(7.1978, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05487912 tensor(7.0735, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024479048 tensor(7.0759, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06985849 tensor(7.1993, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.022962777 tensor(7.1569, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05756311 tensor(7.1393, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027632885 tensor(7.1777, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.072422825 tensor(7.1046, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.021762175 tensor(7.0556, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06453002 tensor(7.1144, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036592428 tensor(7.1299, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05609573 tensor(7.0664, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028610466 tensor(7.0175, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05755265 tensor(7.1177, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0345945 tensor(7.1950, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061966646 tensor(7.1655, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024424473 tensor(7.1937, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.068084605 tensor(7.0294, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.022818038 tensor(7.1303, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061468564 tensor(7.0915, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025498973 tensor(7.0163, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060226016 tensor(7.1626, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025084203 tensor(7.1708, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0619737 tensor(7.0862, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028990118 tensor(7.1533, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0652281 tensor(7.0846, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024767304 tensor(7.0653, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06439307 tensor(7.1266, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030901644 tensor(7.1828, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064454295 tensor(7.0795, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026369667 tensor(7.0798, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08713947 tensor(7.1138, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044384684 tensor(7.2011, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049829006 tensor(7.1679, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.024375262 tensor(7.2185, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.073100045 tensor(7.1052, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027965395 tensor(7.1525, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.076703556 tensor(7.0231, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035414733 tensor(7.1703, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05669683 tensor(7.0978, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0392856 tensor(7.0838, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063177064 tensor(7.0725, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03655006 tensor(7.0806, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060560666 tensor(7.1087, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032653697 tensor(7.1649, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05973863 tensor(7.0466, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025305294 tensor(7.1787, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055644214 tensor(7.1856, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034567885 tensor(7.1377, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051563136 tensor(7.1622, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028915148 tensor(7.0613, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07054229 tensor(7.0676, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025320698 tensor(7.1795, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06680349 tensor(7.0380, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02555592 tensor(7.1315, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07645038 tensor(6.9874, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025297921 tensor(7.1642, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051857665 tensor(7.1762, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027400864 tensor(7.0671, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05261714 tensor(7.0406, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02319597 tensor(7.1157, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06549585 tensor(7.1165, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034525957 tensor(7.1799, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0820353 tensor(7.1735, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036174987 tensor(7.1664, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06089165 tensor(7.1908, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.021775842 tensor(7.1643, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07313146 tensor(7.0949, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029242944 tensor(7.1074, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06376494 tensor(6.9832, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033324607 tensor(7.2293, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056083947 tensor(7.0474, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025667183 tensor(7.0710, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06967147 tensor(7.1293, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031437907 tensor(7.1262, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063812144 tensor(7.0584, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031606924 tensor(7.1679, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057058934 tensor(7.1205, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032545354 tensor(7.0402, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052716266 tensor(7.1323, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041234203 tensor(4.1761, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046433598 tensor(4.1380, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "Encodings trained\n",
      "Eval:  (0.014707950509707385, 0.03860218844176093)\n",
      "Epoch: 1. batch_idx: 100\n",
      "Loss:  0.02372874453663826 0.05269677806645632\n",
      "Epoch: 1. batch_idx: 200\n",
      "Loss:  0.02309388667345047 0.04554437883198261\n",
      "Epoch: 1. batch_idx: 300\n",
      "Loss:  0.022707213098183274 0.0422152541950345\n",
      "Epoch: 2. batch_idx: 100\n",
      "Loss:  0.017691771695390344 0.03050260081887245\n",
      "Epoch: 2. batch_idx: 200\n",
      "Loss:  0.01631910881958902 0.0279226715862751\n",
      "Epoch: 2. batch_idx: 300\n",
      "Loss:  0.016227207574993373 0.02757173163816333\n",
      "Epoch: 3. batch_idx: 100\n",
      "Loss:  0.012741056336089969 0.020056414818391204\n",
      "Epoch: 3. batch_idx: 200\n",
      "Loss:  0.012386371055617929 0.01843764998950064\n",
      "Epoch: 3. batch_idx: 300\n",
      "Loss:  0.012197491694241763 0.01844743836671114\n"
     ]
    }
   ],
   "source": [
    "pn_autodecoder = None\n",
    "load_saved_weight = True\n",
    "\n",
    "if load_saved_weight:\n",
    "    try:\n",
    "        pn_autodecoder = AutoDecoder(HP.encoding_size, HP.num_point_cloud)\n",
    "        pn_autodecoder.load_state_dict(torch.load(DS.AUTODECODER_TRAINED_WEIGHT_DIR+'/decoder.pth'))\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        pn_autodecoder = train_decoder(adnet_HP, DS, train_ds=pn_train_ds, test_ds=pn_test_ds)\n",
    "else:\n",
    "    pn_autodecoder = train_decoder(adnet_HP, DS, train_ds=pn_train_ds, test_ds=pn_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pn_train_data', 265838720),\n",
       " ('pn_test_data', 61612160),\n",
       " ('idx', 20152),\n",
       " ('pn_train_labels', 10929),\n",
       " ('pn_test_labels', 2619),\n",
       " ('DataLoader', 1464),\n",
       " ('AutoDecoder', 1184),\n",
       " ('CompNet', 1184),\n",
       " ('EnsembleCompNet', 1184),\n",
       " ('HyperParameter', 1184),\n",
       " ('Dataset', 1056),\n",
       " ('DirectorySetting', 1056),\n",
       " ('EncodingDS', 1056),\n",
       " ('EncodingDataset', 1056),\n",
       " ('GenericDS', 1056),\n",
       " ('PointDriftDS', 1056),\n",
       " ('PointNetDS', 1056),\n",
       " ('Variable', 1056),\n",
       " ('diff_idx', 192),\n",
       " ('same_idx', 192),\n",
       " ('chamfer_loss', 136),\n",
       " ('classify', 136),\n",
       " ('eval_compnet', 136),\n",
       " ('eval_decoder', 136),\n",
       " ('find_encoding', 136),\n",
       " ('its_fucking_raw', 136),\n",
       " ('test_nn_modules', 136),\n",
       " ('train_compnet', 136),\n",
       " ('train_decoder', 136),\n",
       " ('d', 96),\n",
       " ('labels', 96),\n",
       " ('F', 80),\n",
       " ('cudnn', 80),\n",
       " ('nn', 80),\n",
       " ('np', 80),\n",
       " ('optim', 80),\n",
       " ('plt', 80),\n",
       " ('test_result', 80),\n",
       " ('train_result', 80),\n",
       " ('X', 72),\n",
       " ('a', 72),\n",
       " ('aa', 72),\n",
       " ('cls_samples', 72),\n",
       " ('x', 72),\n",
       " ('diff_cls', 64),\n",
       " ('same_cls', 64),\n",
       " ('DS', 56),\n",
       " ('HP', 56),\n",
       " ('adnet_HP', 56),\n",
       " ('compnet1', 56),\n",
       " ('cpnet_HP', 56),\n",
       " ('dl1', 56),\n",
       " ('dl2', 56),\n",
       " ('gds', 56),\n",
       " ('pn_autodecoder', 56),\n",
       " ('pn_test_ds', 56),\n",
       " ('pn_train_ds', 56),\n",
       " ('test_encoding_ds', 56),\n",
       " ('train_encoding_ds', 56),\n",
       " ('DEBUG', 28),\n",
       " ('SEED', 28),\n",
       " ('i', 28),\n",
       " ('load_saved_weight', 28),\n",
       " ('use_cuda', 28),\n",
       " ('device', 24)]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Get memory usage of active variables \"\"\"\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.04245077 tensor(5.0333, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0559642 tensor(5.0682, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029363818 tensor(5.0582, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054658405 tensor(5.0524, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02842458 tensor(5.0882, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057541903 tensor(5.0220, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025425551 tensor(5.0640, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061649907 tensor(5.0695, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03431777 tensor(4.9510, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061498683 tensor(5.0661, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03070212 tensor(5.0931, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053091783 tensor(5.0606, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030673219 tensor(4.9287, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06652331 tensor(5.0520, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03247469 tensor(5.0560, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05442151 tensor(5.0631, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028690657 tensor(4.9932, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044282652 tensor(4.9645, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034010604 tensor(4.9833, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06511389 tensor(5.0291, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030585706 tensor(5.1052, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04934746 tensor(5.1237, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.019106096 tensor(5.0534, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07404935 tensor(4.9668, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0361794 tensor(5.0342, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06814306 tensor(5.0995, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033341084 tensor(5.0481, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.077435635 tensor(5.0896, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036929812 tensor(5.0562, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06945618 tensor(5.0447, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044036627 tensor(5.0436, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061246756 tensor(5.1131, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044050924 tensor(5.0627, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051787123 tensor(5.0558, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043062955 tensor(4.9840, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07492894 tensor(5.0146, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029813463 tensor(4.9235, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050394922 tensor(5.0448, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044345174 tensor(5.1329, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043457825 tensor(5.0956, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042057663 tensor(5.0247, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.076343015 tensor(5.0798, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034876794 tensor(4.9969, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07235691 tensor(5.0406, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030124776 tensor(5.0305, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057075407 tensor(5.0536, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042732466 tensor(4.9811, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05747979 tensor(5.0534, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032015424 tensor(4.9888, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06442264 tensor(5.0580, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03393224 tensor(5.1533, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061427094 tensor(5.0722, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04047452 tensor(4.9789, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.073106006 tensor(5.0945, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041661385 tensor(5.0992, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06355704 tensor(5.0445, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031874254 tensor(5.0340, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06414236 tensor(5.0566, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037476216 tensor(4.9984, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06726225 tensor(4.9658, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03229481 tensor(5.0542, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04335721 tensor(4.9410, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03343932 tensor(5.0542, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042805973 tensor(5.1193, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031755775 tensor(4.9656, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06697785 tensor(5.0550, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0365025 tensor(5.0419, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06444125 tensor(5.0181, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026923832 tensor(5.0784, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05014067 tensor(5.1270, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.020391764 tensor(4.9532, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049938284 tensor(5.0184, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036873627 tensor(4.9899, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05489707 tensor(5.0810, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043037828 tensor(5.0380, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0752007 tensor(4.9553, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028976345 tensor(5.0220, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057986487 tensor(4.9440, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033242583 tensor(5.0291, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0653545 tensor(5.0108, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03598971 tensor(4.9934, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05219817 tensor(5.0888, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030829003 tensor(4.9664, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0635944 tensor(5.0794, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035721052 tensor(4.9600, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05085339 tensor(5.0717, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025652664 tensor(5.0569, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07050648 tensor(5.0062, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031130012 tensor(5.0756, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057235923 tensor(5.0852, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03806743 tensor(5.0868, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06240687 tensor(5.1005, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042984918 tensor(4.9702, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06975878 tensor(5.0145, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0363897 tensor(5.0322, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05896975 tensor(5.1641, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028963571 tensor(5.1125, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06326037 tensor(5.0074, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041229162 tensor(5.0643, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06590679 tensor(5.0476, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03605005 tensor(5.0288, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06274947 tensor(5.0587, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03585067 tensor(4.9866, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05857766 tensor(5.0721, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042206712 tensor(5.0582, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06994976 tensor(5.0126, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035543103 tensor(5.0885, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.072474554 tensor(4.9681, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043156743 tensor(5.1038, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053790197 tensor(5.1454, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037825946 tensor(5.0896, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047784653 tensor(5.0234, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036706373 tensor(5.0298, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0707404 tensor(5.1066, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036656924 tensor(5.1361, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060520954 tensor(5.0100, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029289955 tensor(5.0385, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.05921052 tensor(5.1334, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030303774 tensor(5.0341, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06480897 tensor(5.0151, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027073935 tensor(5.0470, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058161903 tensor(5.0387, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035327677 tensor(5.0665, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057656344 tensor(5.0362, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029650614 tensor(4.9892, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.071847275 tensor(5.0079, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043887194 tensor(5.0918, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053608917 tensor(5.0921, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028803578 tensor(5.0116, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04267385 tensor(5.2007, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03195124 tensor(4.9870, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056514855 tensor(5.0822, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032880947 tensor(4.9071, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08310451 tensor(5.0292, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03772969 tensor(5.0499, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05692786 tensor(5.0349, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052170817 tensor(5.0157, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0556527 tensor(5.0165, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033146992 tensor(5.0736, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06426032 tensor(5.0537, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06847822 tensor(5.0744, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05967475 tensor(5.0661, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042277597 tensor(5.0039, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06376784 tensor(5.0438, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043260135 tensor(5.0049, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.075885445 tensor(5.0553, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024523051 tensor(5.1378, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064956546 tensor(5.0518, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032004897 tensor(4.9647, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04596936 tensor(5.0211, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032286365 tensor(5.0490, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07266719 tensor(5.0469, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029618869 tensor(5.0747, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05339506 tensor(5.0592, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04477809 tensor(5.0549, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05058829 tensor(5.0046, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030526286 tensor(5.0031, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07329901 tensor(5.1121, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029222708 tensor(5.1240, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06429645 tensor(5.0682, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035497647 tensor(5.0557, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06873003 tensor(5.0857, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0287207 tensor(5.0400, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0424642 tensor(5.0894, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029846236 tensor(5.0754, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07242748 tensor(4.9074, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035793755 tensor(5.0540, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.075077415 tensor(5.0621, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026426759 tensor(5.0337, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07022346 tensor(5.0171, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02727799 tensor(5.0420, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06516894 tensor(4.9948, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044093512 tensor(5.0041, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052726742 tensor(5.0698, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040381186 tensor(5.0096, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046897195 tensor(5.0358, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024387479 tensor(5.0385, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07010516 tensor(5.0146, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045215093 tensor(5.0343, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047130853 tensor(5.0322, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034629405 tensor(5.0618, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05319035 tensor(5.1349, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038588624 tensor(5.0508, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052913856 tensor(5.0246, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04166126 tensor(4.9816, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06456963 tensor(5.1237, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024640286 tensor(5.0549, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061819203 tensor(4.9875, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025935834 tensor(5.0027, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053649634 tensor(4.9627, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026945513 tensor(5.1019, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06881609 tensor(5.0571, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038911022 tensor(5.0394, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051742252 tensor(5.1671, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02652801 tensor(5.0880, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.067300275 tensor(5.0258, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04831965 tensor(5.0097, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051473677 tensor(5.0055, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025629297 tensor(5.0436, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05962505 tensor(5.1206, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031520206 tensor(5.0535, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056453977 tensor(5.0801, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048983373 tensor(5.0947, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06279384 tensor(5.0240, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02768262 tensor(5.0579, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06271143 tensor(5.0835, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036686357 tensor(5.0010, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06388694 tensor(4.9293, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035420977 tensor(5.0173, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058943212 tensor(5.0746, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03867669 tensor(5.0222, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052544214 tensor(5.0889, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030304933 tensor(5.1139, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.088385835 tensor(5.0810, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034739237 tensor(5.0588, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06692512 tensor(4.9884, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059542794 tensor(5.0108, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046761367 tensor(5.0380, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056499973 tensor(4.9798, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051568218 tensor(5.0185, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03197424 tensor(5.0709, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.074591115 tensor(5.0716, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0362458 tensor(4.9409, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.066021785 tensor(5.0823, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02936725 tensor(5.0600, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07596123 tensor(5.0470, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0467121 tensor(5.0404, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07318032 tensor(5.0540, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045379136 tensor(5.0494, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053210437 tensor(5.0798, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035883218 tensor(4.9857, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04982846 tensor(5.0377, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03578267 tensor(4.9827, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054987237 tensor(5.0970, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.05088862 tensor(5.0270, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05899048 tensor(5.1139, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027993439 tensor(5.0518, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055730727 tensor(5.0632, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05837634 tensor(5.0183, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053183794 tensor(5.0969, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03027418 tensor(4.9910, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05655142 tensor(5.0248, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041968122 tensor(4.9824, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056949988 tensor(5.0882, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040239304 tensor(4.9939, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05088125 tensor(4.9821, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03179521 tensor(5.0045, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0698159 tensor(5.0462, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051831514 tensor(5.0092, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057602365 tensor(5.0407, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033308994 tensor(5.0366, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052682992 tensor(5.0685, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032405864 tensor(5.0735, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08812879 tensor(4.9858, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031670872 tensor(5.1236, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05406859 tensor(5.0506, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03517993 tensor(5.0810, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07206512 tensor(5.0312, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032394856 tensor(5.0006, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048578054 tensor(5.0952, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0421272 tensor(4.9993, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04881234 tensor(5.0953, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028962975 tensor(4.9754, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.091661654 tensor(5.0349, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03983559 tensor(5.0284, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054268453 tensor(5.0383, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.022837084 tensor(4.9923, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056194812 tensor(5.0432, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033128425 tensor(5.0295, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049826257 tensor(5.0585, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03792643 tensor(4.9688, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05500465 tensor(5.0042, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027231755 tensor(5.1041, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06515433 tensor(5.0455, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029431758 tensor(5.0802, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059637938 tensor(5.0115, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044322304 tensor(4.9993, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08415527 tensor(5.1405, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040374972 tensor(5.0906, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06688184 tensor(5.0669, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03281199 tensor(5.0439, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062469315 tensor(5.1097, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05991726 tensor(5.0795, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05588143 tensor(5.0520, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029526707 tensor(5.0075, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.078150555 tensor(5.2109, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024889078 tensor(5.0388, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06580091 tensor(5.0774, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04038478 tensor(5.0082, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056652065 tensor(5.1105, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045375716 tensor(5.0257, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053191524 tensor(5.0597, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055274427 tensor(5.0428, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047839098 tensor(5.0501, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033897027 tensor(5.0281, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055107057 tensor(4.9929, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03612726 tensor(4.9210, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07699567 tensor(5.0637, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031664167 tensor(5.1769, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0784642 tensor(5.0131, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037409928 tensor(4.9485, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07387805 tensor(5.0886, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03556833 tensor(5.0271, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050217807 tensor(5.0672, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04809324 tensor(5.0154, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05227991 tensor(5.0873, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035039887 tensor(5.0141, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05098502 tensor(5.0821, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041352484 tensor(4.9456, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057119455 tensor(5.0815, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03309447 tensor(4.9954, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043884024 tensor(4.9865, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041641943 tensor(4.1540, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040404797 tensor(4.1653, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "Encodings trained\n",
      "(0.01386112673504717, 0.020280321454925903)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Decoder\n",
    "print(eval_decoder(pn_autodecoder, pn_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode pairs in dataset and train Encodings with trained AutoDecoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.032529186 tensor(5.0401, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050932795 tensor(5.1038, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034167647 tensor(4.9956, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07213158 tensor(4.9627, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038839873 tensor(5.0723, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06123399 tensor(5.0462, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045951586 tensor(5.0231, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053147204 tensor(5.1215, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028430967 tensor(5.1001, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06833577 tensor(5.0784, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038768623 tensor(4.8984, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06709451 tensor(4.9809, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036277864 tensor(5.0551, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06888675 tensor(5.0901, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039128523 tensor(5.0074, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04790336 tensor(5.0783, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038681958 tensor(5.0588, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04773097 tensor(5.1311, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036683585 tensor(4.9475, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05795307 tensor(5.1537, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03139364 tensor(5.0664, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.066470556 tensor(5.0537, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027673671 tensor(5.0612, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053913318 tensor(5.0769, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031758275 tensor(5.0079, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049185272 tensor(5.0882, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027110936 tensor(5.0598, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054259617 tensor(5.0671, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045545407 tensor(5.0860, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05512098 tensor(5.1272, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034806125 tensor(5.0250, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0711519 tensor(5.1280, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032563593 tensor(5.0095, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046306744 tensor(5.2420, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040057953 tensor(5.0455, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051034696 tensor(5.0807, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04442415 tensor(5.0471, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.075947955 tensor(5.0738, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04404093 tensor(5.0779, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07146959 tensor(5.0370, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02784131 tensor(5.0249, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061547197 tensor(5.0751, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03055629 tensor(5.0016, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058376428 tensor(5.1752, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035811055 tensor(5.0274, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06613996 tensor(5.0660, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030013125 tensor(5.0356, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06205423 tensor(5.1337, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04632799 tensor(5.0289, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05064286 tensor(5.0807, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0347886 tensor(5.0868, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060050417 tensor(5.0337, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036796793 tensor(5.0730, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045376427 tensor(5.1631, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052267697 tensor(5.0893, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.065239444 tensor(4.9760, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058121253 tensor(4.9731, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050589297 tensor(5.0878, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025307233 tensor(5.0627, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0613205 tensor(5.0352, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037597366 tensor(5.0476, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04735838 tensor(5.1416, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028515238 tensor(5.0288, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06626152 tensor(5.0284, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034859635 tensor(5.0266, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06566354 tensor(5.0753, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037228785 tensor(5.0501, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06532238 tensor(5.0730, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028803995 tensor(5.0371, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049910225 tensor(5.0547, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037117787 tensor(5.0165, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06789557 tensor(5.0158, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032561228 tensor(4.9850, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.067968704 tensor(5.0652, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030158147 tensor(4.9875, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047268976 tensor(5.0892, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03092012 tensor(5.0085, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06419873 tensor(4.9892, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028143317 tensor(5.0625, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08099474 tensor(5.0486, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0371059 tensor(5.0310, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.067015074 tensor(5.0275, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028925776 tensor(5.0099, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07323479 tensor(5.0580, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03151786 tensor(5.0927, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06542244 tensor(5.1177, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036159188 tensor(5.0717, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05461974 tensor(5.1403, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04423044 tensor(4.9662, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057717223 tensor(5.0805, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03975878 tensor(5.0458, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05916561 tensor(5.0317, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043323588 tensor(5.0360, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060927574 tensor(4.9483, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0280435 tensor(5.0141, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04797366 tensor(5.1748, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03177868 tensor(4.9882, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060985614 tensor(5.1789, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036678366 tensor(4.9994, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07412513 tensor(5.0093, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03560796 tensor(5.0715, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058435913 tensor(5.1003, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027894866 tensor(5.0300, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055993825 tensor(5.1108, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04322662 tensor(5.0153, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060716055 tensor(4.9627, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031703386 tensor(5.0379, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0509771 tensor(5.0650, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037079528 tensor(5.0859, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06963122 tensor(4.9959, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046054933 tensor(5.0373, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054783408 tensor(5.0566, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04971882 tensor(5.0658, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052881747 tensor(5.0624, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024660213 tensor(5.0836, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07056551 tensor(5.1015, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04343385 tensor(5.0319, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06081442 tensor(5.0954, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.038022753 tensor(5.0863, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0742639 tensor(5.0312, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.022308918 tensor(5.0297, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055431984 tensor(5.0756, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03234293 tensor(5.0142, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0762884 tensor(5.0159, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046983767 tensor(5.0803, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044718765 tensor(5.0586, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026398914 tensor(5.0212, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047207598 tensor(5.0613, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036190324 tensor(5.0857, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05543442 tensor(4.9396, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027880248 tensor(4.9841, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060168736 tensor(5.0461, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036063977 tensor(5.0257, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058724456 tensor(5.0419, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033283178 tensor(5.0236, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05730683 tensor(5.0217, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03690471 tensor(5.0894, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.070461035 tensor(4.9963, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038418222 tensor(5.0068, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06412817 tensor(5.0365, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027247805 tensor(5.0283, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06444848 tensor(5.0699, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03375068 tensor(5.0580, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050729655 tensor(5.0453, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041002423 tensor(4.9093, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055629954 tensor(5.0662, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04192751 tensor(5.0681, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04573773 tensor(5.1594, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031235542 tensor(5.1002, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06455906 tensor(5.0119, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027082976 tensor(5.0274, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05081734 tensor(5.0421, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030126723 tensor(5.0480, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0545971 tensor(5.0733, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039709195 tensor(5.0454, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063134804 tensor(5.0217, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036656816 tensor(5.0343, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06567392 tensor(5.0701, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052535765 tensor(5.0857, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06921582 tensor(5.0552, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041294873 tensor(5.1250, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06176087 tensor(5.0537, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024667308 tensor(5.1073, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0625022 tensor(5.0267, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024706258 tensor(5.0632, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0490968 tensor(5.1387, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038285248 tensor(5.0252, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.067549035 tensor(5.0047, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.023705853 tensor(5.0709, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.066272594 tensor(5.0815, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036831733 tensor(5.0408, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059534326 tensor(5.0039, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029960155 tensor(5.0664, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048185904 tensor(5.0335, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036568917 tensor(5.0353, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06312283 tensor(5.0102, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028870204 tensor(5.0083, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07986021 tensor(5.0948, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040505428 tensor(5.0394, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05367922 tensor(5.0415, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045041956 tensor(4.9887, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059425116 tensor(5.0465, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030543493 tensor(4.9608, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059470538 tensor(5.0313, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04464429 tensor(5.0218, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05357428 tensor(5.0506, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04238687 tensor(5.0012, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06310734 tensor(4.9960, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0374499 tensor(5.0382, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048972268 tensor(5.0820, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031268455 tensor(5.0515, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053193763 tensor(5.0185, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027077325 tensor(5.0476, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061894327 tensor(4.9704, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029288167 tensor(5.0598, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060155973 tensor(5.1114, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04105294 tensor(5.0696, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062291097 tensor(4.9888, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041374158 tensor(5.0199, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06281695 tensor(5.0349, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040170122 tensor(5.0001, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.073611334 tensor(5.1002, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037409585 tensor(5.1782, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045606606 tensor(5.0621, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034980502 tensor(4.9635, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050128084 tensor(5.0858, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029886171 tensor(5.0402, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049497075 tensor(5.0610, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033526354 tensor(5.0234, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.071284294 tensor(4.9556, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04018501 tensor(5.0521, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056760706 tensor(5.0383, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045845237 tensor(4.9703, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06545971 tensor(5.1324, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05977184 tensor(5.0733, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045862537 tensor(5.1373, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029470414 tensor(4.9925, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05465703 tensor(5.0457, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037559025 tensor(5.0893, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0638381 tensor(4.9431, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027576257 tensor(5.1183, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05385761 tensor(5.0880, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03555308 tensor(5.0295, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053685714 tensor(5.0915, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034086112 tensor(5.0412, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05433276 tensor(5.1169, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037955254 tensor(5.0247, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06195715 tensor(5.0128, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05063532 tensor(5.1379, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040797044 tensor(5.0377, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048301734 tensor(5.0445, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060869288 tensor(5.0085, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049113598 tensor(5.0355, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06632696 tensor(5.0591, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03566414 tensor(5.0357, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051257815 tensor(5.0469, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.029002566 tensor(5.0584, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048890233 tensor(5.1368, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028203465 tensor(5.0491, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050233576 tensor(5.0009, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039719246 tensor(4.9343, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046281416 tensor(5.0586, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035832535 tensor(5.1237, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060002707 tensor(5.1007, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041172363 tensor(5.0880, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.075259864 tensor(5.0449, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029564457 tensor(5.0037, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047360476 tensor(5.0415, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0368851 tensor(5.0331, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054960158 tensor(5.0800, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034234893 tensor(5.0145, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05428361 tensor(5.0368, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032627277 tensor(5.0684, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06660475 tensor(5.0584, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030583948 tensor(5.0782, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05464988 tensor(5.0516, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047832306 tensor(5.0911, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05443014 tensor(4.9995, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03115551 tensor(5.1030, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059542872 tensor(5.0192, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032472886 tensor(5.0273, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07047185 tensor(5.0050, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040240142 tensor(5.0480, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058194168 tensor(5.0747, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03340684 tensor(5.0637, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060465008 tensor(5.0968, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030950604 tensor(5.1302, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05962261 tensor(5.1144, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032505464 tensor(5.0333, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059179515 tensor(5.1143, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042984307 tensor(5.0482, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07403293 tensor(5.0950, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026646845 tensor(5.1191, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05445348 tensor(5.1323, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038912725 tensor(5.0527, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055663627 tensor(5.0821, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032008126 tensor(5.1488, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051793367 tensor(5.0858, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.023750791 tensor(5.0942, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05746063 tensor(5.0804, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030413 tensor(4.9892, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06513666 tensor(5.0524, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.023062283 tensor(5.0977, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04548474 tensor(5.0425, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032598414 tensor(4.9743, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052104164 tensor(4.9970, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03368334 tensor(5.0287, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05108271 tensor(5.1436, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027525406 tensor(5.0367, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.066237174 tensor(5.0660, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036488496 tensor(4.9814, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059573207 tensor(5.1147, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034853112 tensor(5.0074, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07263216 tensor(5.0627, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02414591 tensor(5.0098, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056692544 tensor(5.0725, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027993698 tensor(5.1088, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058768578 tensor(4.9940, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027516926 tensor(5.0325, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05155391 tensor(4.9812, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030786451 tensor(5.1031, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05680911 tensor(5.0220, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028768353 tensor(5.0539, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07224411 tensor(5.0310, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030353507 tensor(5.0339, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058095112 tensor(5.0016, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025653975 tensor(5.0382, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07211313 tensor(5.0189, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038970243 tensor(5.0241, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050644614 tensor(5.0150, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02983535 tensor(4.9776, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041551538 tensor(5.0449, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026134495 tensor(5.1569, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.075479485 tensor(5.0558, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032825425 tensor(5.0406, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05593285 tensor(5.0802, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043158006 tensor(4.9925, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045567553 tensor(5.0686, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03011474 tensor(5.0353, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05681294 tensor(5.0757, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03196528 tensor(5.0118, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04250502 tensor(5.0856, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04783011 tensor(5.0413, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059050128 tensor(5.1169, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040165633 tensor(4.9635, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04777376 tensor(5.1143, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031286962 tensor(5.0567, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05354508 tensor(5.1146, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037400365 tensor(5.0222, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051895544 tensor(5.0246, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04275833 tensor(5.0713, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057561476 tensor(5.0675, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02757264 tensor(5.0909, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062160805 tensor(5.0391, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.020505464 tensor(5.1071, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06528082 tensor(5.0016, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04254428 tensor(5.0378, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044585016 tensor(5.0925, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02541906 tensor(5.0680, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055844996 tensor(4.9970, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034899782 tensor(5.0007, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07515341 tensor(5.0146, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031439584 tensor(5.0384, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05505793 tensor(5.0090, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03201361 tensor(5.0660, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055487905 tensor(5.1420, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.023878362 tensor(5.0734, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06641655 tensor(5.0484, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024607891 tensor(4.9720, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.067867376 tensor(5.0109, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035900027 tensor(5.0178, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04973137 tensor(5.0471, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035175245 tensor(5.0252, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044060618 tensor(5.1067, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.04955804 tensor(5.0580, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05419884 tensor(5.0757, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026668906 tensor(5.0435, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06104942 tensor(5.1225, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0487376 tensor(5.0916, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05303946 tensor(5.0855, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028226908 tensor(5.0813, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06311846 tensor(5.1841, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032730166 tensor(5.0979, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055374753 tensor(5.0247, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039350253 tensor(5.0761, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04915029 tensor(5.0781, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02885806 tensor(5.1001, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05647045 tensor(5.0652, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031087171 tensor(5.0492, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07629618 tensor(5.0531, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04903524 tensor(5.0007, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064584434 tensor(5.0486, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03634969 tensor(4.9912, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061061952 tensor(5.0948, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031146746 tensor(5.0120, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06471289 tensor(5.0079, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03654711 tensor(5.0274, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07410199 tensor(5.0911, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02709029 tensor(5.0742, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060293607 tensor(5.1072, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02419737 tensor(5.0351, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07992165 tensor(5.0622, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033239413 tensor(4.9886, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058035485 tensor(5.1074, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030524269 tensor(5.0814, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049563758 tensor(5.0866, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025944868 tensor(5.0700, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048131593 tensor(5.1057, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03287826 tensor(5.0554, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056133315 tensor(5.1272, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040417574 tensor(5.0146, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06720747 tensor(5.0005, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03469222 tensor(5.0355, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05826956 tensor(4.9890, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025588794 tensor(5.0996, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060212202 tensor(5.1781, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031963807 tensor(5.0903, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054943133 tensor(5.0411, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046088308 tensor(4.9885, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043897156 tensor(5.0300, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027572101 tensor(5.0900, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060483802 tensor(5.0787, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027127486 tensor(5.0322, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05129206 tensor(5.0647, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04280482 tensor(4.9973, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06634407 tensor(5.0547, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03389358 tensor(5.0181, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063439064 tensor(5.0718, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043130513 tensor(5.0644, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04912472 tensor(5.0857, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030483257 tensor(5.0981, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07118999 tensor(5.0449, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051086485 tensor(5.0194, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055753935 tensor(5.0994, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03409441 tensor(5.0864, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048847236 tensor(5.0324, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03762541 tensor(5.0832, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.073481895 tensor(5.0588, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037464403 tensor(5.0307, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0580047 tensor(5.0842, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030118607 tensor(5.0076, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049544692 tensor(5.1119, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03545436 tensor(5.0647, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06861463 tensor(5.0538, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030867657 tensor(5.0575, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046565074 tensor(5.0234, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042731564 tensor(5.0241, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07278066 tensor(5.0993, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032906793 tensor(5.0282, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06772225 tensor(5.0536, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064794205 tensor(5.0987, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.075263135 tensor(5.0566, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030568948 tensor(5.0634, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057698496 tensor(5.0627, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03586844 tensor(4.9785, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046783384 tensor(5.0382, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04797598 tensor(4.9728, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04815864 tensor(5.1010, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034467913 tensor(5.1151, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053443123 tensor(5.1382, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053999983 tensor(5.0795, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049480587 tensor(5.1488, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032820202 tensor(5.0174, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06450503 tensor(5.0578, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036099095 tensor(5.1122, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04706211 tensor(5.0373, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039644267 tensor(5.0804, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07289193 tensor(4.9799, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037007518 tensor(5.0942, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.075691015 tensor(5.0240, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02554369 tensor(5.0108, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040281944 tensor(5.0536, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029444503 tensor(5.0689, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06495146 tensor(5.0362, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036595553 tensor(5.0254, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044829212 tensor(5.1130, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04416319 tensor(5.0299, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06481169 tensor(5.0424, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041779257 tensor(5.0299, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05826931 tensor(5.0311, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034554146 tensor(4.9915, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05762789 tensor(5.0916, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046236366 tensor(4.9989, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07271042 tensor(5.0642, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04041542 tensor(5.1389, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058433253 tensor(5.0415, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04968494 tensor(5.0977, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05893671 tensor(5.0244, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03534364 tensor(5.0698, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049124498 tensor(5.0417, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03165372 tensor(5.0901, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05466347 tensor(5.0776, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.034144726 tensor(4.9978, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08958808 tensor(4.9895, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03518563 tensor(4.9588, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050989326 tensor(5.0857, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039131537 tensor(4.9812, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07002605 tensor(5.0007, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040175058 tensor(4.9942, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055718664 tensor(5.1392, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048449274 tensor(5.0405, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05626098 tensor(4.9846, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05348089 tensor(5.1250, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06337779 tensor(5.0111, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029319031 tensor(5.0189, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047759864 tensor(5.0871, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029462796 tensor(5.0814, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064064965 tensor(5.0771, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026947804 tensor(5.1071, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054295525 tensor(5.1096, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0340628 tensor(5.0759, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05773683 tensor(5.1510, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040558863 tensor(4.9818, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05230545 tensor(5.1156, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046077423 tensor(4.9486, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054766823 tensor(4.9532, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03277722 tensor(4.9719, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060291726 tensor(5.0369, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035279643 tensor(5.0859, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07438366 tensor(5.0730, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03029403 tensor(5.0070, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047700662 tensor(5.0767, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032724597 tensor(5.0343, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052178204 tensor(4.9897, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03668381 tensor(4.9607, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058050554 tensor(5.1227, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042284463 tensor(5.1288, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058662124 tensor(5.0965, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040734347 tensor(4.9977, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06891152 tensor(5.0464, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04198785 tensor(5.0914, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054565083 tensor(5.0636, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036088046 tensor(5.0135, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06812562 tensor(5.0828, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04809333 tensor(5.0286, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05519508 tensor(5.0710, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027897498 tensor(5.0343, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07394579 tensor(5.0860, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043838274 tensor(5.0351, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06266507 tensor(5.0305, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03719063 tensor(5.0820, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04475783 tensor(5.0977, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033625834 tensor(5.0104, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05707387 tensor(5.0710, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045592304 tensor(5.0180, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050500594 tensor(5.1494, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0335681 tensor(5.0641, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053709045 tensor(5.0171, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045889758 tensor(5.0491, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04660607 tensor(5.0549, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04199963 tensor(5.0825, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05973864 tensor(5.0820, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029477144 tensor(5.0603, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055282827 tensor(5.0005, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038428407 tensor(5.0534, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05376122 tensor(5.0893, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039739113 tensor(5.0241, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049948514 tensor(5.0632, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036250517 tensor(5.0760, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060875274 tensor(5.0668, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04080887 tensor(5.0139, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061603706 tensor(5.0314, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036863197 tensor(5.0602, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06554554 tensor(5.0693, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028722225 tensor(4.9691, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06074919 tensor(5.0747, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04331581 tensor(5.0324, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047084626 tensor(5.1327, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035341382 tensor(5.0467, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06666062 tensor(5.0590, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0261287 tensor(5.0690, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05929656 tensor(5.0677, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0325115 tensor(5.0464, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05872121 tensor(5.0348, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03441268 tensor(4.9841, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048550036 tensor(5.1571, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03646496 tensor(5.0478, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.070304304 tensor(5.0990, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03468109 tensor(5.0513, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04947584 tensor(4.9958, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04038147 tensor(4.9969, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06573195 tensor(5.1083, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030116063 tensor(4.9705, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053662017 tensor(5.0503, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035419203 tensor(5.0208, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.071320266 tensor(5.0363, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040115394 tensor(5.0938, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053189017 tensor(5.1440, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035775434 tensor(5.0336, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05763736 tensor(5.0559, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028131716 tensor(5.0508, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055632696 tensor(5.0562, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025178276 tensor(4.9995, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061048336 tensor(5.1519, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04179472 tensor(5.0599, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0696252 tensor(5.0638, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034258485 tensor(5.0352, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07187018 tensor(5.0310, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028016703 tensor(4.9167, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05659267 tensor(4.9835, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040270682 tensor(5.0331, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05315105 tensor(5.0199, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.022378696 tensor(5.0554, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046807133 tensor(5.0713, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04846693 tensor(5.0653, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0756706 tensor(5.0497, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025493061 tensor(5.0368, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052187417 tensor(5.0034, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029495295 tensor(5.1113, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050872423 tensor(4.9611, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0374144 tensor(5.0717, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06071168 tensor(5.0164, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03810377 tensor(5.0360, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064596735 tensor(5.0679, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03964437 tensor(5.0345, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057873223 tensor(5.0438, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04723036 tensor(5.0214, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08965307 tensor(5.0619, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043941647 tensor(5.0132, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06813317 tensor(4.9499, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03319797 tensor(5.1464, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039826803 tensor(5.0134, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03058159 tensor(4.9561, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06042797 tensor(5.0653, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032091405 tensor(5.0365, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.078055345 tensor(5.0278, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04628453 tensor(5.0720, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052475892 tensor(5.1508, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028889865 tensor(5.0493, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058343757 tensor(5.1052, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02984315 tensor(4.9751, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056047276 tensor(5.1287, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03288655 tensor(5.0558, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0611499 tensor(5.1031, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03432468 tensor(5.0624, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057572484 tensor(5.1065, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02628938 tensor(5.0610, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057848092 tensor(5.0761, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043632764 tensor(5.0251, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05560702 tensor(5.0326, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03167712 tensor(5.0712, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044144586 tensor(5.0905, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049994625 tensor(4.9573, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05052147 tensor(5.0912, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032461073 tensor(4.9911, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044582114 tensor(5.0466, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.022666305 tensor(4.9702, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057641137 tensor(5.0635, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028240645 tensor(5.0924, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057351712 tensor(5.1458, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04571711 tensor(5.0458, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048029725 tensor(5.0507, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03416395 tensor(5.0766, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06933834 tensor(5.0361, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026059082 tensor(5.0592, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059400287 tensor(5.0787, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041468803 tensor(4.9973, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051128395 tensor(5.0594, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041674815 tensor(5.0075, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053379264 tensor(5.0109, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042232502 tensor(5.0405, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061933022 tensor(5.0571, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025910111 tensor(5.0396, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06898267 tensor(5.0312, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03856842 tensor(5.0595, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043890048 tensor(5.1120, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034466587 tensor(4.9940, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07688897 tensor(4.9614, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028346043 tensor(5.0189, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.069299586 tensor(5.0793, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044821832 tensor(5.1469, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04897775 tensor(5.1215, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02201552 tensor(5.0172, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056716505 tensor(5.0699, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039205845 tensor(4.9996, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05671903 tensor(5.1332, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030778877 tensor(4.9792, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07261206 tensor(5.0206, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04452762 tensor(5.0087, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07069159 tensor(5.1583, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029538782 tensor(5.0166, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08215269 tensor(5.0723, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027332965 tensor(5.0961, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.071529396 tensor(5.0305, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02722047 tensor(5.0299, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058387194 tensor(5.0392, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03133164 tensor(5.0007, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061721757 tensor(5.0809, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029118301 tensor(5.0194, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.073161334 tensor(4.9593, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031535484 tensor(4.9406, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05963588 tensor(5.0918, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.022147473 tensor(5.0672, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057671756 tensor(5.0538, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03836885 tensor(5.0250, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07353505 tensor(5.1461, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031815663 tensor(4.9776, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048995458 tensor(5.1235, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034250267 tensor(5.0245, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049433216 tensor(5.0769, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031426944 tensor(5.0203, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04581596 tensor(5.1021, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043986704 tensor(5.0101, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.066792525 tensor(5.0887, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027093928 tensor(4.9762, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06815689 tensor(5.0812, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03372784 tensor(5.0092, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05464886 tensor(5.0458, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031635765 tensor(5.0014, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049934413 tensor(5.0621, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04001849 tensor(5.0368, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07216036 tensor(5.0255, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029094381 tensor(5.1182, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05355139 tensor(5.2377, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0312188 tensor(5.0986, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.081652515 tensor(5.0497, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031808503 tensor(5.1383, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05325655 tensor(5.0600, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028825445 tensor(4.9701, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045689337 tensor(5.0136, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037196938 tensor(5.0602, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0762928 tensor(4.9987, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04085005 tensor(5.0717, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06453756 tensor(5.0662, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042400092 tensor(5.1434, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05312563 tensor(5.1108, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033302303 tensor(5.0462, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064908266 tensor(5.0645, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.04883987 tensor(5.0654, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06167769 tensor(5.0436, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03646827 tensor(5.1033, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045226824 tensor(5.1043, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03340043 tensor(5.0333, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05074271 tensor(5.0253, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03660967 tensor(5.0547, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06385676 tensor(5.1225, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053422574 tensor(5.0574, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06779236 tensor(5.0720, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030865291 tensor(5.0280, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07867313 tensor(4.9857, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03515766 tensor(5.0532, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062560566 tensor(5.0882, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036614776 tensor(5.1325, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055379238 tensor(5.0765, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.020308467 tensor(5.0961, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.069003694 tensor(5.0361, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044795543 tensor(5.1282, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06322122 tensor(5.0269, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036512263 tensor(5.0595, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05747414 tensor(5.0198, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042726524 tensor(5.0208, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054913864 tensor(5.1362, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030853244 tensor(5.0202, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050032787 tensor(5.1263, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025948029 tensor(5.0423, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06230828 tensor(5.0935, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05127715 tensor(5.1055, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06309832 tensor(5.0245, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026401386 tensor(5.0785, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.066509575 tensor(5.0245, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030484034 tensor(5.0236, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051225197 tensor(5.0925, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04363342 tensor(4.9639, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.072379865 tensor(5.0766, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04008951 tensor(5.0021, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046032302 tensor(5.1114, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048625555 tensor(5.0963, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0670573 tensor(5.1042, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028963214 tensor(5.0757, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.074321695 tensor(5.0860, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03667713 tensor(5.0990, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06142239 tensor(5.1931, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034689054 tensor(5.1176, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062357757 tensor(5.1068, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034368645 tensor(5.0575, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05904366 tensor(5.0547, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028966937 tensor(5.0530, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06146953 tensor(5.1328, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034595683 tensor(5.1012, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062313497 tensor(5.0851, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033333987 tensor(4.9619, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063809276 tensor(5.0585, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040725652 tensor(4.9277, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04920392 tensor(5.0403, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037314 tensor(5.0750, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05668638 tensor(5.1221, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038822938 tensor(5.0809, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056358818 tensor(5.0172, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03396961 tensor(5.0462, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057532426 tensor(4.9953, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029137071 tensor(4.9479, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06473002 tensor(4.9992, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034823183 tensor(5.0629, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06443473 tensor(5.0821, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04253777 tensor(4.9947, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06388514 tensor(5.0959, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030943757 tensor(5.0628, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054964375 tensor(5.2002, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02820355 tensor(5.0241, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056930386 tensor(5.0325, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031983722 tensor(5.0232, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.072244264 tensor(5.1183, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051028177 tensor(5.0192, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050522286 tensor(5.0314, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0272486 tensor(5.0146, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059238005 tensor(5.0459, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05686639 tensor(4.9047, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06208407 tensor(5.0670, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028542202 tensor(5.0662, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05210452 tensor(4.9693, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036009174 tensor(5.0671, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06729916 tensor(5.0113, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040457297 tensor(4.9633, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06332519 tensor(5.0828, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034162782 tensor(4.9498, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0615181 tensor(5.1120, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032952115 tensor(5.0672, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057400826 tensor(4.9688, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04286592 tensor(4.9697, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052358143 tensor(5.1032, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035019483 tensor(5.0466, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06680051 tensor(5.0519, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04792157 tensor(5.0157, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055469982 tensor(5.0046, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0460037 tensor(4.9773, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054209888 tensor(5.1167, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034266952 tensor(5.0124, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050396726 tensor(5.0526, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035249818 tensor(5.0575, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.073185645 tensor(5.0013, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03577419 tensor(5.0761, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05431827 tensor(5.0329, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026597882 tensor(5.0771, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.065316394 tensor(5.0793, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04154977 tensor(5.0182, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06523925 tensor(5.1028, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044092037 tensor(5.0665, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057020966 tensor(5.0047, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02624009 tensor(5.0760, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07359072 tensor(4.9954, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03226114 tensor(5.0675, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06030701 tensor(5.1410, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029468631 tensor(5.0188, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05980922 tensor(4.9950, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039161995 tensor(5.0858, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06143328 tensor(4.9258, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.03155414 tensor(4.9643, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06526656 tensor(5.0705, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046879396 tensor(5.0347, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05809047 tensor(5.0092, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05024164 tensor(4.9660, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055480376 tensor(5.0861, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.020748625 tensor(5.1913, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056425143 tensor(5.0816, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030400245 tensor(5.0174, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05359852 tensor(5.0420, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03744618 tensor(5.0453, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040392455 tensor(5.0997, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032531418 tensor(5.1239, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07590711 tensor(5.0326, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035813585 tensor(5.0656, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058110524 tensor(5.0765, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03690853 tensor(5.0273, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05182501 tensor(5.0628, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032875035 tensor(5.0461, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07415689 tensor(5.0898, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030533496 tensor(5.0802, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0468532 tensor(5.0152, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047364913 tensor(5.0484, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044890326 tensor(5.0879, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03745983 tensor(5.1524, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05769449 tensor(5.0417, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028649865 tensor(5.0499, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05549165 tensor(5.1444, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042093948 tensor(5.0620, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056951486 tensor(5.1099, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03301868 tensor(5.0500, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05767196 tensor(5.0229, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043084994 tensor(5.0513, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.066094875 tensor(5.0548, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0253781 tensor(4.9897, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048269298 tensor(5.1368, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03756387 tensor(4.9734, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06779263 tensor(5.0568, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025590744 tensor(5.0835, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042245936 tensor(5.1167, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02707321 tensor(5.0552, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07288045 tensor(5.0761, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043503243 tensor(5.0144, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049841426 tensor(5.1253, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045029797 tensor(5.0290, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05609725 tensor(5.1078, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0403357 tensor(5.0587, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05748751 tensor(5.0514, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034031056 tensor(4.9852, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07935745 tensor(5.1040, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037131537 tensor(5.0488, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060110044 tensor(5.0826, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03334591 tensor(5.0114, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061992075 tensor(4.9876, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038948644 tensor(5.0385, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07351546 tensor(5.0141, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03568545 tensor(5.1122, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07636831 tensor(5.0017, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030662032 tensor(5.0998, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.068147205 tensor(4.9530, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04424533 tensor(5.0263, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07048978 tensor(5.1130, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024652693 tensor(5.0232, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06236939 tensor(5.0126, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033670116 tensor(5.1166, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062254965 tensor(5.1023, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027045198 tensor(5.1102, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08203111 tensor(5.0308, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025733965 tensor(5.0508, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055792935 tensor(5.0199, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039407756 tensor(4.9840, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062068935 tensor(5.0705, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048333712 tensor(5.0730, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05494443 tensor(5.0420, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048562113 tensor(5.0312, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053862218 tensor(5.1289, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037596636 tensor(5.1905, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06626838 tensor(5.0328, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03514351 tensor(5.0565, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04816333 tensor(5.1812, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030540692 tensor(4.9727, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058338437 tensor(5.1045, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04676159 tensor(5.0818, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058324527 tensor(5.0560, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030274281 tensor(4.9910, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039811958 tensor(5.1021, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02939205 tensor(5.0132, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054518268 tensor(5.0763, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035411518 tensor(5.0396, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061731767 tensor(5.0296, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04165323 tensor(5.0298, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06230327 tensor(5.0345, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025067523 tensor(5.0973, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04817221 tensor(5.0955, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025095452 tensor(5.1947, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03976241 tensor(5.0508, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02602278 tensor(4.9796, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050342165 tensor(5.0705, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030363549 tensor(5.0803, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048403095 tensor(5.0794, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039197408 tensor(5.0717, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07415014 tensor(5.0515, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032414194 tensor(5.0439, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06440642 tensor(5.0583, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04970537 tensor(5.0895, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049825653 tensor(5.0857, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027162744 tensor(5.0674, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057754688 tensor(5.0100, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.021612639 tensor(5.0174, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050549507 tensor(5.0385, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029814461 tensor(5.0297, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06809381 tensor(5.0991, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03005781 tensor(5.0861, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05027252 tensor(5.1337, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034298114 tensor(5.0634, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063049264 tensor(5.1320, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038059108 tensor(5.0751, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05567534 tensor(5.0557, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.024037566 tensor(5.0036, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06716974 tensor(5.0622, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042001005 tensor(5.0338, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059344966 tensor(5.0587, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049454987 tensor(5.0035, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055365935 tensor(5.0591, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049914863 tensor(4.9523, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05213801 tensor(5.0908, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026830433 tensor(5.0860, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056310102 tensor(5.0160, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042360883 tensor(5.0843, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06998708 tensor(4.9978, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03521442 tensor(5.0780, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061592996 tensor(5.0724, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04143186 tensor(5.0336, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045695018 tensor(5.1245, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04501669 tensor(4.9755, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06460925 tensor(4.9835, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029646369 tensor(5.0433, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060695153 tensor(5.0508, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03406398 tensor(4.9895, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055104576 tensor(5.0465, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040488858 tensor(5.0133, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06230885 tensor(5.0703, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025168225 tensor(5.1025, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05889305 tensor(5.0807, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.023016565 tensor(5.0948, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055000644 tensor(5.1038, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03385832 tensor(4.9854, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060695276 tensor(5.0132, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027512133 tensor(5.0547, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05028724 tensor(5.0198, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03284059 tensor(4.9856, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05824368 tensor(5.0306, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03167143 tensor(5.0555, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04523711 tensor(5.1086, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03565865 tensor(5.0228, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05104581 tensor(5.0426, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032469753 tensor(5.0768, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060270406 tensor(4.9895, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029411824 tensor(5.0943, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.077387504 tensor(5.0658, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03003902 tensor(5.0288, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07002465 tensor(5.0922, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03363554 tensor(5.0495, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044181604 tensor(5.0967, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046225827 tensor(5.0754, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07568297 tensor(4.9774, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028128438 tensor(5.0263, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059906933 tensor(4.9850, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038938764 tensor(5.0449, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053478703 tensor(5.0821, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034945007 tensor(5.0703, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05684892 tensor(4.9791, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034895133 tensor(5.0962, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07439792 tensor(5.0734, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034016106 tensor(5.1206, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06544376 tensor(5.0989, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043012507 tensor(5.0916, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06419038 tensor(5.0995, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02914597 tensor(5.1523, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0714376 tensor(5.0721, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033287585 tensor(5.0227, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04913144 tensor(5.0104, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0373015 tensor(5.0164, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050598305 tensor(5.0134, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044096556 tensor(5.0327, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053755395 tensor(5.0431, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0403056 tensor(4.9653, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05379548 tensor(5.0235, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03603318 tensor(5.0374, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05587351 tensor(5.0237, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026862852 tensor(4.9939, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07523254 tensor(5.0040, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030921724 tensor(5.0418, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05088155 tensor(5.1315, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043406907 tensor(5.0178, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062657386 tensor(5.0430, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04635291 tensor(5.0680, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059609912 tensor(5.1216, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029178087 tensor(4.9977, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062376276 tensor(5.0397, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03721879 tensor(5.0989, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059312176 tensor(5.0760, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035138074 tensor(5.0939, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052441955 tensor(5.0759, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04250206 tensor(4.9567, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06259544 tensor(5.1006, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036490317 tensor(5.0724, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04742625 tensor(5.0743, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02567693 tensor(5.0747, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048558537 tensor(5.0197, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043480102 tensor(5.0407, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056585517 tensor(5.0121, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0456075 tensor(5.0980, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07731419 tensor(4.9953, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035401266 tensor(5.0297, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050957453 tensor(5.0387, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04251642 tensor(5.0116, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07407987 tensor(5.0283, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041985635 tensor(4.9916, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07272081 tensor(5.0750, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027888339 tensor(5.0133, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07036024 tensor(4.9339, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030411316 tensor(5.0248, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06875016 tensor(5.0245, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038753 tensor(5.0906, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055100895 tensor(5.0689, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035156723 tensor(5.0368, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.075257 tensor(5.0421, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04249198 tensor(5.0348, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043052256 tensor(5.0252, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029202843 tensor(5.0149, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07238583 tensor(5.0566, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03177306 tensor(5.0795, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06297771 tensor(5.0318, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05044981 tensor(5.0590, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048546467 tensor(5.0441, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.045787062 tensor(4.9894, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05107644 tensor(5.1737, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034816843 tensor(5.0325, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05134147 tensor(5.0650, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03052251 tensor(4.9653, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04907767 tensor(5.0826, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04388918 tensor(5.0834, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050341953 tensor(5.0954, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040746886 tensor(5.0920, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07545399 tensor(5.0343, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039682336 tensor(4.9422, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052981466 tensor(4.9538, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03807544 tensor(4.9763, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059855774 tensor(5.0175, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030449955 tensor(4.9783, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06405277 tensor(5.0901, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041993372 tensor(5.0058, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07672509 tensor(4.9937, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04306795 tensor(5.0257, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057077438 tensor(5.0043, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040822875 tensor(4.9921, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051385336 tensor(5.0469, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03305975 tensor(5.0981, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06345984 tensor(5.1173, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032459576 tensor(5.0289, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07737607 tensor(5.1429, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030847965 tensor(5.0407, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042708434 tensor(5.1307, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027800916 tensor(5.0616, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057161752 tensor(5.0474, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024052013 tensor(5.1160, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04686088 tensor(4.9664, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034995727 tensor(5.0877, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05498632 tensor(5.1061, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028917603 tensor(5.0220, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05104264 tensor(5.0292, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04605004 tensor(5.0129, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06549729 tensor(5.1052, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04561453 tensor(5.0941, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05527887 tensor(5.1566, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027534155 tensor(5.0074, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07100179 tensor(5.0488, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03250175 tensor(5.0382, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049353033 tensor(5.0532, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03823753 tensor(4.9589, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054732695 tensor(5.0921, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04346778 tensor(4.9906, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063813716 tensor(5.1713, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030011846 tensor(5.0563, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049565002 tensor(5.0063, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028483875 tensor(5.0288, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054197405 tensor(5.0197, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03273504 tensor(4.9573, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06598088 tensor(4.8832, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033792716 tensor(5.1490, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06318656 tensor(5.0271, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033974692 tensor(4.9944, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05919674 tensor(4.9899, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054137044 tensor(5.0420, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06921796 tensor(5.0864, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04505052 tensor(4.9976, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.068425335 tensor(5.0984, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03513768 tensor(4.9744, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06122033 tensor(5.0560, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029284889 tensor(5.1086, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04963128 tensor(5.0036, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04436575 tensor(5.1068, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08566334 tensor(5.0008, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040198985 tensor(5.0295, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07261603 tensor(5.0527, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037621215 tensor(5.0100, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044079036 tensor(5.1089, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035196554 tensor(5.0310, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062282313 tensor(5.1376, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032881938 tensor(5.0150, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07286324 tensor(4.9735, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034519613 tensor(5.0188, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.071786806 tensor(4.8996, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046386898 tensor(5.0620, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045423523 tensor(5.0988, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03085235 tensor(5.0277, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06206272 tensor(5.0678, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04322796 tensor(5.0433, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06738912 tensor(5.0389, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029980158 tensor(5.0391, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042419296 tensor(4.9753, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033405285 tensor(5.1255, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053679828 tensor(5.0404, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030459551 tensor(5.1108, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037648812 tensor(5.0677, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033793345 tensor(5.0466, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049221657 tensor(5.1569, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043447178 tensor(5.0487, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04733243 tensor(5.0439, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033425868 tensor(5.0460, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051496364 tensor(5.0920, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03820464 tensor(5.0385, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060741972 tensor(5.0589, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037424926 tensor(4.9897, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050915502 tensor(5.1107, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036755312 tensor(5.1006, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06155752 tensor(5.0896, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.045932524 tensor(4.9697, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07099909 tensor(5.0165, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031089297 tensor(4.9640, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043836292 tensor(5.1011, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02540198 tensor(5.0405, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06268132 tensor(5.0886, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043016918 tensor(5.0011, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.071932286 tensor(5.0113, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03828606 tensor(5.0903, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07633234 tensor(5.1089, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043277036 tensor(5.0221, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054836936 tensor(5.0831, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033874433 tensor(5.0319, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05748732 tensor(5.1115, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02431336 tensor(4.9718, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.076480545 tensor(4.9796, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.034665953 tensor(5.0397, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061525583 tensor(5.0636, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03864727 tensor(5.0297, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060166728 tensor(5.0262, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030239552 tensor(5.0779, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05854877 tensor(5.0375, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028398244 tensor(5.0273, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04714556 tensor(5.0336, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03292894 tensor(5.0257, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043741666 tensor(5.0722, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028735366 tensor(4.9982, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05813053 tensor(5.0098, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029311989 tensor(5.0627, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057496227 tensor(4.9898, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030246837 tensor(5.0452, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04759185 tensor(5.0907, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026679091 tensor(5.1356, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04602446 tensor(5.1197, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030553108 tensor(5.0270, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052182935 tensor(5.0539, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03198885 tensor(5.0050, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051418554 tensor(5.0052, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03519092 tensor(5.0072, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06086426 tensor(5.0375, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038353898 tensor(4.9371, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06946335 tensor(5.0283, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040595118 tensor(5.0643, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054775495 tensor(5.0560, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027072571 tensor(5.0822, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040809214 tensor(5.0839, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.022301102 tensor(5.0804, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06788928 tensor(5.1215, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03160234 tensor(5.0574, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06167099 tensor(5.0209, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02972887 tensor(4.9972, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06831333 tensor(5.0168, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026560854 tensor(5.0655, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05414079 tensor(5.0799, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027324783 tensor(5.0559, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04720208 tensor(5.0389, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030784456 tensor(5.0557, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058328856 tensor(5.0344, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036601838 tensor(4.9699, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06509773 tensor(5.0020, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02384664 tensor(5.0831, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059049793 tensor(5.0798, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02260098 tensor(5.0648, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0598799 tensor(5.0392, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036341447 tensor(5.0367, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056487255 tensor(5.0704, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051313885 tensor(5.0964, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048596617 tensor(5.0161, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04219034 tensor(5.0417, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057218745 tensor(5.0090, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03218407 tensor(5.1346, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0463662 tensor(5.0637, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032332532 tensor(5.0805, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.076715104 tensor(5.0260, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03907557 tensor(5.0932, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.065881535 tensor(5.1050, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030223772 tensor(5.0116, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046253722 tensor(5.1165, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030933542 tensor(5.0746, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049715742 tensor(5.0089, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048920862 tensor(5.0640, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04902218 tensor(5.1642, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028753499 tensor(5.0634, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07604854 tensor(5.0066, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03537935 tensor(5.0665, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07630283 tensor(5.0561, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030271541 tensor(5.0620, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06694011 tensor(4.9482, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028465575 tensor(5.0021, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05115571 tensor(5.0807, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025523078 tensor(5.0720, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07157754 tensor(5.0068, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032030877 tensor(5.0139, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056882773 tensor(5.0490, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04685963 tensor(4.9471, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06278725 tensor(5.0196, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03069142 tensor(5.1714, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04007336 tensor(5.0660, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03052731 tensor(5.0507, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058011197 tensor(5.0485, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033771455 tensor(5.0144, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0694598 tensor(4.9952, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033456873 tensor(5.0393, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059624344 tensor(4.9721, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036770876 tensor(4.9538, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06509496 tensor(5.0516, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034044124 tensor(5.0175, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061486598 tensor(5.1129, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030081736 tensor(5.0445, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04403359 tensor(5.0878, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02970974 tensor(5.0372, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06920078 tensor(4.9604, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04542424 tensor(5.0368, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056995552 tensor(5.0685, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03686085 tensor(5.0350, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05351924 tensor(5.1002, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03220329 tensor(5.0768, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04494252 tensor(5.0570, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03257056 tensor(4.9909, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063266724 tensor(5.0037, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037571102 tensor(5.0419, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048608303 tensor(5.0722, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04215198 tensor(5.0274, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06381488 tensor(5.0942, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03553066 tensor(5.0198, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07064334 tensor(5.1110, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029977791 tensor(5.0897, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049059477 tensor(5.0353, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032732766 tensor(5.0315, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06068634 tensor(5.0341, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035169497 tensor(5.0061, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049380932 tensor(5.0249, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024059365 tensor(5.0353, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06097562 tensor(5.0802, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.03337167 tensor(5.0705, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055968378 tensor(4.9985, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03697508 tensor(5.0452, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061428968 tensor(5.0975, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029275866 tensor(5.0290, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0540514 tensor(5.0142, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036163244 tensor(4.9476, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055060234 tensor(5.0800, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042271435 tensor(5.0684, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043723926 tensor(5.0652, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03135756 tensor(5.0360, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058332585 tensor(4.9500, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027485851 tensor(5.1516, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06728072 tensor(5.0868, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04352163 tensor(4.9141, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062439088 tensor(5.0746, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032304168 tensor(5.0079, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061041377 tensor(5.0947, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03452401 tensor(5.0646, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0656131 tensor(5.0799, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032243658 tensor(4.9930, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048913345 tensor(5.1252, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.023576131 tensor(5.0299, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06462404 tensor(5.0819, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028217215 tensor(5.0649, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07316719 tensor(5.0840, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034310363 tensor(5.0624, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050322212 tensor(5.1223, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04106491 tensor(5.0723, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054487076 tensor(5.0274, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03692033 tensor(4.9894, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062425565 tensor(5.0353, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034237973 tensor(5.0534, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064110614 tensor(5.0908, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026807755 tensor(5.0420, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05621525 tensor(5.1024, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0514568 tensor(5.0293, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06865903 tensor(5.0425, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038465988 tensor(4.9615, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.081908345 tensor(5.1032, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034451842 tensor(5.0112, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04788955 tensor(5.0629, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032802254 tensor(5.0253, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052898835 tensor(5.0278, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041213274 tensor(5.1750, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07239845 tensor(5.1011, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032034248 tensor(5.0590, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06451286 tensor(5.0675, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034952883 tensor(5.0399, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05424475 tensor(5.0027, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034046967 tensor(5.0070, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0628889 tensor(5.0341, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030795971 tensor(5.0477, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058238033 tensor(5.0585, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028777042 tensor(1.2222, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026636142 tensor(1.2325, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "Encodings trained\n"
     ]
    }
   ],
   "source": [
    "train_encoding_ds = EncodingDS(PointDriftDS(pn_train_data, pn_train_labels), pn_autodecoder)\n",
    "train_result = train_encoding_ds.train_encodings(num_iterations=15, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.026764631 tensor(4.9817, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054824222 tensor(5.0565, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031203177 tensor(5.0370, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07388829 tensor(5.0208, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04083364 tensor(5.1003, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061858054 tensor(5.1549, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026042178 tensor(4.9962, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06847145 tensor(5.1118, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032685135 tensor(5.1021, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.069376126 tensor(4.9552, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039120417 tensor(5.1156, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0605455 tensor(5.0275, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.026757041 tensor(5.0323, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050557528 tensor(5.1113, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032181047 tensor(5.0783, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06601241 tensor(5.0286, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037899334 tensor(5.1237, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06734746 tensor(5.1146, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032299247 tensor(4.9654, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0552592 tensor(5.0411, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03726978 tensor(5.1114, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055107005 tensor(5.1737, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030348819 tensor(5.1168, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058858555 tensor(5.1162, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04423148 tensor(5.0130, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06077972 tensor(5.1340, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034744516 tensor(5.0407, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055888176 tensor(5.1791, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032357793 tensor(4.9931, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059886493 tensor(5.1084, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035436977 tensor(5.0337, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063848145 tensor(5.0696, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0366876 tensor(5.0707, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06334661 tensor(5.0740, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04252611 tensor(5.0548, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059120078 tensor(5.1075, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028615346 tensor(5.0383, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04711123 tensor(5.1447, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056029905 tensor(5.0043, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05891894 tensor(5.0521, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03973342 tensor(5.0568, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06167276 tensor(5.0958, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04468782 tensor(4.9859, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043520942 tensor(5.0654, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03842005 tensor(5.0655, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06728688 tensor(5.0904, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05122824 tensor(4.9777, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048118234 tensor(5.0046, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02967334 tensor(5.0745, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06640348 tensor(5.0603, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036226086 tensor(5.1328, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060308833 tensor(5.1086, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048081707 tensor(4.9798, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057197448 tensor(5.0483, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044537846 tensor(4.9939, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061911665 tensor(5.1024, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03215821 tensor(5.0366, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.072162084 tensor(5.1020, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02601342 tensor(5.1366, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048971877 tensor(5.0484, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.020964038 tensor(5.1136, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058234017 tensor(5.0457, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044728763 tensor(5.0222, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0628289 tensor(5.0631, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052594274 tensor(5.0801, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.073207766 tensor(5.0541, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038750738 tensor(5.0897, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.063503526 tensor(4.9961, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051564716 tensor(5.0128, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049789105 tensor(5.0432, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028593957 tensor(5.0107, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06193349 tensor(5.0750, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03191839 tensor(4.9606, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06317276 tensor(5.0922, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02759082 tensor(5.1043, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08516481 tensor(5.0977, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03481861 tensor(5.0457, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060772132 tensor(5.0017, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03328081 tensor(5.0477, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04991849 tensor(5.0484, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03812065 tensor(5.0174, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042759 tensor(5.0267, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.022601193 tensor(5.1376, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07039041 tensor(5.1139, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03464409 tensor(5.0894, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06908194 tensor(5.1400, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025790619 tensor(4.9391, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054230276 tensor(5.0632, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031484004 tensor(5.0430, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05136615 tensor(5.1519, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032702826 tensor(5.0531, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053086694 tensor(5.1297, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04056707 tensor(5.0232, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.069926195 tensor(5.0762, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03987492 tensor(5.0576, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06318416 tensor(5.0127, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030968701 tensor(5.0778, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.076327324 tensor(5.0575, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03485443 tensor(4.9656, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06397795 tensor(4.9608, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034229074 tensor(5.0607, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059199367 tensor(5.0280, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027630031 tensor(5.0093, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05463903 tensor(5.0917, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046963323 tensor(5.0413, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059339352 tensor(5.0400, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04526191 tensor(4.9924, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057048213 tensor(5.0370, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03007839 tensor(5.0606, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057860546 tensor(5.1315, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04107975 tensor(5.0211, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.067665294 tensor(4.9970, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040531766 tensor(5.0199, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07666523 tensor(5.0705, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042060047 tensor(5.0207, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.070321634 tensor(5.1337, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0358691 tensor(4.9822, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058353495 tensor(5.0247, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.033964828 tensor(5.0511, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061209984 tensor(5.0189, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042764235 tensor(5.0092, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07181399 tensor(5.0037, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03226217 tensor(5.0198, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07586773 tensor(5.0468, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028946979 tensor(4.9685, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.055411723 tensor(5.0750, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04495098 tensor(4.9969, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0753815 tensor(5.0247, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024106447 tensor(5.0654, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07238311 tensor(4.9969, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04800698 tensor(5.0628, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06549365 tensor(5.1059, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033348646 tensor(5.0365, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05681713 tensor(5.0560, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02937544 tensor(5.0438, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06289556 tensor(5.0176, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06115527 tensor(4.9527, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053130075 tensor(5.1956, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027739694 tensor(5.0308, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07135694 tensor(5.0737, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049677618 tensor(5.0610, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08360289 tensor(4.9752, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051951677 tensor(5.0937, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06657883 tensor(4.9732, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035657458 tensor(4.9716, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05390259 tensor(5.0968, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034705788 tensor(5.0692, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060945794 tensor(5.0906, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048245985 tensor(5.0952, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.051727977 tensor(5.0930, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03217779 tensor(5.0945, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053712223 tensor(4.9601, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030320868 tensor(5.0027, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07010742 tensor(4.9862, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04982351 tensor(4.9996, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0571261 tensor(5.0703, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030143704 tensor(4.9240, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058990266 tensor(5.1505, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033886 tensor(5.0511, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059178174 tensor(4.9470, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035730805 tensor(5.0443, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050213825 tensor(5.0098, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034264922 tensor(5.0456, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07477104 tensor(5.0903, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03483285 tensor(5.0306, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064818926 tensor(5.0217, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03200565 tensor(5.0770, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06012066 tensor(5.0719, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03181337 tensor(5.0167, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059389487 tensor(5.0869, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030807355 tensor(5.0594, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.058662985 tensor(5.0717, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029985012 tensor(5.1023, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06492293 tensor(5.0928, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03388004 tensor(5.0590, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06661864 tensor(5.1136, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024551135 tensor(5.0311, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062025066 tensor(5.0755, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.048756108 tensor(5.0084, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04060281 tensor(5.0029, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028492711 tensor(4.9711, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064202875 tensor(5.0996, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.049807895 tensor(5.0915, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06880065 tensor(5.0413, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032492135 tensor(5.0427, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054634914 tensor(5.0491, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032306977 tensor(5.0698, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0548738 tensor(5.0320, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.024960995 tensor(4.9418, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061174832 tensor(5.0097, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025635928 tensor(4.9982, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06862005 tensor(4.9731, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032847244 tensor(4.9890, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040502336 tensor(5.0814, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03044335 tensor(4.9951, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050364833 tensor(5.0607, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029927406 tensor(5.1577, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059190802 tensor(5.0924, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02891427 tensor(5.0891, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046113975 tensor(5.1021, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03820673 tensor(5.0703, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.073975936 tensor(5.1097, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03641993 tensor(5.0676, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04857674 tensor(4.9739, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03368871 tensor(5.0158, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05964816 tensor(5.0184, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030232046 tensor(5.0166, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.047809508 tensor(5.1054, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038608775 tensor(5.0265, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04745186 tensor(5.0513, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.035561286 tensor(5.0696, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.08050833 tensor(5.0736, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029506503 tensor(5.0655, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.061824374 tensor(5.0872, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.040598094 tensor(4.9666, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060540587 tensor(4.9919, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042879682 tensor(5.0688, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05849382 tensor(5.0752, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044888962 tensor(5.0622, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06891482 tensor(5.1044, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04258792 tensor(5.1072, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06977794 tensor(5.0375, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034771904 tensor(5.0444, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064470805 tensor(5.0135, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034353085 tensor(5.0418, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06817951 tensor(5.0057, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.031876437 tensor(5.0620, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07158829 tensor(5.1047, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041198336 tensor(5.1089, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.060116947 tensor(5.0086, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03017835 tensor(5.0848, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.088831104 tensor(5.0083, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029415393 tensor(5.0065, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.046944518 tensor(5.0645, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03818974 tensor(5.0702, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052529264 tensor(5.0520, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.031404063 tensor(5.1480, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044893067 tensor(5.0615, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.044061877 tensor(4.9687, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07238374 tensor(5.0295, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029386401 tensor(4.9783, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05170419 tensor(5.0616, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.02811368 tensor(5.0409, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.056610197 tensor(5.0379, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025119407 tensor(4.9436, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059610862 tensor(4.9930, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027422875 tensor(5.0062, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.093910426 tensor(5.0278, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.037004944 tensor(4.9948, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.059526548 tensor(5.0631, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03999209 tensor(5.1374, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04475285 tensor(5.0609, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03096963 tensor(5.0315, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05074861 tensor(5.1297, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033331156 tensor(5.0002, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.054943547 tensor(4.9525, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029482262 tensor(5.0551, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.073239215 tensor(4.9699, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.039282456 tensor(5.1274, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06893817 tensor(5.0662, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.042585492 tensor(5.0239, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05670291 tensor(5.0436, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04271665 tensor(5.0078, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05910274 tensor(5.0095, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.027330417 tensor(5.0386, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07848607 tensor(5.0595, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.025160167 tensor(5.0542, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06846372 tensor(5.1274, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029568719 tensor(5.0766, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.071337976 tensor(5.1105, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.033174712 tensor(5.0194, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06289147 tensor(4.9652, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028943958 tensor(5.0511, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06153493 tensor(5.0620, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03391203 tensor(5.0514, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.062503666 tensor(5.0625, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.053242035 tensor(5.0614, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.043553784 tensor(5.0829, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038675882 tensor(4.9858, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07781797 tensor(5.0407, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.029395951 tensor(5.0584, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05843228 tensor(5.0684, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.052118182 tensor(5.0549, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.050236803 tensor(5.0725, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034868132 tensor(4.9744, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06159705 tensor(5.0031, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.022973921 tensor(5.0707, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07232188 tensor(5.0035, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.036666777 tensor(5.0400, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.068372704 tensor(5.0831, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030575532 tensor(5.0522, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04610104 tensor(5.0431, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05850638 tensor(5.0730, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06191531 tensor(5.0553, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.038407616 tensor(4.9961, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06371428 tensor(5.1038, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.032690924 tensor(4.9556, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06060668 tensor(4.9416, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.030103587 tensor(5.0238, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.057749156 tensor(5.0721, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034459874 tensor(5.0265, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06281022 tensor(5.0353, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.028459245 tensor(5.0445, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.05481023 tensor(5.0813, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04678874 tensor(5.0256, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.06553187 tensor(5.0385, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.03735943 tensor(5.1553, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07395575 tensor(5.0956, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.04729247 tensor(5.0836, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.064022005 tensor(5.1220, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.034717906 tensor(5.0684, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.066491544 tensor(5.1013, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.041910365 tensor(4.2331, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.07392955 tensor(4.2154, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "Encodings trained\n"
     ]
    }
   ],
   "source": [
    "test_encoding_ds = EncodingDS(PointDriftDS(pn_test_data, pn_test_labels), pn_autodecoder)\n",
    "test_result = test_encoding_ds.train_encodings(num_iterations=15, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a single CompNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpnet_HP = HyperParameter(epochs=20, batch_size=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. batch_idx: 100\n",
      "Loss:  0.47375884890556336 0.5985329768061638\n",
      "Epoch: 0. batch_idx: 200\n",
      "Loss:  0.3678618086874485 0.38697339296340943\n",
      "Epoch: 0. batch_idx: 300\n",
      "Loss:  0.384534215927124 0.35996296517550946\n",
      "Epoch: 0. batch_idx: 400\n",
      "Loss:  0.3650429932773113 0.3572743085026741\n",
      "Epoch: 0. batch_idx: 500\n",
      "Loss:  0.3835951338708401 0.3580167056620121\n",
      "Epoch: 0. batch_idx: 600\n",
      "Loss:  0.35218377351760866 0.34372315883636473\n",
      "Eval:  (0.3502203288256742, 0.3461675195462385, array([0.84244116]), array([0.86238532]))\n",
      "Epoch: 1. batch_idx: 100\n",
      "Loss:  0.34379714585840704 0.3020548690855503\n",
      "Epoch: 1. batch_idx: 200\n",
      "Loss:  0.3338519622385502 0.32599783018231393\n",
      "Epoch: 1. batch_idx: 300\n",
      "Loss:  0.33520618721842765 0.31140077516436576\n",
      "Epoch: 1. batch_idx: 400\n",
      "Loss:  0.33837755732238295 0.3158174542337656\n",
      "Epoch: 1. batch_idx: 500\n",
      "Loss:  0.30661105684936046 0.29735548615455626\n",
      "Epoch: 1. batch_idx: 600\n",
      "Loss:  0.3344981502741575 0.30796699300408364\n",
      "Epoch: 2. batch_idx: 100\n",
      "Loss:  0.30618038833141326 0.2966724303364754\n",
      "Epoch: 2. batch_idx: 200\n",
      "Loss:  0.3007676104456186 0.2877648696675897\n",
      "Epoch: 2. batch_idx: 300\n",
      "Loss:  0.30033751055598257 0.29201728142797945\n",
      "Epoch: 2. batch_idx: 400\n",
      "Loss:  0.30837036818265917 0.2947824819386005\n",
      "Epoch: 2. batch_idx: 500\n",
      "Loss:  0.31390609350055454 0.30248628873378036\n",
      "Epoch: 2. batch_idx: 600\n",
      "Loss:  0.306486399397254 0.2911778900772333\n",
      "Epoch: 3. batch_idx: 100\n",
      "Loss:  0.31556920751929285 0.2825002332031727\n",
      "Epoch: 3. batch_idx: 200\n",
      "Loss:  0.2856895049661398 0.28471182465553285\n",
      "Epoch: 3. batch_idx: 300\n",
      "Loss:  0.2785158775746822 0.273801079466939\n",
      "Epoch: 3. batch_idx: 400\n",
      "Loss:  0.2748928402364254 0.2539480502158403\n",
      "Epoch: 3. batch_idx: 500\n",
      "Loss:  0.2953689393028617 0.2925401934236288\n",
      "Epoch: 3. batch_idx: 600\n",
      "Loss:  0.29905766382813453 0.29031159847974775\n",
      "Epoch: 4. batch_idx: 100\n",
      "Loss:  0.28889165695756674 0.2770941915363073\n",
      "Epoch: 4. batch_idx: 200\n",
      "Loss:  0.27525538589805365 0.2866433150693774\n",
      "Epoch: 4. batch_idx: 300\n",
      "Loss:  0.2618540252745152 0.2596368865668774\n",
      "Epoch: 4. batch_idx: 400\n",
      "Loss:  0.3086385357007384 0.2827606866136193\n",
      "Epoch: 4. batch_idx: 500\n",
      "Loss:  0.2800758346170187 0.27240951001644137\n",
      "Epoch: 4. batch_idx: 600\n",
      "Loss:  0.27255351908504966 0.2575953252613544\n",
      "Epoch: 5. batch_idx: 100\n",
      "Loss:  0.2809454621374607 0.27219635114073754\n",
      "Epoch: 5. batch_idx: 200\n",
      "Loss:  0.26300123475492 0.2732258746773005\n",
      "Epoch: 5. batch_idx: 300\n",
      "Loss:  0.2662894306704402 0.24363777123391628\n",
      "Epoch: 5. batch_idx: 400\n",
      "Loss:  0.26390423223376275 0.2667889540269971\n",
      "Epoch: 5. batch_idx: 500\n",
      "Loss:  0.2825207404047251 0.26849795661866666\n",
      "Epoch: 5. batch_idx: 600\n",
      "Loss:  0.2670798801258206 0.2633258571848273\n",
      "Eval:  (0.30360189889361905, 0.3074972387759169, array([0.87355405]), array([0.88033506]))\n",
      "Epoch: 6. batch_idx: 100\n",
      "Loss:  0.2640870273858309 0.24440436087548734\n",
      "Epoch: 6. batch_idx: 200\n",
      "Loss:  0.2518307263404131 0.24872299931943417\n",
      "Epoch: 6. batch_idx: 300\n",
      "Loss:  0.2701407142728567 0.2658663918823004\n",
      "Epoch: 6. batch_idx: 400\n",
      "Loss:  0.24081400591880084 0.24651681907474995\n",
      "Epoch: 6. batch_idx: 500\n",
      "Loss:  0.26985365062952044 0.2463078187406063\n",
      "Epoch: 6. batch_idx: 600\n",
      "Loss:  0.2573286232724786 0.2629384554177523\n",
      "Epoch: 7. batch_idx: 100\n",
      "Loss:  0.24956455267965794 0.245488119199872\n",
      "Epoch: 7. batch_idx: 200\n",
      "Loss:  0.23291838210076093 0.24669118523597716\n",
      "Epoch: 7. batch_idx: 300\n",
      "Loss:  0.25544165782630446 0.2500422433018684\n",
      "Epoch: 7. batch_idx: 400\n",
      "Loss:  0.26941130965948107 0.24771423354744912\n",
      "Epoch: 7. batch_idx: 500\n",
      "Loss:  0.24866231977939607 0.24518126830458642\n",
      "Epoch: 7. batch_idx: 600\n",
      "Loss:  0.2652916080504656 0.2643432343006134\n",
      "Epoch: 8. batch_idx: 100\n",
      "Loss:  0.2528194510191679 0.2411960106343031\n",
      "Epoch: 8. batch_idx: 200\n",
      "Loss:  0.258222688883543 0.24963407315313815\n",
      "Epoch: 8. batch_idx: 300\n",
      "Loss:  0.2342566105350852 0.22900019355118276\n",
      "Epoch: 8. batch_idx: 400\n",
      "Loss:  0.238444059304893 0.22925160206854345\n",
      "Epoch: 8. batch_idx: 500\n",
      "Loss:  0.25657577961683276 0.23860605224967002\n",
      "Epoch: 8. batch_idx: 600\n",
      "Loss:  0.24873896338045598 0.2427306104451418\n",
      "Epoch: 9. batch_idx: 100\n",
      "Loss:  0.23034186489880085 0.22865243323147297\n",
      "Epoch: 9. batch_idx: 200\n",
      "Loss:  0.22980264879763126 0.24453446783125402\n",
      "Epoch: 9. batch_idx: 300\n",
      "Loss:  0.23408940741792322 0.21565145879983902\n",
      "Epoch: 9. batch_idx: 400\n",
      "Loss:  0.2482684285007417 0.24016337379813193\n",
      "Epoch: 9. batch_idx: 500\n",
      "Loss:  0.2418627443164587 0.228750215806067\n",
      "Epoch: 9. batch_idx: 600\n",
      "Loss:  0.25846346765756606 0.24218494776636362\n",
      "Epoch: 10. batch_idx: 100\n",
      "Loss:  0.23856803476810456 0.23993355132639407\n",
      "Epoch: 10. batch_idx: 200\n",
      "Loss:  0.23031531766057015 0.23092969175428152\n",
      "Epoch: 10. batch_idx: 300\n",
      "Loss:  0.22127048462629317 0.22608648363500833\n",
      "Epoch: 10. batch_idx: 400\n",
      "Loss:  0.24488233346492053 0.23673286497592927\n",
      "Epoch: 10. batch_idx: 500\n",
      "Loss:  0.24180097546428442 0.2280031728371978\n",
      "Epoch: 10. batch_idx: 600\n",
      "Loss:  0.22401001513004304 0.2134199954941869\n",
      "Eval:  (0.2960171272420579, 0.30302567791881835, array([0.88392501]), array([0.87913841]))\n",
      "Epoch: 11. batch_idx: 100\n",
      "Loss:  0.21898234711959957 0.22400459438562392\n",
      "Epoch: 11. batch_idx: 200\n",
      "Loss:  0.23623457547277213 0.23526677377521993\n",
      "Epoch: 11. batch_idx: 300\n",
      "Loss:  0.20727516777813434 0.20981071788817643\n",
      "Epoch: 11. batch_idx: 400\n",
      "Loss:  0.22395758111029862 0.21054953802376986\n",
      "Epoch: 11. batch_idx: 500\n",
      "Loss:  0.23172182317823173 0.2257242226973176\n",
      "Epoch: 11. batch_idx: 600\n",
      "Loss:  0.2291165517270565 0.22657276451587677\n",
      "Epoch: 12. batch_idx: 100\n",
      "Loss:  0.22772585939615964 0.21284583274275065\n",
      "Epoch: 12. batch_idx: 200\n",
      "Loss:  0.21117643646895887 0.20345300231128932\n",
      "Epoch: 12. batch_idx: 300\n",
      "Loss:  0.22229427069425584 0.22473411804065108\n",
      "Epoch: 12. batch_idx: 400\n",
      "Loss:  0.22765948832035066 0.2217979009822011\n",
      "Epoch: 12. batch_idx: 500\n",
      "Loss:  0.2246453064866364 0.20843064848333598\n",
      "Epoch: 12. batch_idx: 600\n",
      "Loss:  0.2314461662992835 0.22790688909590245\n",
      "Epoch: 13. batch_idx: 100\n",
      "Loss:  0.20001366440206766 0.20332589380443097\n",
      "Epoch: 13. batch_idx: 200\n",
      "Loss:  0.21703509230166673 0.2083623318746686\n",
      "Epoch: 13. batch_idx: 300\n",
      "Loss:  0.22349307525902987 0.20798934314399958\n",
      "Epoch: 13. batch_idx: 400\n",
      "Loss:  0.21322685036808253 0.20975563038140535\n",
      "Epoch: 13. batch_idx: 500\n",
      "Loss:  0.20891474969685078 0.21474213566631078\n",
      "Epoch: 13. batch_idx: 600\n",
      "Loss:  0.24143335293978452 0.22403575647622348\n",
      "Epoch: 14. batch_idx: 100\n",
      "Loss:  0.2134638001397252 0.21369344115257263\n",
      "Epoch: 14. batch_idx: 200\n",
      "Loss:  0.21244615711271764 0.21724327638745308\n",
      "Epoch: 14. batch_idx: 300\n",
      "Loss:  0.2143236929178238 0.20664525166153908\n",
      "Epoch: 14. batch_idx: 400\n",
      "Loss:  0.2254837877675891 0.20454387914389371\n",
      "Epoch: 14. batch_idx: 500\n",
      "Loss:  0.20709104254841804 0.2062843234837055\n",
      "Epoch: 14. batch_idx: 600\n",
      "Loss:  0.2071253915131092 0.2147897269204259\n",
      "Epoch: 15. batch_idx: 100\n",
      "Loss:  0.20699487959966065 0.183624529838562\n",
      "Epoch: 15. batch_idx: 200\n",
      "Loss:  0.2028550705127418 0.1988040955737233\n",
      "Epoch: 15. batch_idx: 300\n",
      "Loss:  0.1971360666677356 0.2126406201720238\n",
      "Epoch: 15. batch_idx: 400\n",
      "Loss:  0.2153484422713518 0.20119605377316474\n",
      "Epoch: 15. batch_idx: 500\n",
      "Loss:  0.2070491488277912 0.20686818124726414\n",
      "Epoch: 15. batch_idx: 600\n",
      "Loss:  0.2132892278395593 0.20462819945067168\n",
      "Eval:  (0.30160546371606506, 0.2990281580454984, array([0.88113283]), array([0.88392501]))\n",
      "Epoch: 16. batch_idx: 100\n",
      "Loss:  0.20613650787621737 0.21410928616300226\n",
      "Epoch: 16. batch_idx: 200\n",
      "Loss:  0.1999294704757631 0.1928131066262722\n",
      "Epoch: 16. batch_idx: 300\n",
      "Loss:  0.20433046486228704 0.2084036072343588\n",
      "Epoch: 16. batch_idx: 400\n",
      "Loss:  0.1940306193009019 0.1915052224136889\n",
      "Epoch: 16. batch_idx: 500\n",
      "Loss:  0.2061843504756689 0.18566461611539126\n",
      "Epoch: 16. batch_idx: 600\n",
      "Loss:  0.1860313502699137 0.1947285808250308\n",
      "Epoch: 17. batch_idx: 100\n",
      "Loss:  0.19794667774811386 0.1881820172443986\n",
      "Epoch: 17. batch_idx: 200\n",
      "Loss:  0.18336304727941752 0.20058865584433078\n",
      "Epoch: 17. batch_idx: 300\n",
      "Loss:  0.20669875174760818 0.20059961888939143\n",
      "Epoch: 17. batch_idx: 400\n",
      "Loss:  0.18140179369598627 0.18539605859667063\n",
      "Epoch: 17. batch_idx: 500\n",
      "Loss:  0.19116742312908172 0.19122660476714373\n",
      "Epoch: 17. batch_idx: 600\n",
      "Loss:  0.2007860191166401 0.19547719649970532\n",
      "Epoch: 18. batch_idx: 100\n",
      "Loss:  0.1943885136768222 0.18805481180548667\n",
      "Epoch: 18. batch_idx: 200\n",
      "Loss:  0.18151117162778974 0.18187648754566907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18. batch_idx: 300\n",
      "Loss:  0.17963618656620384 0.18682645505294204\n",
      "Epoch: 18. batch_idx: 400\n",
      "Loss:  0.2007624620385468 0.1863664785400033\n",
      "Epoch: 18. batch_idx: 500\n",
      "Loss:  0.21408594775944947 0.1808680745214224\n",
      "Epoch: 18. batch_idx: 600\n",
      "Loss:  0.18257142029702664 0.20344378707930447\n",
      "Epoch: 19. batch_idx: 100\n",
      "Loss:  0.20267280858010053 0.19696943862363697\n",
      "Epoch: 19. batch_idx: 200\n",
      "Loss:  0.18093894310295583 0.18581741668283938\n",
      "Epoch: 19. batch_idx: 300\n",
      "Loss:  0.19211545936763286 0.18406942227855325\n",
      "Epoch: 19. batch_idx: 400\n",
      "Loss:  0.181600288413465 0.1796147649176419\n",
      "Epoch: 19. batch_idx: 500\n",
      "Loss:  0.1803071815147996 0.18550939083099366\n",
      "Epoch: 19. batch_idx: 600\n",
      "Loss:  0.17854316072538495 0.17236405927687884\n"
     ]
    }
   ],
   "source": [
    "compnet1 = train_compnet(cpnet_HP, DS, train_ds=train_encoding_ds, test_ds=test_encoding_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Evaluation Report ------------------\n",
      "Total Accuracy: 0.8835261268448344\n",
      "After 157 batches and 2507 test points\n",
      "\n",
      "Metrics for the same class:\n",
      "Avg loss: 0.30215612478601706\n",
      "Precision: 0.8832204065364687\n",
      "Recall: 0.8839250099720782\n",
      "F1 Score: 0.8835725677830941\n",
      "\n",
      "Metrics for the diff class:\n",
      "Avg loss: 0.31159593186275975\n",
      "Precision: 0.8838323353293414\n",
      "Recall: 0.8831272437175908\n",
      "F1 Score: 0.8834796488427774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(47.438511591404676,\n",
       " 48.92056130245328,\n",
       " 2216.0,\n",
       " 2214.0,\n",
       " 291.0,\n",
       " 293.0,\n",
       " 157,\n",
       " 2507)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_compnet(compnet1, test_encoding_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig X Shape: torch.Size([16, 3, 683])\n",
      "X shape torch.Size([96, 3, 683]) num_X: 16\n",
      "cls_samples Shape torch.Size([96, 3, 683])\n",
      "0 0.036418088 tensor(9.8927, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0.009972334 ecnoding=; torch.Size([96, 256])\n",
      "torch.Size([96, 1])\n",
      "preds shape torch.Size([16, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8061, 0.6746],\n",
       "         [0.3498, 0.4677],\n",
       "         [0.3448, 0.1721]],\n",
       "\n",
       "        [[0.2337, 0.8045],\n",
       "         [0.8321, 0.1947],\n",
       "         [0.0540, 0.2732]],\n",
       "\n",
       "        [[0.3428, 0.2974],\n",
       "         [0.8226, 0.2165],\n",
       "         [0.2875, 0.1139]],\n",
       "\n",
       "        [[0.3850, 0.2115],\n",
       "         [0.8655, 0.7313],\n",
       "         [0.5704, 0.8183]],\n",
       "\n",
       "        [[0.2319, 0.3773],\n",
       "         [0.8274, 0.6851],\n",
       "         [0.2484, 0.4167]],\n",
       "\n",
       "        [[0.6508, 0.5369],\n",
       "         [0.7720, 0.4706],\n",
       "         [0.1952, 0.3059]],\n",
       "\n",
       "        [[0.0557, 0.3426],\n",
       "         [0.5807, 0.0748],\n",
       "         [0.0276, 0.0970]],\n",
       "\n",
       "        [[0.8802, 0.4073],\n",
       "         [0.4812, 0.6655],\n",
       "         [0.1131, 0.5441]],\n",
       "\n",
       "        [[0.7369, 0.7889],\n",
       "         [0.3450, 0.7472],\n",
       "         [0.4409, 0.1384]],\n",
       "\n",
       "        [[0.0769, 0.1446],\n",
       "         [0.3961, 0.2290],\n",
       "         [0.0677, 0.1873]],\n",
       "\n",
       "        [[0.5449, 0.3570],\n",
       "         [0.7489, 0.0524],\n",
       "         [0.0794, 0.0586]],\n",
       "\n",
       "        [[0.1462, 0.7153],\n",
       "         [0.5541, 0.2122],\n",
       "         [0.2986, 0.1806]],\n",
       "\n",
       "        [[0.6321, 0.3198],\n",
       "         [0.5154, 0.2456],\n",
       "         [0.1514, 0.1232]],\n",
       "\n",
       "        [[0.3628, 0.4811],\n",
       "         [0.6409, 0.4134],\n",
       "         [0.3313, 0.7673]],\n",
       "\n",
       "        [[0.3008, 0.3829],\n",
       "         [0.2428, 0.1353],\n",
       "         [0.1977, 0.0500]],\n",
       "\n",
       "        [[0.1209, 0.4895],\n",
       "         [0.8148, 0.3873],\n",
       "         [0.0299, 0.1795]]], device='cuda:0', grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl1 = DataLoader(pn_test_ds, batch_size=16, shuffle=False)\n",
    "dl2 = DataLoader(pn_test_ds, batch_size=6, shuffle=False)\n",
    "x = next(iter(dl1))[0]\n",
    "cls_samples = next(iter(dl2))[0]\n",
    "classify(x, cls_samples, 3, pn_autodecoder, compnet1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an AutoDecoder_Ensemble with an ensemble of randomly initialized CompNets\n",
    "\n",
    "The individual ensemble nets are the same CompNets but are initialized with differing initializations to the classifiers independent as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_HP = HyperParameter(lr=0.001, batch_size=16)\n",
    "ensemble_DS = DirectorySetting(CLASSIFIER_TRAINED_WEIGHT_DIR='./ensemble_classifier_trained_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. batch_idx: 100\n",
      "Loss:  0.8218167006969452 0.5923093342781067\n",
      "Epoch: 0. batch_idx: 200\n",
      "Loss:  0.6958015757799149 0.6790514653921127\n",
      "Epoch: 0. batch_idx: 300\n",
      "Loss:  0.5258185368776321 0.7428328293561935\n",
      "Epoch: 0. batch_idx: 400\n",
      "Loss:  0.37643086373806 0.6744326055049896\n",
      "Epoch: 0. batch_idx: 500\n",
      "Loss:  0.2867506204545498 0.6066402149200439\n",
      "Epoch: 0. batch_idx: 600\n",
      "Loss:  0.2583423684537411 0.5624333655834198\n",
      "------------------ Evaluation Report ------------------\n",
      "Total Accuracy: 0.9365775827682489\n",
      "After 157 batches and 2507 test points\n",
      "\n",
      "Metrics for the same class:\n",
      "Avg loss: 0.2358952956214832\n",
      "Precision: 0.8990156762668611\n",
      "Recall: 0.9836457917830076\n",
      "F1 Score: 0.9394285714285713\n",
      "\n",
      "Metrics for the diff class:\n",
      "Avg loss: 0.48819872926754554\n",
      "Precision: 0.9819462791721708\n",
      "Recall: 0.8895093737534903\n",
      "F1 Score: 0.9334449560485558\n",
      "Eval:  (37.03556141257286, 76.64720049500465, 2466.0, 2230.0, 41.0, 277.0, 157, 2507)\n",
      "Epoch: 1. batch_idx: 100\n",
      "Loss:  0.20541204780340194 0.4753797325491905\n",
      "Epoch: 1. batch_idx: 200\n",
      "Loss:  0.17846414558589457 0.433234286904335\n",
      "Epoch: 1. batch_idx: 300\n",
      "Loss:  0.1547908444330096 0.40351844966411593\n",
      "Epoch: 1. batch_idx: 400\n",
      "Loss:  0.16021181181073188 0.36934830963611603\n",
      "Epoch: 1. batch_idx: 500\n",
      "Loss:  0.15634659864008427 0.3755010223388672\n",
      "Epoch: 1. batch_idx: 600\n",
      "Loss:  0.1391781197115779 0.3363396918773651\n",
      "Epoch: 2. batch_idx: 100\n",
      "Loss:  0.1168396077118814 0.30274182453751564\n",
      "Epoch: 2. batch_idx: 200\n",
      "Loss:  0.13274434234946966 0.2924363113939762\n",
      "Epoch: 2. batch_idx: 300\n",
      "Loss:  0.1134888886846602 0.2725349871814251\n",
      "Epoch: 2. batch_idx: 400\n",
      "Loss:  0.11862315641716123 0.26062425211071966\n",
      "Epoch: 2. batch_idx: 500\n",
      "Loss:  0.09622564667835831 0.2554376359283924\n",
      "Epoch: 2. batch_idx: 600\n",
      "Loss:  0.10540158207528293 0.24484374567866327\n",
      "Epoch: 3. batch_idx: 100\n",
      "Loss:  0.0899004546366632 0.21275793746113777\n",
      "Epoch: 3. batch_idx: 200\n",
      "Loss:  0.09416527467779816 0.2032986129820347\n",
      "Epoch: 3. batch_idx: 300\n",
      "Loss:  0.09485430985689164 0.20575201250612735\n",
      "Epoch: 3. batch_idx: 400\n",
      "Loss:  0.08106042035855353 0.19366139471530913\n",
      "Epoch: 3. batch_idx: 500\n",
      "Loss:  0.07377082677558064 0.17745630629360676\n",
      "Epoch: 3. batch_idx: 600\n",
      "Loss:  0.08276843198575079 0.1754742806404829\n"
     ]
    }
   ],
   "source": [
    "ensemble1 = train_compnet(ensemble_HP, ensemble_DS, \n",
    "                          train_ds=train_encoding_ds, \n",
    "                          test_ds=test_encoding_ds,\n",
    "                          compnet=EnsembleCompNet())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Ensemble CompNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Evaluation Report ------------------\n",
      "Total Accuracy: 0.9754686876745113\n",
      "After 157 batches and 2507 test points\n",
      "\n",
      "Metrics for the same class:\n",
      "Avg loss: 0.09116257365887902\n",
      "Precision: 0.9707740916271722\n",
      "Recall: 0.9804547267650578\n",
      "F1 Score: 0.975590394919627\n",
      "\n",
      "Metrics for the diff class:\n",
      "Avg loss: 0.1923845513326347\n",
      "Precision: 0.9802578565672845\n",
      "Recall: 0.9704826485839649\n",
      "F1 Score: 0.9753457606734817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14.312524064444005, 30.204374559223652, 2458.0, 2433.0, 49.0, 74.0, 157, 2507)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_compnet(ensemble1, test_encoding_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble CompNets with NNs with increasing depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_HP = HyperParameter(lr=0.001, batch_size=16)\n",
    "ensemble_DS = DirectorySetting(CLASSIFIER_TRAINED_WEIGHT_DIR='./ensemble_classifier_trained_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ensemble_compnet_diff = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(ensemble_HP.encoding_size, 64),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(64, 1),\n",
    "        nn.Sigmoid()),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(ensemble_HP.encoding_size, 128),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(64, 1),\n",
    "        nn.Sigmoid()),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(ensemble_HP.encoding_size, 256),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(64, 1),\n",
    "        nn.Sigmoid()),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(ensemble_HP.encoding_size, 512),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(64, 1),\n",
    "        nn.Sigmoid()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. batch_idx: 100\n",
      "Loss:  0.6069590747356415 0.5226758068799973\n",
      "Epoch: 0. batch_idx: 200\n",
      "Loss:  0.5611259976029396 0.39650227546691896\n",
      "Epoch: 0. batch_idx: 300\n",
      "Loss:  0.524142044186592 0.3353692851960659\n",
      "Epoch: 0. batch_idx: 400\n",
      "Loss:  0.49901885628700254 0.313214952647686\n",
      "Epoch: 0. batch_idx: 500\n",
      "Loss:  0.4597411745786667 0.28801489874720576\n",
      "Epoch: 0. batch_idx: 600\n",
      "Loss:  0.43802073508501055 0.2499128845334053\n",
      "------------------ Evaluation Report ------------------\n",
      "Total Accuracy: 0.9415636218587954\n",
      "After 157 batches and 2507 test points\n",
      "\n",
      "Metrics for the same class:\n",
      "Avg loss: 0.4268379583480252\n",
      "Precision: 0.9738869863013698\n",
      "Recall: 0.9074591144794575\n",
      "F1 Score: 0.9395003097253768\n",
      "\n",
      "Metrics for the diff class:\n",
      "Avg loss: 0.23827302759620034\n",
      "Precision: 0.9133681852128454\n",
      "Recall: 0.9756681292381332\n",
      "F1 Score: 0.9434908389585341\n",
      "Eval:  (67.01355946063995, 37.408865332603455, 2275.0, 2446.0, 232.0, 61.0, 157, 2507)\n",
      "Epoch: 1. batch_idx: 100\n",
      "Loss:  0.4035860705375671 0.21816798292100428\n",
      "Epoch: 1. batch_idx: 200\n",
      "Loss:  0.39044531106948854 0.21704174675047397\n",
      "Epoch: 1. batch_idx: 300\n",
      "Loss:  0.35128047615289687 0.2061162032186985\n",
      "Epoch: 1. batch_idx: 400\n",
      "Loss:  0.3505309204757214 0.1908326730877161\n",
      "Epoch: 1. batch_idx: 500\n",
      "Loss:  0.32672894522547724 0.19190269142389296\n",
      "Epoch: 1. batch_idx: 600\n",
      "Loss:  0.31121939554810524 0.1737293642014265\n",
      "Epoch: 2. batch_idx: 100\n",
      "Loss:  0.2865927354991436 0.16240012414753438\n",
      "Epoch: 2. batch_idx: 200\n",
      "Loss:  0.2698232674598694 0.14338651951402426\n",
      "Epoch: 2. batch_idx: 300\n",
      "Loss:  0.2596932727098465 0.157068115696311\n",
      "Epoch: 2. batch_idx: 400\n",
      "Loss:  0.2582130324840546 0.1442495658993721\n",
      "Epoch: 2. batch_idx: 500\n",
      "Loss:  0.23764713034033774 0.13656154423952102\n",
      "Epoch: 2. batch_idx: 600\n",
      "Loss:  0.22859659358859064 0.13296196199953556\n",
      "Epoch: 3. batch_idx: 100\n",
      "Loss:  0.2108368444442749 0.11954003568738698\n",
      "Epoch: 3. batch_idx: 200\n",
      "Loss:  0.2138703180849552 0.11449058383703231\n",
      "Epoch: 3. batch_idx: 300\n",
      "Loss:  0.19209585100412369 0.11281893352046608\n",
      "Epoch: 3. batch_idx: 400\n",
      "Loss:  0.1886816668510437 0.11204563450068235\n",
      "Epoch: 3. batch_idx: 500\n",
      "Loss:  0.19691486805677413 0.10658632716163993\n",
      "Epoch: 3. batch_idx: 600\n",
      "Loss:  0.21533737860620022 0.12158524803817272\n"
     ]
    }
   ],
   "source": [
    "ensemble2 = train_compnet(ensemble_HP, ensemble_DS, \n",
    "                          train_ds=train_encoding_ds, \n",
    "                          test_ds=test_encoding_ds,\n",
    "                          compnet=EnsembleCompNet(comp_net=ensemble_compnet_diff,\n",
    "                                                 num_ensemble=len(ensemble_compnet_diff))\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Ensemble 2 CompNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Evaluation Report ------------------\n",
      "Total Accuracy: 0.9615077782209812\n",
      "After 157 batches and 2507 test points\n",
      "\n",
      "Metrics for the same class:\n",
      "Avg loss: 0.21810709794235836\n",
      "Precision: 0.9792874896437448\n",
      "Recall: 0.9429597128041484\n",
      "F1 Score: 0.9607803292013818\n",
      "\n",
      "Metrics for the diff class:\n",
      "Avg loss: 0.10121579959419123\n",
      "Precision: 0.945\n",
      "Recall: 0.9800558436378142\n",
      "F1 Score: 0.9622087331114156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(34.242814376950264,\n",
       " 15.890880536288023,\n",
       " 2364.0,\n",
       " 2457.0,\n",
       " 143.0,\n",
       " 50.0,\n",
       " 157,\n",
       " 2507)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_compnet(ensemble2, test_encoding_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "decoderless",
   "language": "python",
   "name": "dencoder_less_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
