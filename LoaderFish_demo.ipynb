{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdqm.notebook not found. Try updating tdqm. Reverting to base tqdm No module named 'tqdm.notebook'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Try importing tqdm notebook\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ModuleNotFoundError as e:\n",
    "    print('tdqm.notebook not found. Try updating tdqm. Reverting to base tqdm',e)\n",
    "    import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import joblib # To save models\n",
    "from sklearn.metrics import hinge_loss\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# CUSTOM IMPORTS\n",
    "import LoaderFish                                               # function to gen custom point clouds (Wang et. al)\n",
    "from utils import chamfer_loss                                  # utility functions\n",
    "\n",
    "# TODO UNCOMMENT when implemented\n",
    "# from utils import chamfer_loss, HyperParameter, DirectorySetting\n",
    "\n",
    "# TODO UNCOMMENT when implemented\n",
    "# from nn_modles import AutoDecoder, CompNet, EnsembleCompNet       # autodecoder, comp_net, and ensemble_compnet modules\n",
    "# from dataset_module import PointDriftDS, EncodingDS, EncodingDS   # dataset modules\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0,
     28
    ]
   },
   "outputs": [],
   "source": [
    "class HyperParameter:\n",
    "    def __init__(self,\n",
    "                 l2_reg=None,\n",
    "                 encoding_size=256,\n",
    "                 encoding_iters=50,\n",
    "                 num_point_cloud=3,\n",
    "                 epochs=4,\n",
    "                 lr=0.00001,\n",
    "                 batch_size=32):\n",
    "\n",
    "        self.l2_reg = l2_reg\n",
    "        self.learning_rate = lr\n",
    "        self.encoding_size = encoding_size\n",
    "        self.encoding_iters = encoding_iters\n",
    "        self.num_point_cloud = num_point_cloud\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"l2_reg: {self.l2_reg}\\n\" + \\\n",
    "               f\"learning_rate: {self.learning_rate}\\n\" + \\\n",
    "               f\"encoding_size: {self.encoding_size}\\n\" + \\\n",
    "               f\"encoding_iters: {self.encoding_iters}\\n\" + \\\n",
    "               f\"num_point_cloud: {self.num_point_cloud}\\n\" + \\\n",
    "               f\"epochs: {self.epochs}\\n\" + \\\n",
    "               f\"batch_size: {self.batch_size}\\n\"\n",
    "\n",
    "\n",
    "class DirectorySetting:\n",
    "\n",
    "    def __init__(self,\n",
    "                 DATA_DIR=\"./data\",\n",
    "                 OUTPUT_DIR=\"./tranformed/\",\n",
    "                 AUTODECODER_TRAINED_WEIGHT_DIR=\"./autodecoder_trained_weights\",\n",
    "                 CLASSIFIER_TRAINED_WEIGHT_DIR=\"./classifier_trained_weights\"):\n",
    "\n",
    "        self.AUTODECODER_TRAINED_WEIGHT_DIR = AUTODECODER_TRAINED_WEIGHT_DIR\n",
    "        self.CLASSIFIER_TRAINED_WEIGHT_DIR = CLASSIFIER_TRAINED_WEIGHT_DIR\n",
    "        self.OUTPUT_DIR = OUTPUT_DIR\n",
    "        self.DATA_DIR = DATA_DIR\n",
    "\n",
    "        os.makedirs(self.AUTODECODER_TRAINED_WEIGHT_DIR, exist_ok=True)\n",
    "        os.makedirs(self.CLASSIFIER_TRAINED_WEIGHT_DIR, exist_ok=True)\n",
    "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DATA_DIR: {self.DATA_DIR}\\n\" + \\\n",
    "               f\"OUTPUT_DIR: {self.OUTPUT_DIR}\\n\" + \\\n",
    "               f\"AUTODECODER_TRAINED_WEIGHT_DIR: {self.AUTODECODER_TRAINED_WEIGHT_DIR}\\n\" + \\\n",
    "               f\"CLASSIFIER_TRAINED_WEIGHT_DIR: {self.CLASSIFIER_TRAINED_WEIGHT_DIR}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Cuda device and SEED val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU training available\n",
      "Index of CUDA device in use is 0\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    # inbuilt cudnn auto-tuner searches for best algorithm for hardware\n",
    "    # cuddn.benchmark should be set to True when our input size does not vary\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"GPU training available\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Index of CUDA device in use is {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"GPU training NOT available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Can only train on CPU\")\n",
    "\n",
    "DEBUG = True\n",
    "SEED  = 17*19\n",
    "\n",
    "HP = HyperParameter(epochs=10,\n",
    "                    l2_reg=None, \n",
    "                    batch_size=16,\n",
    "                    num_point_cloud=3,\n",
    "                    encoding_iters=1000, \n",
    "                    encoding_size=256)\n",
    "DS = DirectorySetting()\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     22,
     38
    ]
   },
   "outputs": [],
   "source": [
    "class AutoDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoDecoder NN to learn point drift (latent encoding) between two 3D shapes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  encoding_dim=256, point_dim=3):\n",
    "        super(AutoDecoder, self).__init__()\n",
    "        self.fc1 = nn.Conv1d(encoding_dim + point_dim, 128, 1)\n",
    "        self.fc2 = nn.Conv1d(128, 64, 1)\n",
    "        self.fc3 = nn.Conv1d(64, point_dim, 1)\n",
    "\n",
    "    def forward(self, X, encoding):\n",
    "        num_points = X.shape[-1]  # num of points in each shape\n",
    "        enc = encoding.unsqueeze(-1).repeat(1, 1, num_points)\n",
    "        X_enc = torch.cat([X, enc], 1)\n",
    "        X_enc = F.leaky_relu(self.fc1(X_enc))\n",
    "        X_enc = F.leaky_relu(self.fc2(X_enc))\n",
    "\n",
    "        # Return the drift from obj X determined by the latent encoding\n",
    "        return X + self.fc3(X_enc)\n",
    "\n",
    "\n",
    "class CompNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Ingests the latent encoding of two 3D objects \n",
    "    and outputs the similarity score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding_size=256):\n",
    "        super(CompNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(encoding_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, encoding):\n",
    "        X = F.leaky_relu(self.fc1(encoding))\n",
    "        return torch.sigmoid(self.fc2(X))\n",
    "\n",
    "\n",
    "class EnsembleCompNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Ingests the latent encoding of two 3D objects \n",
    "    and outputs the similarity score using an ensemble of CompNets\n",
    "    Stacked Ensemble\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, comp_net=CompNet, num_ensemble=5, encoding_dim=256, seed_val=SEED):\n",
    "        \"\"\"\n",
    "        if comp_net is a module, EnsembleCompNet creates num_ensemble*comp_net NN modules\n",
    "        if comp_net is a list of modules, EnsembleCompNet iterates through comp_net to get the NN modules\n",
    "        \"\"\"\n",
    "        super(EnsembleCompNet, self).__init__()\n",
    "        self.ensemble_compnet = nn.ModuleList()\n",
    "\n",
    "        if isinstance(comp_net, list):\n",
    "            if num_ensemble != len(comp_net):\n",
    "                raise IndexError(\n",
    "                    f\"Length of comp_nets: {len(comp_net)} and num_ensemble: {num_ensemble} do not match\")\n",
    "            comp_net_list = comp_net\n",
    "            for i in range(num_ensemble):\n",
    "                self.ensemble_compnet.append(comp_net_list[i])\n",
    "        else:\n",
    "            for i in range(num_ensemble):\n",
    "                torch.manual_seed(seed_val*i+1)\n",
    "                if use_cuda:\n",
    "                    torch.cuda.manual_seed(seed_val*i+1)\n",
    "                self.ensemble_compnet.append(comp_net(encoding_dim))\n",
    "        self.final = nn.Linear(num_ensemble, 1)\n",
    "\n",
    "    def forward(self, encoding):\n",
    "        \"\"\" Returns the final value of the results after a nn.Linear layer \"\"\"\n",
    "        total_pred = torch.cat([net(encoding)\n",
    "                                for net in self.ensemble_compnet])\n",
    "        total_pred = total_pred.reshape(-1, len(self.ensemble_compnet))\n",
    "\n",
    "        return torch.sigmoid(self.final(total_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Modules and DataLoader implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     17,
     22,
     53
    ]
   },
   "outputs": [],
   "source": [
    "class PointNetDS(Dataset):\n",
    "    \"\"\"\n",
    "    Create train dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, sampling_interval=3):\n",
    "        # sample every sampling_interval-th point to speed up\n",
    "        self.data = data.transpose((0, 2, 1))[:, :, ::sampling_interval]\n",
    "        self.data = torch.from_numpy(self.data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class PointDriftDS(Dataset):\n",
    "    \"\"\"\n",
    "    Pairs each shape with one shape from the same class and one shape from a different class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, labels, sampling_interval=3):\n",
    "        # sample every sampling_interval-th point to speed up\n",
    "        self.data = data.transpose((0, 2, 1))[:, :, ::sampling_interval]\n",
    "        self.labels = labels.squeeze()\n",
    "\n",
    "        self.same_cls = []\n",
    "        self.diff_cls = []\n",
    "        idx_arr = np.arange(self.data.shape[0])\n",
    "        same_idx = []\n",
    "        diff_idx = []\n",
    "        for i in range(self.labels.max() + 1):\n",
    "            same_idx.append(idx_arr[self.labels == i])\n",
    "            diff_idx.append(idx_arr[self.labels != i])\n",
    "        for i in range(data.shape[0]):\n",
    "            same = same_idx[self.labels[i]]\n",
    "            diff = diff_idx[self.labels[i]]\n",
    "            self.same_cls.append(same[random.randint(0, len(same) - 1)])\n",
    "            self.diff_cls.append(diff[random.randint(0, len(diff) - 1)])\n",
    "        self.data = torch.from_numpy(self.data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx]\n",
    "        same_cls_data = self.data[self.same_cls[idx]]\n",
    "        diff_cls_data = self.data[self.diff_cls[idx]]\n",
    "\n",
    "        return X, same_cls_data, diff_cls_data, idx\n",
    "\n",
    "\n",
    "class EncodingDS(Dataset):\n",
    "    \"\"\"\n",
    "    Generate encoding for each pair of shapes in the PointDriftDS\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, PDDS, autodecoder, latent_size=256):\n",
    "        self.PointDriftDS = PDDS\n",
    "        self.autodecoder = autodecoder\n",
    "        self.latent_size = latent_size\n",
    "        self.same_cls = torch.zeros((len(self.PointDriftDS), latent_size))\n",
    "        self.diff_cls = torch.zeros((len(self.PointDriftDS), latent_size))\n",
    "\n",
    "    def train_encodings(self, num_iterations=50, lr=0.01, l2_reg=False, batch_size=16):\n",
    "        dl = DataLoader(self.PointDriftDS,\n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "        i = 0\n",
    "        batch_cnt = 0\n",
    "        same_cls_loss = 0.0\n",
    "        diff_cls_loss = 0.0\n",
    "        self.autodecoder.eval()\n",
    "\n",
    "        for batch_idx, (x, y, z, idx) in enumerate(dl):\n",
    "            j = i + len(idx)\n",
    "            loss, encoding = find_encoding(x, y, self.autodecoder, encoding_iters=num_iterations,\n",
    "                                           encoding_size=self.latent_size, lr=lr, l2_reg=l2_reg,)\n",
    "            same_cls_loss += loss\n",
    "            self.same_cls[i:j] = encoding\n",
    "            loss, encoding = find_encoding(x, z, self.autodecoder, encoding_iters=num_iterations,\n",
    "                                           encoding_size=self.latent_size, lr=lr, l2_reg=l2_reg,)\n",
    "            diff_cls_loss += loss\n",
    "            self.diff_cls[i:j] = encoding\n",
    "\n",
    "            i = j\n",
    "            batch_cnt += 1\n",
    "        print(\"Encodings trained\")\n",
    "        return (self.same_cls, self.diff_cls, same_cls_loss / batch_cnt, diff_cls_loss / batch_cnt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.PointDriftDS)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (*self.PointDriftDS[idx], self.same_cls[idx], self.diff_cls[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def find_encoding(X, y, autodecoder, encoding_iters=300,\n",
    "                  encoding_size=256, lr=5e-4, l2_reg=False):\n",
    "    \"\"\"\n",
    "    Generate the encoding (latent vector) for each data in X\n",
    "    \"\"\"\n",
    "\n",
    "    def _adjust_lr(initial_lr, optimizer, num_iters, decreased_by, adjust_lr_every):\n",
    "\n",
    "        lr = initial_lr * ((1 / decreased_by) **\n",
    "                           (num_iters // adjust_lr_every))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    decreased_by = 10\n",
    "    adjust_lr_every = encoding_iters // 2\n",
    "\n",
    "    encoding = torch.ones(X.shape[0], encoding_size).normal_(\n",
    "        mean=0, std=1.0 / math.sqrt(encoding_size)).cuda()\n",
    "\n",
    "    encoding.requires_grad = True\n",
    "    optimizer = torch.optim.Adam([encoding], lr=lr)\n",
    "    loss_num = 0\n",
    "\n",
    "    for i in range(encoding_iters):\n",
    "        autodecoder.eval()\n",
    "        _adjust_lr(lr, optimizer, i, decreased_by, adjust_lr_every)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = autodecoder(X, encoding)\n",
    "        loss = chamfer_loss(y_pred, y, ps=y.shape[-1])\n",
    "\n",
    "        if l2_reg:\n",
    "            loss += 1e-4 * torch.mean(encoding.pow(2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(i, loss.cpu().data.numpy(), encoding.norm())\n",
    "        loss_num = loss.cpu().data.numpy()\n",
    "\n",
    "    return loss_num, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoder(HP, DS, train_ds, test_ds=None, decoder=None, save_wt_fname='decoder.pth'):\n",
    "    \"\"\" \n",
    "    Default training is for 3D point dimensions\n",
    "    \n",
    "    Suggested Settings\n",
    "        EPOCHS = 10\n",
    "        point_dim = 3\n",
    "        batch_size = 16\n",
    "        learning_rate = 0.001\n",
    "        encoding_size = 256\n",
    "        \n",
    "    Set save_wt_fname to None to disable weight saves\n",
    "    \"\"\"\n",
    "    EPOCHS = HP.epochs\n",
    "    point_dim = HP.num_point_cloud\n",
    "    batch_size = HP.batch_size\n",
    "    encoding_size = HP.encoding_size\n",
    "    lr = HP.learning_rate\n",
    "    \n",
    "    if decoder is None:\n",
    "        adnet = AutoDecoder(encoding_size, point_dim)\n",
    "    else:\n",
    "        adnet = decoder\n",
    "    adnet = adnet.cuda()\n",
    "\n",
    "    # encodings for same class transformation\n",
    "    same_encoding = torch.nn.Embedding(\n",
    "        len(train_ds), encoding_size, max_norm=1.0)\n",
    "    # init encoding with Kaiming Initialization\n",
    "    torch.nn.init.normal_(same_encoding.weight.data,\n",
    "                          0.0,\n",
    "                          1.0 / math.sqrt(encoding_size))\n",
    "\n",
    "    # encodings for different class transformation\n",
    "    diff_encoding = torch.nn.Embedding(\n",
    "        len(train_ds), encoding_size, max_norm=1.0)\n",
    "    # init encoding with Kaiming Initialization\n",
    "    torch.nn.init.normal_(diff_encoding.weight.data,\n",
    "                          0.0,\n",
    "                          1.0 / math.sqrt(encoding_size))\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {\"params\": adnet.parameters(), \"lr\": lr, },\n",
    "        {\"params\": same_encoding.parameters(), \"lr\": lr, },\n",
    "        {\"params\": diff_encoding.parameters(), \"lr\": lr, }, ])\n",
    "\n",
    "    op_schedule = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    adnet = nn.DataParallel(adnet)\n",
    "    adnet.cuda()\n",
    "\n",
    "    data_loader_train = DataLoader(train_ds, batch_size=batch_size,\n",
    "                                   shuffle=True)\n",
    "    \n",
    "    for epoch in range(0, EPOCHS):\n",
    "        adnet.train()\n",
    "        same_total_loss = 0.0\n",
    "        diff_total_loss = 0.0\n",
    "\n",
    "        for batch_idx, (x, y, z, idx) in enumerate(data_loader_train):\n",
    "            optimizer.zero_grad()\n",
    "            x, y, z = x.cuda(), y.cuda(), z.cuda()\n",
    "            x, y, z = (Variable(x).float(),\n",
    "                       Variable(y).float(),\n",
    "                       Variable(z).float())\n",
    "            y_pred = adnet(x, same_encoding(torch.LongTensor(idx)))\n",
    "            loss_cham = chamfer_loss(y, y_pred, ps=y.shape[-1])\n",
    "            same_total_loss += loss_cham.data.cpu().numpy()\n",
    "            loss_cham.backward()\n",
    "\n",
    "            z_pred = adnet(x, diff_encoding(torch.LongTensor(idx)))\n",
    "            loss_cham = chamfer_loss(z, z_pred, ps=z.shape[-1])\n",
    "            diff_total_loss += loss_cham.data.cpu().numpy()\n",
    "            loss_cham.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                print(f\"Epoch: {epoch}. batch_idx: {batch_idx}\")\n",
    "                print(\"Loss: \",same_total_loss / 100, diff_total_loss / 100)\n",
    "                same_total_loss = 0.0\n",
    "                diff_total_loss = 0.0\n",
    "        op_schedule.step(epoch)\n",
    "\n",
    "        if test_ds is not None and epoch % 5 == 0:\n",
    "            print(\"Eval: \", eval_decoder(adnet, test_ds, batch_size=batch_size))\n",
    "\n",
    "    if save_wt_fname is not None:\n",
    "        torch.save(adnet.module.state_dict(), DS.AUTODECODER_TRAINED_WEIGHT_DIR + '/' + save_wt_fname)\n",
    "    return adnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_decoder(decoder, eval_ds, batch_size=16):\n",
    "    decoder.eval()\n",
    "    encoding_ds = EncodingDS(eval_ds, decoder)\n",
    "    return encoding_ds.train_encodings(num_iterations=10, lr=0.05, batch_size=batch_size)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_whole_dset(HP, DS, train_ds, save_wt_fname='svm_whole.pkl'):\n",
    "    \"\"\"\n",
    "    Train the SVM\n",
    "\n",
    "    Suggested Parameters\n",
    "    batch_size=16\n",
    "    \"\"\"\n",
    "    batch_size = HP.batch_size\n",
    "    data_loader_train = DataLoader(train_ds, batch_size=16,\n",
    "                                   shuffle=True)\n",
    "    X,y = None, None\n",
    "    # Combine the entire dataset\n",
    "    for batch_idx, (_x, _y, _z, _idx, same_cls, diff_cls) in enumerate(data_loader_train):\n",
    "        same_cls, diff_cls = same_cls.detach().numpy(), diff_cls.detach().numpy()\n",
    "        same_target, diff_target = np.ones(\n",
    "            same_cls.shape[0]), np.zeros(diff_cls.shape[0])\n",
    "        \n",
    "        if X is None and y is None:\n",
    "            X = np.concatenate([same_cls, diff_cls], axis=0)\n",
    "            y = np.concatenate([same_target, diff_target], axis=0)\n",
    "        else:\n",
    "            X = np.concatenate([X, same_cls, diff_cls], axis=0)\n",
    "            y = np.concatenate([y, same_target, diff_target], axis=0)\n",
    "    \n",
    "    rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "    sgd_clf = SGDClassifier(loss='hinge', penalty='l2')\n",
    "\n",
    "    X_features = rbf_feature.fit_transform(X)\n",
    "    sgd_clf.fit(X_features, y)\n",
    "    y_pred = sgd_clf.predict(X_features)\n",
    "    \n",
    "    print(f\"Total Hinge Loss: {hinge_loss(y, y_pred)}\")\n",
    "    if save_wt_fname is not None:\n",
    "        _ = joblib.dump(sgd_clf, DS.CLASSIFIER_TRAINED_WEIGHT_DIR+'/'+save_wt_fname,\n",
    "                    compress=9)\n",
    "    return sgd_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_svm_whole_dset(svm_clf, test_ds, batch_size=16):\n",
    "    print(len(test_ds))\n",
    "    rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "    test_dl = DataLoader(test_ds, batch_size=batch_size,\n",
    "                         shuffle=False)\n",
    "    X, y = None, None\n",
    "    # Combine the entire dataset\n",
    "    for batch_idx, (_x, _y, _z, _idx, same_cls, diff_cls) in enumerate(data_loader_train):\n",
    "        same_cls, diff_cls = same_cls.detach().numpy(), diff_cls.detach().numpy()\n",
    "        same_target, diff_target = np.ones(\n",
    "            same_cls.shape[0]), np.zeros(diff_cls.shape[0])\n",
    "\n",
    "        if X is None and y is None:\n",
    "            X = np.concatenate([same_cls, diff_cls], axis=0)\n",
    "            y = np.concatenate([same_target, diff_target], axis=0)\n",
    "        else:\n",
    "            X = np.concatenate([X, same_cls, diff_cls], axis=0)\n",
    "            y = np.concatenate([y, same_target, diff_target], axis=0)\n",
    "\n",
    "    X_features = rbf_feature.fit_transform(X)\n",
    "    y_pred = svm_clf.predict(X_features)\n",
    "    y, y_pred = y.astype(int), y_pred.astype(int)\n",
    "    \n",
    "    same_corr_cnt = np.sum(y_pred & y)\n",
    "    same_incorr_cnt = np.sum(y & (y_pred ^ 1))\n",
    "    diff_corr_cnt = np.sum((y ^ 1) & (y_pred ^ 1))\n",
    "    diff_incorr_cnt = np.sum((y ^ 1) & y_pred)\n",
    "\n",
    "    total_loss = hinge_loss(y, y_pred)\n",
    "    if batch_idx % 100 == 0:\n",
    "        print(f\"Batch_idx: {batch_idx}\")\n",
    "        print(\"Cur loss: \", total_loss)\n",
    "\n",
    "    precision = same_corr_cnt / (same_corr_cnt+diff_incorr_cnt)\n",
    "    recall = same_corr_cnt / (same_corr_cnt+same_incorr_cnt)\n",
    "    print(\"------------------ Evaluation Report ------------------\")\n",
    "    print(f\"After {len(test_ds)} test points\")\n",
    "    print(f\"Total Accuracy: {(same_corr_cnt+diff_corr_cnt)/(2*len(test_ds))}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"Metrics for the same class:\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {(2*precision*recall)/(precision+recall)}\")\n",
    "\n",
    "    precision = diff_corr_cnt / (diff_corr_cnt+same_incorr_cnt)\n",
    "    recall = diff_corr_cnt / (diff_corr_cnt+diff_incorr_cnt)\n",
    "    print()\n",
    "    print(f\"Metrics for the diff class:\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {(2*precision*recall)/(precision+recall)}\")\n",
    "\n",
    "    return (total_loss,\n",
    "            same_corr_cnt, diff_corr_cnt,\n",
    "            same_incorr_cnt, diff_incorr_cnt,\n",
    "            len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_compnet(HP, DS, train_ds, test_ds=None, compnet=None, save_wt_fname='compnet.pth'):\n",
    "    \"\"\"\n",
    "    Train the CompNet\n",
    "\n",
    "    Suggested Parameters\n",
    "    EPOCHS=10\n",
    "    batch_size=16\n",
    "    encoding_size=256\n",
    "    learning_rate=0.001\n",
    "    \"\"\"\n",
    "    EPOCHS = HP.epochs\n",
    "    point_dim = HP.num_point_cloud\n",
    "    batch_size = HP.batch_size\n",
    "    encoding_size = HP.encoding_size\n",
    "    lr = HP.learning_rate\n",
    "\n",
    "    if compnet is None:\n",
    "        cpnet = CompNet(encoding_size=encoding_size)\n",
    "    else:\n",
    "        cpnet = compnet\n",
    "    cpnet = cpnet.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(cpnet.parameters(), lr=lr)\n",
    "    op_schedule = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    cpnet = nn.DataParallel(cpnet)\n",
    "    cpnet.cuda()\n",
    "\n",
    "    data_loader_train = DataLoader(train_ds, batch_size=batch_size,\n",
    "                                   shuffle=True)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(EPOCHS):\n",
    "        same_total_loss = 0.0\n",
    "        diff_total_loss = 0.0\n",
    "        cpnet.train()\n",
    "        for batch_idx, (x, y, z, idx, same_cls, diff_cls) in enumerate(data_loader_train):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            same_cls, diff_cls = same_cls.cuda(), diff_cls.cuda()\n",
    "            same_cls, diff_cls = Variable(\n",
    "                same_cls).float(), Variable(diff_cls).float()\n",
    "\n",
    "            same_pred = cpnet(same_cls)\n",
    "            same_target = torch.ones(same_pred.shape).float().cuda()\n",
    "            same_loss = loss_fn(same_pred, same_target)\n",
    "            same_loss.backward()\n",
    "            same_total_loss += same_loss.data.cpu().numpy()\n",
    "\n",
    "            diff_pred = cpnet(diff_cls)\n",
    "            diff_target = torch.zeros(diff_pred.shape).float().cuda()\n",
    "            diff_loss = loss_fn(diff_pred, diff_target)\n",
    "            diff_loss.backward()\n",
    "            diff_total_loss += diff_loss.data.cpu().numpy()\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                print(f\"Epoch: {epoch}. batch_idx: {batch_idx}\")\n",
    "                print(\"Loss: \", same_total_loss / 100, diff_total_loss / 100)\n",
    "                same_total_loss = 0.0\n",
    "                diff_total_loss = 0.0\n",
    "        op_schedule.step(epoch)\n",
    "\n",
    "        if test_ds is not None and epoch % 5 == 0:\n",
    "            print(\"Eval: \", eval_compnet(cpnet, test_ds, batch_size=batch_size))\n",
    "\n",
    "    if save_wt_fname is not None:\n",
    "        torch.save(cpnet.module.state_dict(),\n",
    "                   DS.CLASSIFIER_TRAINED_WEIGHT_DIR + '/' + save_wt_fname)\n",
    "    return cpnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_compnet(cpnet, test_ds, batch_size=16, pred_threshold=0.5):\n",
    "    cpnet.eval()\n",
    "    test_dl = DataLoader(test_ds, batch_size=batch_size,\n",
    "                         shuffle=False)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    same_total_loss = 0.0\n",
    "    diff_total_loss = 0.0\n",
    "    batch_cnt = 0\n",
    "    same_corr_cnt = 0.0\n",
    "    diff_corr_cnt = 0.0\n",
    "    same_incorr_cnt = 0.0\n",
    "    diff_incorr_cnt = 0.0\n",
    "\n",
    "    for batch_idx, (x, y, z, idx, same_cls, diff_cls) in enumerate(test_dl):\n",
    "        batch_cnt += 1\n",
    "        same_cls, diff_cls = same_cls.cuda(), diff_cls.cuda()\n",
    "\n",
    "        same_pred = cpnet(same_cls)\n",
    "        same_target = torch.ones(same_pred.shape).float().cuda()\n",
    "        same_loss = loss_fn(same_pred, same_target)\n",
    "        same_total_loss += same_loss.data.cpu().numpy()\n",
    "\n",
    "        same_corr_cnt += np.sum(same_pred.detach().cpu().numpy()\n",
    "                                > pred_threshold)\n",
    "        same_incorr_cnt += np.sum(same_pred.detach().cpu().numpy()\n",
    "                                  <= pred_threshold)\n",
    "\n",
    "        diff_pred = cpnet(diff_cls)\n",
    "        diff_target = torch.zeros(diff_pred.shape).float().cuda()\n",
    "        diff_loss = loss_fn(diff_pred, diff_target)\n",
    "        diff_total_loss += diff_loss.data.cpu().numpy()\n",
    "\n",
    "        diff_corr_cnt += np.sum(diff_pred.detach().cpu().numpy()\n",
    "                                < pred_threshold)\n",
    "        diff_incorr_cnt += np.sum(diff_pred.detach().cpu().numpy()\n",
    "                                  >= pred_threshold)\n",
    "\n",
    "    precision = same_corr_cnt / (same_corr_cnt+diff_incorr_cnt)\n",
    "    recall = same_corr_cnt / (same_corr_cnt+same_incorr_cnt) # same_corr_cnt / len(test_ds)\n",
    "    print(\"------------------ Evaluation Report ------------------\")\n",
    "    print(f\"Total Accuracy: {(same_corr_cnt+diff_corr_cnt)/(2*len(test_ds))}\")\n",
    "    print(f\"After {batch_cnt} batches and {len(test_ds)} test points\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Metrics for the same class:\")\n",
    "    print(f\"Avg loss: {same_total_loss / batch_cnt}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {(2*precision*recall)/(precision+recall)}\")\n",
    "\n",
    "    precision = diff_corr_cnt / (diff_corr_cnt+same_incorr_cnt) \n",
    "    recall = diff_corr_cnt / (diff_corr_cnt+diff_incorr_cnt) # diff_corr_cnt / len(test_ds)\n",
    "    print()\n",
    "    print(f\"Metrics for the diff class:\")\n",
    "    print(f\"Avg loss: {diff_total_loss / batch_cnt}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {(2*precision*recall)/(precision+recall)}\")\n",
    "\n",
    "    return (same_total_loss, diff_total_loss,\n",
    "            same_corr_cnt, diff_corr_cnt,\n",
    "            same_incorr_cnt, diff_incorr_cnt,\n",
    "            batch_cnt, len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, cls_samples, cls_num, autodecoder, compnet,\n",
    "             num_iterations=15, latent_size=256, lr=0.01, l2_reg=False):\n",
    "    autodecoder.eval()\n",
    "    compnet.eval()\n",
    "    num_samples = len(cls_samples) # 6\n",
    "    num_X = len(X)\n",
    "    print(\"Orig X Shape:\", X.shape) # 16 3 683\n",
    "    X = X.unsqueeze(1).repeat((1, num_samples, 1, 1))\n",
    "    # print(X.shape) # [16, 6, 3, 683]\n",
    "    # print(X.reshape((X.shape[0]*num_samples, 1, *X.shape[2:])).shape) # [96, 1, 3, 683]\n",
    "    X = X.reshape((X.shape[0]*num_samples, 1, *X.shape[2:])).squeeze()\n",
    "    print(\"X shape\", X.shape, \"num_X:\", num_X) # [96, 3, 683], 16\n",
    "    cls_samples = cls_samples.repeat(num_X, 1, 1)\n",
    "    print(\"cls_samples Shape\", cls_samples.shape) # [96, 3, 683]\n",
    "\n",
    "    loss, encoding = find_encoding(X, cls_samples, autodecoder, encoding_iters=num_iterations,\n",
    "                                   encoding_size=latent_size, lr=lr, l2_reg=l2_reg)\n",
    "    print(loss, 'ecnoding=;',encoding.shape) # 96 x 256\n",
    "    preds = compnet(encoding)\n",
    "    print(preds.shape) # 96,1\n",
    "    preds = preds.reshape(\n",
    "        (num_X, cls_num, (preds.shape[0]//(num_X * cls_num))))  # 16, 3, 96/(16x3)\n",
    "    \n",
    "    print(\"preds shape\", preds.shape) # 16 3 2\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "# Testing our AutoDecoder\n",
    "\n",
    "## Generating dummy Train and Test pair Fish shapes using LoaderFish (Wang et al. 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......Synthesizing Training Pairs......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:28<00:00, 706.47it/s]\n",
      "  8%|▊         | 76/1000 [00:00<00:01, 758.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......Synthesizing Testing Pairs......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 758.42it/s]\n"
     ]
    }
   ],
   "source": [
    "CD_before = []  # chamfer distance before registration\n",
    "CD_after = []  # chamfer distance after registration\n",
    "\n",
    "train_size = 20000  # size for training data\n",
    "test_size = 1000    # size for testing data\n",
    "deformation_list = [0.5]\n",
    "deformation = deformation_list[0]\n",
    "\n",
    "print(\".......Synthesizing Training Pairs......\")\n",
    "lf_train = LoaderFish.PointRegDataset(total_data=train_size,\n",
    "                                      deform_level=deformation,\n",
    "                                      noise_ratio=0,\n",
    "                                      outlier_ratio=0,\n",
    "                                      outlier_s=False,\n",
    "                                      outlier_t=True,\n",
    "                                      noise_s=False,\n",
    "                                      noise_t=True,\n",
    "                                      missing_points=0,\n",
    "                                      miss_source=False,\n",
    "                                      miss_targ=True)\n",
    "\n",
    "data_loader_lf_train = torch.utils.data.DataLoader(\n",
    "    lf_train, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\".......Synthesizing Testing Pairs......\")\n",
    "lf_test = LoaderFish.PointRegDataset(total_data=test_size,\n",
    "                                     deform_level=deformation,\n",
    "                                     noise_ratio=0,\n",
    "                                     outlier_ratio=0,\n",
    "                                     outlier_s=False,\n",
    "                                     outlier_t=True,\n",
    "                                     noise_s=False,\n",
    "                                     noise_t=True,\n",
    "                                     missing_points=0,\n",
    "                                     miss_source=False,\n",
    "                                     miss_targ=True)\n",
    "\n",
    "data_loader_lf_test = torch.utils.data.DataLoader(\n",
    "    lf_test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([16, 2, 91]) 16 torch.Size([2, 91])\n"
     ]
    }
   ],
   "source": [
    "sample_data = next(iter(data_loader_lf_train))\n",
    "print(len(sample_data), sample_data[0].shape, len(sample_data[0]), sample_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the AutoDecoder on the sample generated LoaderFish dataset\n",
    "\n",
    "We generate encoding latent vectors for the shapes generated by the LoaderFish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Setup for loaderfish test\n",
    "EPOCHS = 4\n",
    "lr = 0.0001\n",
    "encoding_size = 256\n",
    "scheduler_gamma = 0.5\n",
    "scheduler_step_sz = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch Index:0\n",
      "0.039074585\n",
      "Epoch:0 Batch Index:100\n",
      "0.046686605\n",
      "Epoch:0 Batch Index:200\n",
      "0.03604628\n",
      "Epoch:0 Batch Index:300\n",
      "0.034093272\n",
      "Epoch:0 Batch Index:400\n",
      "0.03187909\n",
      "Epoch:0 Batch Index:500\n",
      "0.032592937\n",
      "Epoch:0 Batch Index:600\n",
      "0.02686105\n",
      "Epoch:0 Batch Index:700\n",
      "0.023645239\n",
      "Epoch:0 Batch Index:800\n",
      "0.022990186\n",
      "Epoch:0 Batch Index:900\n",
      "0.02095288\n",
      "Epoch:0 Batch Index:1000\n",
      "0.021544505\n",
      "Epoch:0 Batch Index:1100\n",
      "0.018804103\n",
      "Epoch:0 Batch Index:1200\n",
      "0.017172476\n",
      "Epoch:1 Batch Index:0\n",
      "0.019202758\n",
      "Epoch:1 Batch Index:100\n",
      "0.019225465\n",
      "Epoch:1 Batch Index:200\n",
      "0.015909288\n",
      "Epoch:1 Batch Index:300\n",
      "0.017095147\n",
      "Epoch:1 Batch Index:400\n",
      "0.014226408\n",
      "Epoch:1 Batch Index:500\n",
      "0.01501642\n",
      "Epoch:1 Batch Index:600\n",
      "0.0135903545\n",
      "Epoch:1 Batch Index:700\n",
      "0.012498623\n",
      "Epoch:1 Batch Index:800\n",
      "0.014098689\n",
      "Epoch:1 Batch Index:900\n",
      "0.013716753\n",
      "Epoch:1 Batch Index:1000\n",
      "0.010699049\n",
      "Epoch:1 Batch Index:1100\n",
      "0.012806572\n",
      "Epoch:1 Batch Index:1200\n",
      "0.012028878\n",
      "Epoch:2 Batch Index:0\n",
      "0.009834815\n",
      "Epoch:2 Batch Index:100\n",
      "0.011857872\n",
      "Epoch:2 Batch Index:200\n",
      "0.011231398\n",
      "Epoch:2 Batch Index:300\n",
      "0.011437714\n",
      "Epoch:2 Batch Index:400\n",
      "0.013063169\n",
      "Epoch:2 Batch Index:500\n",
      "0.009515977\n",
      "Epoch:2 Batch Index:600\n",
      "0.0102491295\n",
      "Epoch:2 Batch Index:700\n",
      "0.009426787\n",
      "Epoch:2 Batch Index:800\n",
      "0.009435374\n",
      "Epoch:2 Batch Index:900\n",
      "0.008473066\n",
      "Epoch:2 Batch Index:1000\n",
      "0.0093408935\n",
      "Epoch:2 Batch Index:1100\n",
      "0.010208545\n",
      "Epoch:2 Batch Index:1200\n",
      "0.008396012\n",
      "Epoch:3 Batch Index:0\n",
      "0.008301421\n",
      "Epoch:3 Batch Index:100\n",
      "0.008152752\n",
      "Epoch:3 Batch Index:200\n",
      "0.008444963\n",
      "Epoch:3 Batch Index:300\n",
      "0.007044398\n",
      "Epoch:3 Batch Index:400\n",
      "0.008349875\n",
      "Epoch:3 Batch Index:500\n",
      "0.007954646\n",
      "Epoch:3 Batch Index:600\n",
      "0.0070231487\n",
      "Epoch:3 Batch Index:700\n",
      "0.008027348\n",
      "Epoch:3 Batch Index:800\n",
      "0.0077239065\n",
      "Epoch:3 Batch Index:900\n",
      "0.007846674\n",
      "Epoch:3 Batch Index:1000\n",
      "0.0074914363\n",
      "Epoch:3 Batch Index:1100\n",
      "0.0077788113\n",
      "Epoch:3 Batch Index:1200\n",
      "0.0071987817\n"
     ]
    }
   ],
   "source": [
    "adnet = AutoDecoder(point_dim=2)\n",
    "adnet.cuda()\n",
    "\n",
    "encoding = torch.nn.Embedding(len(lf_train), encoding_size, max_norm=1.0)\n",
    "# init encoding with Kaiming Initialization\n",
    "torch.nn.init.normal_(\n",
    "    encoding.weight.data,\n",
    "    0.0,\n",
    "    1.0 / math.sqrt(encoding_size))\n",
    "\n",
    "optimizer = torch.optim.Adam([{\"params\": adnet.parameters(), \"lr\": lr, },\n",
    "                           {\"params\": encoding.parameters(), \"lr\": lr, }, ])\n",
    "op_schedule = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=scheduler_step_sz, gamma=scheduler_gamma)\n",
    "\n",
    "# use multiple gpus\n",
    "adnet = nn.DataParallel(adnet)\n",
    "adnet.cuda()\n",
    "\n",
    "# Train the AutoDecoder adnet on the training data\n",
    "for epoch in range(EPOCHS):\n",
    "    adnet.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (X, y, theta, _, idx) in enumerate(data_loader_lf_train):\n",
    "        X, y, theta = X.cuda(), y.cuda(), theta.cuda()\n",
    "        X, y, theta = Variable(X).float(), Variable(y).float(), Variable(theta).float()\n",
    "        y_pred = adnet(X, encoding(torch.LongTensor(idx)))\n",
    "        loss_cham = chamfer_loss(y, y_pred, ps=y.shape[-1])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_cham.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch:{epoch} Batch Index:{batch_idx}\")\n",
    "            print(loss_cham.data.cpu().numpy())\n",
    "    op_schedule.step(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights of the decoder\n",
    "torch.save(adnet.state_dict(), DS.AUTODECODER_TRAINED_WEIGHT_DIR+'/loaderfish_encoder01.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the AudoDecoder network on the sample LoaderFish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0075545073 tensor(3.1510, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006043713 tensor(3.1520, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056344485 tensor(3.2068, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054054316 tensor(3.2882, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053869146 tensor(3.2961, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0053696525 tensor(3.3052, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "At batch:0, Chamfer Loss:0.005351841449737549, Encoding Chamfer Loss:0.005352201871573925\n",
      "0 0.008483854 tensor(3.1695, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006258505 tensor(3.1938, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005798897 tensor(3.2611, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005542446 tensor(3.3562, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005514102 tensor(3.3672, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0054917466 tensor(3.3791, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0098328525 tensor(3.1304, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007169248 tensor(3.1864, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006435076 tensor(3.3011, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0061248927 tensor(3.4087, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006099813 tensor(3.4188, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006074196 tensor(3.4296, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009701333 tensor(3.1958, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006688758 tensor(3.2257, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006260037 tensor(3.3034, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006031249 tensor(3.3911, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006011477 tensor(3.4006, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0059922026 tensor(3.4105, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007558029 tensor(3.1476, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005872137 tensor(3.1866, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0055107353 tensor(3.2652, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005277631 tensor(3.3879, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0052567776 tensor(3.4009, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0052355966 tensor(3.4154, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008533659 tensor(3.1607, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064284042 tensor(3.1837, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006015107 tensor(3.2269, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057704262 tensor(3.3055, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005749497 tensor(3.3157, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0057292418 tensor(3.3262, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008082025 tensor(3.1586, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005949456 tensor(3.1790, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005513844 tensor(3.2304, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005274117 tensor(3.3201, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005254663 tensor(3.3289, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0052349647 tensor(3.3393, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0074271876 tensor(3.1508, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064246706 tensor(3.1640, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0060862913 tensor(3.2699, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005895031 tensor(3.3879, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005878042 tensor(3.4017, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0058623296 tensor(3.4158, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008594128 tensor(3.2042, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0066560186 tensor(3.2135, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006220314 tensor(3.2856, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0059863646 tensor(3.3706, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0059674946 tensor(3.3784, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0059495945 tensor(3.3870, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.006772482 tensor(3.2140, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0057343617 tensor(3.2010, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005444633 tensor(3.2400, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005251298 tensor(3.3242, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0052366364 tensor(3.3345, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00522246 tensor(3.3453, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010444242 tensor(3.1535, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0067459173 tensor(3.1931, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0062570316 tensor(3.2308, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0060339505 tensor(3.2760, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0060169264 tensor(3.2821, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0059999973 tensor(3.2886, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.00861412 tensor(3.1823, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0068476056 tensor(3.1888, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006323251 tensor(3.2589, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0060114446 tensor(3.3639, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005980172 tensor(3.3755, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005955383 tensor(3.3861, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.00827288 tensor(3.2444, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064148027 tensor(3.2954, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006061438 tensor(3.3476, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005871361 tensor(3.4297, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0058559594 tensor(3.4374, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0058410596 tensor(3.4458, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009435881 tensor(3.1386, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0071588363 tensor(3.1635, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0064230836 tensor(3.2373, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.00619064 tensor(3.3160, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0061686253 tensor(3.3267, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0061450596 tensor(3.3386, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008190289 tensor(3.1714, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006454302 tensor(3.2175, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005988176 tensor(3.2968, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057516648 tensor(3.4018, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0057345764 tensor(3.4116, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0057163965 tensor(3.4227, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008841435 tensor(3.1701, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0068305824 tensor(3.1610, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0063322666 tensor(3.1878, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006070772 tensor(3.2386, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006050833 tensor(3.2440, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006030799 tensor(3.2506, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0101149995 tensor(3.1808, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0074829645 tensor(3.2295, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006853416 tensor(3.3012, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006444108 tensor(3.4164, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006409271 tensor(3.4296, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0063750227 tensor(3.4435, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.00781579 tensor(3.1205, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061356737 tensor(3.1242, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056420583 tensor(3.1839, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005411847 tensor(3.2680, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005394017 tensor(3.2762, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005376771 tensor(3.2849, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008147807 tensor(3.1282, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064178826 tensor(3.1663, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0060260007 tensor(3.2419, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057965335 tensor(3.3488, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.005779987 tensor(3.3598, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00576331 tensor(3.3714, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008223071 tensor(3.1483, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006684003 tensor(3.1749, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006155052 tensor(3.2760, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005930447 tensor(3.3771, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0059146634 tensor(3.3860, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0058992137 tensor(3.3953, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009290014 tensor(3.1815, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0073304493 tensor(3.2031, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006774002 tensor(3.2666, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0065398486 tensor(3.3537, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006521936 tensor(3.3620, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0065036095 tensor(3.3706, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009590235 tensor(3.1778, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0065701175 tensor(3.2063, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006049165 tensor(3.2782, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057887146 tensor(3.3554, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005767439 tensor(3.3631, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005745613 tensor(3.3721, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009727449 tensor(3.1515, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007310019 tensor(3.1716, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0067766304 tensor(3.2465, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0064974376 tensor(3.3477, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0064758756 tensor(3.3578, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0064550443 tensor(3.3686, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0083605 tensor(3.1538, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.00610509 tensor(3.1677, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056001106 tensor(3.2439, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0053371945 tensor(3.3466, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053153597 tensor(3.3574, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00529312 tensor(3.3695, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008483314 tensor(3.1353, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0067713736 tensor(3.1307, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006090904 tensor(3.2246, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005775475 tensor(3.3361, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0057334905 tensor(3.3505, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0056914324 tensor(3.3651, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.006736229 tensor(3.1725, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005578531 tensor(3.1870, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0052567096 tensor(3.2438, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005079301 tensor(3.3312, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0050656907 tensor(3.3405, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005048396 tensor(3.3508, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0073860884 tensor(3.1259, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0059448485 tensor(3.1455, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005665462 tensor(3.1963, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055107707 tensor(3.2723, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005494627 tensor(3.2810, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005479009 tensor(3.2906, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008694622 tensor(3.2447, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007002015 tensor(3.2579, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.00654637 tensor(3.3296, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006235147 tensor(3.4365, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0062092124 tensor(3.4468, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006185813 tensor(3.4578, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007957547 tensor(3.0910, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0058055623 tensor(3.1499, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005336094 tensor(3.2705, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0051337224 tensor(3.3805, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0051153917 tensor(3.3908, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005097885 tensor(3.4016, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010381631 tensor(3.1303, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0071872454 tensor(3.1845, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006669363 tensor(3.2697, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0063516367 tensor(3.3824, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0063193473 tensor(3.3931, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006289466 tensor(3.4048, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0067359097 tensor(3.1994, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005684524 tensor(3.2284, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0054020877 tensor(3.2969, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005256488 tensor(3.3794, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0052450686 tensor(3.3880, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005233926 tensor(3.3970, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "At batch:30, Chamfer Loss:0.005222289822995663, Encoding Chamfer Loss:0.005222536623477936\n",
      "0 0.009612962 tensor(3.1926, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0074432273 tensor(3.2015, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006864446 tensor(3.2615, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006551687 tensor(3.3598, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0065255933 tensor(3.3697, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0064991494 tensor(3.3809, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009168889 tensor(3.1960, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0060986266 tensor(3.2141, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056579877 tensor(3.2595, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005473005 tensor(3.3068, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005458112 tensor(3.3124, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005443009 tensor(3.3186, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009228979 tensor(3.0641, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0071902904 tensor(3.0841, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0066427547 tensor(3.1589, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006341304 tensor(3.2608, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0063157114 tensor(3.2727, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0062888996 tensor(3.2861, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009072458 tensor(3.1387, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0066018715 tensor(3.1289, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006145288 tensor(3.1638, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0059251217 tensor(3.2201, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0059045395 tensor(3.2277, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0058809062 tensor(3.2365, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0087609915 tensor(3.1297, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006348608 tensor(3.1583, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005939335 tensor(3.1959, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005708923 tensor(3.2595, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005688114 tensor(3.2674, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005665483 tensor(3.2767, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.006287933 tensor(3.0917, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0052773776 tensor(3.1147, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0050046314 tensor(3.1862, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0048714364 tensor(3.2622, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0048596915 tensor(3.2689, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.004848541 tensor(3.2764, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008046249 tensor(3.0769, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0060782153 tensor(3.1119, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.005641676 tensor(3.2150, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005379372 tensor(3.3318, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005360034 tensor(3.3435, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005341499 tensor(3.3554, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009007725 tensor(3.1478, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006669062 tensor(3.1803, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006271222 tensor(3.2339, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006045394 tensor(3.3282, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0060261777 tensor(3.3379, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006005489 tensor(3.3490, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.006994224 tensor(3.1576, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0055223806 tensor(3.1206, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.00518781 tensor(3.1540, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0050348593 tensor(3.2091, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0050224895 tensor(3.2159, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0050107613 tensor(3.2232, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0075987964 tensor(3.1584, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0059723556 tensor(3.1554, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005603888 tensor(3.2069, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0053738197 tensor(3.2833, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053584855 tensor(3.2906, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0053428495 tensor(3.2985, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007212419 tensor(3.2079, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005705262 tensor(3.2270, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005308658 tensor(3.3027, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005124888 tensor(3.3755, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0051097316 tensor(3.3834, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005094904 tensor(3.3917, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010731267 tensor(3.1404, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0081483135 tensor(3.1727, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0076684663 tensor(3.2311, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.007375444 tensor(3.3190, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.007349808 tensor(3.3304, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.007322415 tensor(3.3433, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009272292 tensor(3.1566, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0066823685 tensor(3.2003, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006089363 tensor(3.2550, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005845745 tensor(3.3203, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0058083413 tensor(3.3284, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005766064 tensor(3.3358, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0064646853 tensor(3.1997, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005635984 tensor(3.2019, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005351182 tensor(3.2892, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0051896228 tensor(3.3993, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0051765833 tensor(3.4102, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005163911 tensor(3.4220, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0074261664 tensor(3.2324, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0060878578 tensor(3.2590, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056757694 tensor(3.3303, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054703024 tensor(3.4230, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0054538427 tensor(3.4319, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005437828 tensor(3.4420, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0069215433 tensor(3.1252, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061336746 tensor(3.1001, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0058773793 tensor(3.1506, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057036174 tensor(3.2388, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0056915223 tensor(3.2475, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005680117 tensor(3.2566, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008896476 tensor(3.1817, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006841735 tensor(3.2041, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0062450045 tensor(3.2867, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0060033286 tensor(3.3787, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0059837606 tensor(3.3885, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00596392 tensor(3.3993, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.00878863 tensor(3.1534, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006411817 tensor(3.1573, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0060105333 tensor(3.2219, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057971743 tensor(3.3080, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0057794773 tensor(3.3168, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005762785 tensor(3.3257, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007578246 tensor(3.1216, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061412742 tensor(3.1610, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0058181994 tensor(3.2403, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005611137 tensor(3.3462, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0055929385 tensor(3.3571, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0055760145 tensor(3.3689, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.006878048 tensor(3.1472, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005684503 tensor(3.1424, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0053802566 tensor(3.2043, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005199153 tensor(3.2966, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0051847952 tensor(3.3055, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005169386 tensor(3.3152, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009041298 tensor(3.0798, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006702573 tensor(3.1283, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0061717066 tensor(3.2222, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005950562 tensor(3.3181, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005933188 tensor(3.3270, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0059140394 tensor(3.3365, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007914007 tensor(3.2217, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064805965 tensor(3.2532, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0059754844 tensor(3.3617, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005790191 tensor(3.4566, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0057739704 tensor(3.4654, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005758872 tensor(3.4747, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.006997141 tensor(3.1091, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0058422103 tensor(3.1108, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0055372524 tensor(3.1696, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005334056 tensor(3.2485, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053181285 tensor(3.2568, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005302448 tensor(3.2658, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008188519 tensor(3.1747, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006597496 tensor(3.1785, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0062273857 tensor(3.2564, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006033247 tensor(3.3600, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0060174116 tensor(3.3692, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0060013286 tensor(3.3799, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.010016667 tensor(3.1780, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0066708196 tensor(3.2139, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0062235435 tensor(3.2653, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0059581236 tensor(3.3325, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0059329174 tensor(3.3391, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005907761 tensor(3.3474, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.00824733 tensor(3.1046, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061658043 tensor(3.1503, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.005730929 tensor(3.2262, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055020256 tensor(3.3251, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0054806476 tensor(3.3361, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005460027 tensor(3.3483, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.006828017 tensor(3.1770, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005648057 tensor(3.1840, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005253704 tensor(3.2478, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0050491574 tensor(3.3408, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005031884 tensor(3.3517, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005015385 tensor(3.3631, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008191011 tensor(3.2175, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006204535 tensor(3.2279, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005767917 tensor(3.2643, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055797296 tensor(3.3208, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0055647446 tensor(3.3267, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005549076 tensor(3.3337, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008366361 tensor(3.2039, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006417876 tensor(3.2144, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0060226996 tensor(3.2616, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057911803 tensor(3.3397, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0057731518 tensor(3.3479, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005755717 tensor(3.3562, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0073751886 tensor(3.1422, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006276113 tensor(3.1504, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0059784763 tensor(3.2126, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0058021187 tensor(3.2991, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005787132 tensor(3.3080, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005771112 tensor(3.3184, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "At batch:60, Chamfer Loss:0.005756291560828686, Encoding Chamfer Loss:0.005756576545536518\n",
      "0 0.008311241 tensor(3.0899, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006651754 tensor(3.1140, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0060710367 tensor(3.1960, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0056759617 tensor(3.2874, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.00560998 tensor(3.3002, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005576154 tensor(3.3131, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009777218 tensor(3.1637, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.008066591 tensor(3.1859, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0075521264 tensor(3.2451, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.007245581 tensor(3.3374, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0072172135 tensor(3.3464, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.007186209 tensor(3.3565, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008116379 tensor(3.1192, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064691096 tensor(3.1295, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.00599468 tensor(3.2014, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005717956 tensor(3.3182, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005698699 tensor(3.3308, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005679983 tensor(3.3442, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007725475 tensor(3.1732, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0058718794 tensor(3.1774, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0054757046 tensor(3.2262, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005255592 tensor(3.2982, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0052366876 tensor(3.3070, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005217474 tensor(3.3164, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009268371 tensor(3.1697, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0074538193 tensor(3.1764, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.00696124 tensor(3.2289, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0066485894 tensor(3.3255, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.006620903 tensor(3.3357, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0065940158 tensor(3.3467, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008543334 tensor(3.1124, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0063648727 tensor(3.1029, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0059784977 tensor(3.1202, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057710903 tensor(3.1724, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005752947 tensor(3.1789, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005734807 tensor(3.1859, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009785434 tensor(3.1168, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006987213 tensor(3.1371, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0064696036 tensor(3.1918, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006264878 tensor(3.2544, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.00624815 tensor(3.2616, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00622988 tensor(3.2699, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0076302295 tensor(3.1986, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.00609363 tensor(3.2016, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056742816 tensor(3.2745, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054498697 tensor(3.3780, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0054309736 tensor(3.3895, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0054132235 tensor(3.4021, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009682935 tensor(3.1648, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0072657377 tensor(3.1873, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0067560365 tensor(3.2715, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0064873216 tensor(3.3955, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0064590373 tensor(3.4078, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0064336034 tensor(3.4205, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007284686 tensor(3.0987, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005829772 tensor(3.0817, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.00553656 tensor(3.1181, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005373906 tensor(3.1940, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053602126 tensor(3.2022, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005346066 tensor(3.2111, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008330966 tensor(3.1360, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0063836356 tensor(3.1610, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005995663 tensor(3.2191, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057642823 tensor(3.2943, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0057433667 tensor(3.3009, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0057242094 tensor(3.3082, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008925962 tensor(3.1891, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061350884 tensor(3.2277, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005713808 tensor(3.2779, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005470534 tensor(3.3568, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.00545154 tensor(3.3656, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005433406 tensor(3.3751, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.006969026 tensor(3.1369, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0059088008 tensor(3.1377, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005578778 tensor(3.2131, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005394632 tensor(3.3099, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005381093 tensor(3.3197, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00536691 tensor(3.3306, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007945257 tensor(3.1139, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061855307 tensor(3.1286, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005656435 tensor(3.2074, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005409455 tensor(3.3010, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0053910306 tensor(3.3100, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005373601 tensor(3.3190, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.009826237 tensor(3.2213, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0082724355 tensor(3.2286, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.007430752 tensor(3.3398, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0070584854 tensor(3.4515, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0069053746 tensor(3.4639, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006851077 tensor(3.4764, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009274318 tensor(3.0716, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0066897753 tensor(3.1091, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0061416943 tensor(3.1598, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005894609 tensor(3.2260, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0058744224 tensor(3.2325, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0058553717 tensor(3.2393, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008901619 tensor(3.1367, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0069697895 tensor(3.1501, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.00661596 tensor(3.1915, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006416945 tensor(3.2627, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0064002303 tensor(3.2705, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006383898 tensor(3.2789, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008325681 tensor(3.1350, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061763492 tensor(3.1867, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056429687 tensor(3.2605, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054247626 tensor(3.3458, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0054065576 tensor(3.3547, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005389228 tensor(3.3639, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008250826 tensor(3.0829, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0062228725 tensor(3.1212, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0057507055 tensor(3.2057, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005495103 tensor(3.3093, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005469188 tensor(3.3216, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0054407422 tensor(3.3348, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009097211 tensor(3.2614, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.007123914 tensor(3.2869, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0066776853 tensor(3.3549, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006446873 tensor(3.4476, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.00642681 tensor(3.4565, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00640718 tensor(3.4660, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007795522 tensor(3.1154, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006116757 tensor(3.1721, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0057185963 tensor(3.2650, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0054331976 tensor(3.3935, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0054136105 tensor(3.4057, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.00539431 tensor(3.4185, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007969292 tensor(3.1678, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061754063 tensor(3.1542, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0058433013 tensor(3.2106, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005641518 tensor(3.2939, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005626444 tensor(3.3023, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005611661 tensor(3.3114, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0071394257 tensor(3.2160, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005887085 tensor(3.2448, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005570592 tensor(3.3284, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005387773 tensor(3.4228, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.00537272 tensor(3.4320, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005358668 tensor(3.4419, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009006631 tensor(3.1765, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0072907233 tensor(3.1869, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0067043914 tensor(3.2610, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006371708 tensor(3.3698, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0063401307 tensor(3.3816, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0063106613 tensor(3.3944, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008861637 tensor(3.2394, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006749882 tensor(3.2918, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0063199517 tensor(3.3595, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006077237 tensor(3.4414, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0060519516 tensor(3.4503, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006026421 tensor(3.4596, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0072583766 tensor(3.2093, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005976613 tensor(3.2152, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005679318 tensor(3.2869, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055257473 tensor(3.3839, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0055112415 tensor(3.3936, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005497428 tensor(3.4047, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008010286 tensor(3.1172, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064189294 tensor(3.0895, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005991023 tensor(3.1296, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005799203 tensor(3.2006, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005783881 tensor(3.2085, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005768179 tensor(3.2170, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0073676817 tensor(3.2278, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0057608476 tensor(3.1957, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0054353653 tensor(3.2200, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0052411146 tensor(3.2943, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0052240766 tensor(3.3031, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0052079344 tensor(3.3124, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.007950731 tensor(3.2106, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0060817534 tensor(3.2049, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.005766352 tensor(3.2579, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005583716 tensor(3.3388, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005568151 tensor(3.3477, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0055534192 tensor(3.3569, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0072352244 tensor(3.1682, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0061146202 tensor(3.1782, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0057810913 tensor(3.2658, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055672224 tensor(3.3638, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005550448 tensor(3.3741, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005535261 tensor(3.3849, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "At batch:90, Chamfer Loss:0.00551962573081255, Encoding Chamfer Loss:0.005519965663552284\n",
      "0 0.009518403 tensor(3.1774, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0070025465 tensor(3.1994, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006531074 tensor(3.2457, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0062367194 tensor(3.3262, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0062092803 tensor(3.3346, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006182483 tensor(3.3434, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.00838891 tensor(3.1359, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0064309877 tensor(3.1240, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0060047773 tensor(3.1901, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0057431087 tensor(3.2781, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005723875 tensor(3.2871, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0057045645 tensor(3.2963, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008397336 tensor(3.1050, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006259997 tensor(3.1522, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0057721534 tensor(3.2569, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0055714245 tensor(3.3580, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.005554156 tensor(3.3689, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0055375122 tensor(3.3801, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.009113792 tensor(3.1094, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.006845657 tensor(3.1290, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.006233858 tensor(3.2059, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0059951586 tensor(3.2926, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005976419 tensor(3.3017, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.005957724 tensor(3.3119, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.008925144 tensor(3.1910, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0069943923 tensor(3.2051, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.00651427 tensor(3.2663, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0062201917 tensor(3.3397, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0061994316 tensor(3.3479, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.006179156 tensor(3.3570, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0063710073 tensor(3.1831, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.005501849 tensor(3.2038, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0051858462 tensor(3.3003, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0050095664 tensor(3.4178, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.00499353 tensor(3.4291, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.004978981 tensor(3.4417, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0074692983 tensor(3.1928, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0055541107 tensor(3.2121, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0051714736 tensor(3.2851, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.0050074663 tensor(3.3469, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0049935086 tensor(3.3530, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.004979605 tensor(3.3594, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0090393005 tensor(3.1496, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0071029034 tensor(3.2019, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0066540185 tensor(3.2688, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.006418979 tensor(3.3523, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.0063975477 tensor(3.3617, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0063769375 tensor(3.3716, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "0 0.0078901155 tensor(3.1667, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "50 0.0060370257 tensor(3.1872, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "100 0.0056710807 tensor(3.2781, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "150 0.005468134 tensor(3.3786, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "200 0.005448955 tensor(3.3907, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "250 0.0054298877 tensor(3.4036, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "adnet.eval()\n",
    "\n",
    "for batch_idx, (X, y, theta, _, idx) in enumerate(data_loader_lf_test):\n",
    "    X, y, theta = X.cuda(), y.cuda(), theta.cuda()\n",
    "    X, y, theta = Variable(X).float(), Variable(y).float(), Variable(theta).float()\n",
    "    loss_encoding, generated_encoding = find_encoding(X, y, adnet, \n",
    "                                             encoding_iters=300, \n",
    "                                             encoding_size=256)\n",
    "    \n",
    "    y_pred = adnet(X, generated_encoding)\n",
    "    loss_cham = chamfer_loss(y, y_pred, ps=y.shape[-1])\n",
    "    if batch_idx % 30 == 0:\n",
    "        print(f\"At batch:{batch_idx}, Chamfer Loss:{loss_cham}, Encoding Chamfer Loss:{loss_encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoderless",
   "language": "python",
   "name": "dencoder_less_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
